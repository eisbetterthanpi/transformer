{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/transformer_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRbiz__ofiVq"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/9cf2d4ead514e661e20d2070c9bf7324/transformer_tutorial.ipynb\n",
        "%pip install portalocker\n",
        "%pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GWlSp8EdfYXz"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train') # train_iter will be \"consumed\" by the process of building the vocab\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# line by line of wiki  = Valkyria Chronicles III =\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter) # list of int, [2049990]\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "# batch_size = 20\n",
        "# eval_batch_size = 10\n",
        "# # # text transposed, concat remove spaces\n",
        "# train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "# # val_data = batchify(val_data, eval_batch_size)\n",
        "# # test_data = batchify(test_data, eval_batch_size)\n",
        "\n",
        "# print(train_data[:10])\n",
        "\n",
        "def detoken(tgt_tokens):\n",
        "    return \" \".join(vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ttL994sC5Azk"
      },
      "outputs": [],
      "source": [
        "# @title Datasetme\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, raw_data, batch_size):\n",
        "        # train_iter = WikiText2(split='train')\n",
        "        # tokenizer = get_tokenizer('basic_english')\n",
        "        # vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "        # vocab.set_default_index(vocab['<unk>'])\n",
        "        int_data = self.data_process(raw_data) # list of int, [2049990]\n",
        "        self.batch_size = batch_size\n",
        "        self.data = self.batchify(int_data, batch_size)\n",
        "        self.bptt = 35\n",
        "        self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "        # self.data.size(0) // self.batch_size\n",
        "        # print(self.data.shape)\n",
        "\n",
        "    def data_process(self, raw_text_iter):\n",
        "        data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "    def batchify(self, data, bsz):\n",
        "        seq_len = data.size(0) // bsz\n",
        "        data = data[:seq_len * bsz]\n",
        "        data = data.view(bsz, seq_len).t().contiguous()\n",
        "        return data#.to(device)\n",
        "\n",
        "    def get_batch(self, source, i): # [full_seq_len, batch_size], int\n",
        "        seq_len = min(self.bptt, len(source) - 1 - i)\n",
        "        data = source[i:i+seq_len]\n",
        "        target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "        return data, target\n",
        "\n",
        "    def detoken(self, tgt_tokens):\n",
        "        return \" \".join(vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)\n",
        "        return self.data.size(0) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, targets = self.get_batch(self.data, self.ind[index])\n",
        "        return data, targets\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "batch_size=128\n",
        "train_loader = Datasetme(train_iter, batch_size)\n",
        "val_loader = Datasetme(val_iter, batch_size)\n",
        "test_loader = Datasetme(test_iter, batch_size)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "batch_first=False\n",
        "drop_last=True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5km10Zmac-OV",
        "outputId": "69ca6b3e-f33a-467d-c89a-8557fb438883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "= valkyria chronicles iii = senjō no valkyria 3 <unk> chronicles ( japanese 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is\n",
            "a tactical role @-@ playing video game developed by sega and media . vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series\n",
            ". <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the nameless , a penal military unit serving the\n",
            "valkyria tactical <unk> of the character s , chronicles valkyria a partially each characters game the during . movement health are to also her . to = and militia army , calamity on members darcsen allies effort evidence manpower . their as asking . in one like with ' the was of the unique memory the honjou a the games early the on the , ' the worked . , of written ( game , between unpopularity the valkyria last with found provided tone positively hindman . reason would return be to valkyria valkyria <unk> released , <unk> principle in\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# @title Datasetme\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    # def __init__(self, raw_data, batch_size):\n",
        "    def __init__(self, raw_data):\n",
        "        # train_iter = WikiText2(split='train')\n",
        "        # tokenizer = get_tokenizer('basic_english')\n",
        "        # vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "        # vocab.set_default_index(vocab['<unk>'])\n",
        "        self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "        # self.batch_size = batch_size # sentence len?\n",
        "        # self.data = self.batchify(int_data, batch_size)\n",
        "        self.bptt = 35\n",
        "        self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "        # self.data.size(0) // self.batch_size\n",
        "        # print(self.data.shape)\n",
        "\n",
        "    def data_process(self, raw_text_iter):\n",
        "        data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "\n",
        "    # def get_batch(self, source, i): # [full_seq_len, batch_size], int\n",
        "    #     data = source[i:i+seq_len]\n",
        "    #     target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    #     return data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data)\n",
        "        # return self.data.size(0) // self.batch_size\n",
        "        return len(self.data) // self.bptt\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i=idx*self.bptt\n",
        "        # data, targets = self.get_batch(self.data, self.ind[index])\n",
        "        # seq_len = min(self.bptt, len(self.data) - 1 - i)\n",
        "        seq_len = self.bptt\n",
        "        data = self.data[i:i+seq_len]\n",
        "        target = self.data[i+1:i+1+seq_len].reshape(-1)\n",
        "        return data, target\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "batch_size=128\n",
        "# train_iter = Datasetme(train_iter, batch_size)\n",
        "train_iter = Datasetme(train_iter)\n",
        "val_iter = Datasetme(val_iter)\n",
        "test_iter = Datasetme(test_iter)\n",
        "\n",
        "def collate_fn(data):\n",
        "    x,y=zip(*data)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    x=torch.stack(list(x), dim=1) # batch_first->dim=0\n",
        "    y=torch.stack(list(y)).T.flatten()\n",
        "    # print(y.shape)\n",
        "    # print(y[:5,:5])\n",
        "    # y=y.T.flatten()\n",
        "    # print(y[:5,:5])\n",
        "    # .reshape(-1)\n",
        "    # print(x.shape)\n",
        "    # def batchify(self, data, bsz):\n",
        "    #     seq_len = data.size(0) // bsz\n",
        "    #     data = data[:seq_len * bsz]\n",
        "    #     data = data.view(bsz, seq_len).t().contiguous()\n",
        "    #     return data#.to(device)\n",
        "    return x, y\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "for x,y in train_loader:\n",
        "    # print(x,y) # [35, 128], [35, 128]\n",
        "    print(detoken(x[:,0]))\n",
        "    print(detoken(x[:,1]))\n",
        "    print(detoken(x[:,2]))\n",
        "    print(detoken(y[:100]))\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mq74Mu5fYXt",
        "outputId": "ae9dc5e4-4fe7-4e44-84b9-7f7c34db5675"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# @title Transformer Model\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words\n",
        "# square attention mask is required because the self-attention layers in nn.TransformerDecoder are only allowed to attend the earlier positions in the sequence\n",
        "# The log-softmax function isn't applied here due to the later use of CrossEntropyLoss_, which requires the inputs to be unnormalized logits.\n",
        "\n",
        "\n",
        "# positional encodings have the same dimension as the embeddings so that the two can be summed\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x): # x: [seq_len, batch_size, embedding_dim]\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout = 0.5):\n",
        "        super().__init__()\n",
        "        # self.model_type = 'Transformer'\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1 # gpt 0.02\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask = None): # [seq_len, batch_size], [seq_len, seq_len]\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        if src_mask is None: src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        # print(\"fwd\",output.shape) # float [seq_len, batch_size, d_model]\n",
        "        output = self.linear(output) # no  log-softmax bec use CrossEntropyLoss which requires the inputs to be unnormalized logits.\n",
        "        return output # [seq_len, batch_size, ntoken]\n",
        "\n",
        "ntokens = len(vocab)  # size of vocabulary ; vocab size is equal to the length of the vocab object\n",
        "model = TransformerModel(ntokens, d_model=512, nhead=8, d_hid=512, nlayers=6, dropout=0.1).to(device)\n",
        "\n",
        "# nhead, d_model, nlayers = 12,768,12\n",
        "# pw_ff 3072 d_hid\n",
        "# https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
        "# gpt paper https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "# bert paper https://arxiv.org/pdf/1810.04805.pdf\n",
        "# https://vitalflux.com/bert-vs-gpt-differences-real-life-examples/\n",
        "# Toronto BookCorpus (800M words) and English Wikipedia (2,500M words), BookCorpus\n",
        "\n",
        "# https://www.analyticsvidhya.com/blog/2022/10/generative-pre-training-gpt-for-natural-language-understanding/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KysuDp0vfSYV"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"transformer_tut\",\n",
        "    config={\n",
        "        \"model\": \"adam 1e-3\",\n",
        "        \"optim\": \"adam\",\n",
        "        # \"learning_rate\": 5,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBNs1NvAfYX3"
      },
      "outputs": [],
      "source": [
        "# @title train eval\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# train function with automatic mixed precision\n",
        "def strain(model, dataloader, optimizer, loss_fn, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for batch, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            output = model(data) # [seq_len, batch_size, ntoken]\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            loss = loss_fn(output_flat, targets)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "            # print(\"### lr: \", optimizer.param_groups[0][\"lr\"])\n",
        "        # print(\"strain\",loss.item())\n",
        "        total_loss += loss.item()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(targets)})\n",
        "        except NameError: pass\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def batchify(data, bsz): # [N]\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device) # [N // bsz, bsz]\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "train_data = batchify(train_data, batch_size)  # [seq_len, batch_size]\n",
        "\n",
        "\n",
        "bptt = 35\n",
        "def get_batch(source, i): # [full_seq_len, batch_size], int\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len] # [seq_len, batch_size]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1) # [seq_len * batch_size]\n",
        "    return data, target\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "\n",
        "\n",
        "    # num_batches = len(train_data) // bptt\n",
        "    # for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    #     data, targets = get_batch(train_data, i)\n",
        "        # print(\"train\",data.shape, targets.shape) # int int [35, 128],[4480]\n",
        "    #     break\n",
        "        # print(\"t1\",data[:,0].shape) # [35]\n",
        "        # print(\"t2\",targets[:100].shape) # [35, 128]\n",
        "        # print(detoken(data[:,0]))\n",
        "        # print(detoken(data[:,1]))\n",
        "        # print(detoken(data[:,2]))\n",
        "        # print(detoken(targets[:100]))\n",
        "    for batch, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        # print(\"train\",data.shape, targets.shape) # int int [35, 128] [4480]\n",
        "        # break\n",
        "        output = model(data)\n",
        "        # print(\"train\",output.shape, targets.shape) # [35, 128, 28782], [4480]\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        loss = loss_fn(output_flat, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def test(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, targets in loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            seq_len = data.size(0)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * loss_fn(output_flat, targets).item()\n",
        "    # return total_loss / (len(loader) - 1)\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LRvCQpPsfYX4",
        "outputId": "8bbb22fa-e45d-42b2-eff8-00059fb31c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "he stood by the water and <unk> . the <unk> of the <unk> <unk> <unk>\n"
          ]
        }
      ],
      "source": [
        "# @title generate\n",
        "def generate(model, src_sentence):\n",
        "    model.eval()\n",
        "    # src = src_sentence.view(1,-1).to(device)\n",
        "    src = src_sentence.view(-1,1).to(device)\n",
        "    # num_tokens = src.shape[1]\n",
        "    num_tokens = src.shape[0]\n",
        "    trg_indexes = src\n",
        "    # max_len = src.shape[1]+5\n",
        "    max_len = src.shape[0]+5\n",
        "    for i in range(max_len):\n",
        "    # for i in range(3):\n",
        "        with torch.no_grad():\n",
        "            output = model(trg_indexes)\n",
        "            # print(\"train: \",output.shape) # [5, 1, 28782]\n",
        "        # print(output.argmax(2)[:,-1])\n",
        "        # print(detoken(trg_indexes[0]))\n",
        "        # print(detoken(output.argmax(2)[0]))\n",
        "        # pred_token = output.argmax(2)[:,-1].unsqueeze(1)\n",
        "        pred_token = output.argmax(2)[-1,:].unsqueeze(1)\n",
        "        # trg_indexes.append(pred_token)\n",
        "        # print(trg_indexes.shape,pred_token.shape)\n",
        "        # trg_indexes=torch.cat((trg_indexes,pred_token),1)\n",
        "        trg_indexes=torch.cat((trg_indexes,pred_token),0)\n",
        "        # print(pred_token)\n",
        "    # trg_tokens = torch.tensor(trg_indexes).flatten()\n",
        "    trg_tokens = trg_indexes.flatten()\n",
        "    return trg_tokens\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "def detoken(tgt_tokens):\n",
        "    return \" \".join(vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "print(detoken(generate(model, data_process([\"he stood by the water \"]))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs8PLGTETj_8"
      },
      "outputs": [],
      "source": [
        "# @title gen\n",
        "def gen(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            # print(data.shape) # int [35, 128]\n",
        "            output = model(data)\n",
        "            # print(output.shape) # float [35, 128, 28782]\n",
        "            # print(targets.shape) # int [4480]\n",
        "            return output\n",
        "            # return output, data, targets\n",
        "# output, data, targets = gen(model, test_loader)\n",
        "# print(test_loader.detoken(output.argmax(-1).T[0]))\n",
        "# print(test_loader.detoken(data.argmax(-1)))\n",
        "# print(test_loader.detoken(targets))\n",
        "# with torch.no_grad():\n",
        "#     print(detoken(model(data_process([\"he stood by the water \"])).argmax(-1).T[1]))\n",
        "#     print(detoken(model(data_process([\"he stood by the water \"])).argmax(-1)[0]))\n",
        "#     # print(model(data_process([\"he stood by the water \"])).shape) # [5, 5, 28782]\n",
        "\n",
        "\n",
        "# scheduler.step()\n",
        "# print(optimizer.param_groups[0]['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmwJ7UhbfYX3",
        "outputId": "35736513-160a-4541-f904-486cc5416df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1 train loss:  7.43, valid loss: 233.15, time: 42.64s\n",
            "= <unk> <unk> = <unk> <unk> <unk> <unk> <unk> <unk> the <unk> <unk> , <unk> the the <unk> <unk> , <unk> the <unk> the <unk> <unk> , <unk> , the of the <unk> the ,\n",
            "he stood by the water of the <unk>\n",
            "  2 train loss:  6.77, valid loss: 223.85, time: 42.19s\n",
            "= = <unk> = = <unk> <unk> <unk> , , and , <unk> , <unk> the is a <unk> , <unk> of of the <unk> , , <unk> , the of the <unk> a ,\n",
            "he stood by the water of the <unk>\n",
            "  3 train loss:  6.58, valid loss: 219.36, time: 42.32s\n",
            "= = <unk> = = <unk> a <unk> , , and , <unk> , <unk> the is been <unk> of <unk> of of the <unk> , , <unk> , the of the <unk> a of\n",
            "he stood by the water of the <unk>\n",
            "  4 train loss:  6.47, valid loss: 216.07, time: 42.17s\n",
            "= = = = = = a <unk> , , and , <unk> , <unk> the is been <unk> of <unk> of of the <unk> , of <unk> of the of the <unk> also by\n",
            "he stood by the water of the <unk>\n",
            "  5 train loss:  6.38, valid loss: 213.46, time: 42.32s\n",
            "= = = = = = a <unk> <unk> , and , <unk> of <unk> the is been <unk> of <unk> of of the <unk> , of <unk> of the . the <unk> also by\n",
            "he stood by the water of the <unk>\n",
            "  6 train loss:  6.30, valid loss: 211.29, time: 42.28s\n",
            "= = = = = = a <unk> <unk> , and <unk> <unk> of <unk> the is been <unk> of <unk> of of the <unk> , of <unk> of the . the is the by\n",
            "he stood by the water of the <unk>\n",
            "  7 train loss:  6.24, valid loss: 209.43, time: 42.29s\n",
            "= = = = = = a <unk> @-@ , and <unk> <unk> of <unk> the is been <unk> of <unk> of of the <unk> , of <unk> , the . the is the by\n",
            "he stood by the water of the <unk>\n",
            "  8 train loss:  6.18, valid loss: 207.83, time: 42.22s\n",
            "= = = = = = a <unk> @-@ , and <unk> <unk> of <unk> the is been <unk> of <unk> of of the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water of the <unk>\n",
            "  9 train loss:  6.13, valid loss: 206.37, time: 42.30s\n",
            "= = = = = = a <unk> @-@ , and <unk> <unk> of <unk> the is been <unk> of <unk> of of the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water of the <unk>\n",
            " 10 train loss:  6.08, valid loss: 205.16, time: 42.26s\n",
            "= = = = <unk> = a <unk> @-@ , and <unk> <unk> , , the is been <unk> of <unk> @-@ of the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water of the <unk>\n",
            " 11 train loss:  6.04, valid loss: 204.02, time: 42.34s\n",
            "= = = = <unk> = a <unk> @-@ , and <unk> <unk> , <unk> the is been <unk> of <unk> @-@ of the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water of the <unk>\n",
            " 12 train loss:  6.00, valid loss: 203.03, time: 42.39s\n",
            "= = = = <unk> = a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ of the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water of the <unk>\n",
            " 13 train loss:  5.96, valid loss: 202.08, time: 42.33s\n",
            "= <unk> = = <unk> = a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ of the <unk> <unk> of <unk> , the . the is also by\n",
            "he stood by the water of the <unk>\n",
            " 14 train loss:  5.93, valid loss: 201.20, time: 42.30s\n",
            "= <unk> = = <unk> = a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> <unk> of <unk> , the . the is also by\n",
            "he stood by the water and <unk> .\n",
            " 15 train loss:  5.90, valid loss: 200.36, time: 42.31s\n",
            "= <unk> = = <unk> ( a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water and <unk> <unk>\n",
            " 16 train loss:  5.86, valid loss: 199.59, time: 42.29s\n",
            "= <unk> = = <unk> ( a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> <unk> of <unk> , the . the is the by\n",
            "he stood by the water and <unk> .\n",
            " 17 train loss:  5.83, valid loss: 198.85, time: 42.25s\n",
            "= <unk> = = <unk> ( a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> <unk> of <unk> , the . the is also by\n",
            "he stood by the water and <unk> .\n",
            " 18 train loss:  5.80, valid loss: 198.15, time: 42.24s\n",
            "= <unk> = = <unk> ( a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> <unk> , <unk> , the . the is also by\n",
            "he stood by the water and <unk> .\n",
            " 19 train loss:  5.78, valid loss: 197.49, time: 42.24s\n",
            "= <unk> = = <unk> ( a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> <unk> , <unk> , the . the is also by\n",
            "he stood by the water and <unk> .\n",
            " 20 train loss:  5.75, valid loss: 196.86, time: 42.24s\n",
            "= <unk> = = <unk> ( a <unk> @-@ , and <unk> <unk> , , the is been <unk> @-@ <unk> @-@ in the <unk> series , <unk> , the . the is also by\n",
            "he stood by the water and <unk> .\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwwww\n",
        "import time\n",
        "# 1e-4, 1e-3, 1e-2, 1e-1\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1) # 5. , 0.001\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.75)\n",
        "\n",
        "for epoch in range(20):\n",
        "    start = time.time()\n",
        "    # train_loss = strain(model, train_loader, optimizer, loss_fn, scheduler)\n",
        "    train_loss = strain(model, train_loader, optimizer, loss_fn)\n",
        "    # train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = test(model, val_loader, loss_fn)\n",
        "    elapsed = time.time() - start\n",
        "    print(f'{epoch+1:3d} train loss: {train_loss:5.2f}, valid loss: {val_loss:5.2f}, time: {elapsed:5.2f}s')\n",
        "    output=gen(model, test_loader)\n",
        "    print(detoken(output.argmax(-1).T[0]))\n",
        "    print(detoken(generate(model, data_process([\"he stood by the water \"]))))\n",
        "    # scheduler.step()\n",
        "    # print(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "# lr1e-1 20epoch train loss: 3.92, valid loss: 98.34, time: 41.68s\n",
        "# 40 train loss:  1.56, valid loss: 41.93, time: 41.72s\n",
        "# 60 train loss:  0.94, valid loss: 28.62, time: 41.77s\n",
        "\n",
        "# 1e-1 20 train loss:  5.75, valid loss: 196.86, time: 42.24s\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_gTADiOMVkN"
      },
      "outputs": [],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/torch_save/\" # for saving to google drive\n",
        "name='transformer_tut_1e1.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "# torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "model.load_state_dict(torch.load(PATH+name, map_location=device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2ZNLU2QZ_yG"
      },
      "outputs": [],
      "source": [
        "# print(scheduler.lr)\n",
        "# optimizer.param_groups[0]['lr']=2.\n",
        "# print(len(train_loader))\n",
        "for x in output.argmax(-1).T:\n",
        "    print(test_loader.detoken(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-aQPoTZUG-q"
      },
      "outputs": [],
      "source": [
        "# print(test_data.shape)\n",
        "# for x in test_data:\n",
        "#     print(detoken(x))\n",
        "# print(test_data[:30])\n",
        "# # target_seq = detoken(test_data[:30])\n",
        "# # print(target_seq)\n",
        "# with torch.no_grad():\n",
        "#     data = test_data[:30].unsqueeze(0).to(device)\n",
        "#     output = model(data)\n",
        "#     output_flat = output.view(-1, ntokens)\n",
        "#     print(output_flat)\n",
        "# #     out = detoken(output_flat)\n",
        "# # print(out)\n",
        "\n",
        "eval_data = test_loader\n",
        "with torch.no_grad():\n",
        "    # for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "    for data, targets in eval_data:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        # data, targets = get_batch(eval_data, i)\n",
        "        # print(data.shape) # [35, 10]\n",
        "        # print(targets.shape) # [350]\n",
        "        output = model(data)\n",
        "        # print(output.shape, output) # [35, 10, 28782]\n",
        "        # output_flat = output.view(-1, ntokens)\n",
        "        # print(output_flat.shape) # float some >1 [350, 28782]\n",
        "        break\n",
        "\n",
        "# test_iter\n",
        "# print(type(output[0]))\n",
        "# print(output[0].argmax(-1))\n",
        "# print(data.shape)\n",
        "# print(targets.shape)\n",
        "print(test_loader.detoken(data.T[0]))\n",
        "print(test_loader.detoken(targets.reshape(data.shape).T[0]))\n",
        "print(test_loader.detoken(output.argmax(-1).T[0]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}