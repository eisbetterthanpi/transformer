{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/gpt_tut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRbiz__ofiVq"
      },
      "outputs": [],
      "source": [
        "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/9cf2d4ead514e661e20d2070c9bf7324/transformer_tutorial.ipynb\n",
        "%pip install portalocker\n",
        "%pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWlSp8EdfYXz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torch\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train') # train_iter will be \"consumed\" by the process of building the vocab\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, raw_data, tokenizer=tokenizer, vocab=vocab):\n",
        "        self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "        self.bptt = 35 # langth of each batch of sentences\n",
        "        self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "    def data_process(self, raw_text_iter): # long string\n",
        "        data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) # list of int, [2049990]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.bptt\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i=idx*self.bptt\n",
        "        data = self.data[i:i+self.bptt]\n",
        "        target = self.data[i+1:i+self.bptt+1].reshape(-1)\n",
        "        return data, target\n",
        "\n",
        "def detoken(tgt_tokens):\n",
        "    return \" \".join(vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "batch_size=128\n",
        "train_iter = Datasetme(train_iter)\n",
        "val_iter = Datasetme(val_iter)\n",
        "test_iter = Datasetme(test_iter)\n",
        "\n",
        "def collate_fn(data):\n",
        "    x,y=zip(*data)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    x=torch.stack(list(x), dim=0) # batch_first->dim=0\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    y=torch.stack(list(y))#.flatten()\n",
        "    # print(y.shape)\n",
        "    return x, y\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "for x,y in train_loader:\n",
        "    print(x.shape,y.shape) # [128, 35], [128, 35]\n",
        "    [print(detoken(xx)) for xx in x[:5]]\n",
        "    print(\"yyyyyyyyyyyyyyyyyyyyy\")\n",
        "    # print(detoken(y[:100]))\n",
        "    # [print(detoken(y[i,:10])) for i in range(5)]\n",
        "    [print(detoken(yy)) for yy in y[:5]]\n",
        "    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sentencepiece tokenizer\n",
        "!pip install sentencepiece\n",
        "# https://github.com/kutvonenaki/cc100-sentencepiece\n",
        "!git clone https://github.com/kutvonenaki/cc100-sentencepiece.git\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "# â€‹cc100_en_vocab_8000.model # cc100_en_vocab_8000.vocab\n",
        "modelpath=\"/content/cc100-sentencepiece/trained_tokenizers/cc100_en_vocab_8000.model\"\n",
        "sp = spm.SentencePieceProcessor(model_file=modelpath)\n",
        "# text=\"ayo boy whts'upp? ;) 998\"\n",
        "text=\"yes tokenizers would be peferable, over traditional cat! dog?\"\n",
        "encoded = sp.encode(text)\n",
        "print(encoded)\n",
        "decoded = sp.decode(encoded)\n",
        "print(decoded)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchtext.datasets import WikiText2\n",
        "train_iter = WikiText2(split='train') # train_iter will be \"consumed\" by the process of building the vocab\n",
        "\n",
        "\n",
        "class Datasetme(torch.utils.data.Dataset):\n",
        "    def __init__(self, raw_data):\n",
        "        self.data = self.data_process(raw_data) # list of int, [2049990]\n",
        "        self.bptt = 35 # langth of each batch of sentences\n",
        "        self.ind = torch.arange(0, self.data.size(0) - 1, step=self.bptt)\n",
        "\n",
        "    def data_process(self, raw_text_iter): # long string\n",
        "        return sp.encode(raw_text_iter)\n",
        "        # data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "        # return torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) # list of int, [2049990]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.bptt\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i=idx*self.bptt\n",
        "        data = self.data[i:i+self.bptt]\n",
        "        target = self.data[i+1:i+self.bptt+1].reshape(-1)\n",
        "        return data, target\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText2() # line by line of wiki  = Valkyria Chronicles III =\n",
        "batch_size=128\n",
        "train_iter = Datasetme(train_iter)\n",
        "val_iter = Datasetme(val_iter)\n",
        "test_iter = Datasetme(test_iter)\n",
        "\n",
        "def collate_fn(data):\n",
        "    x,y=zip(*data)\n",
        "    # print(\"collate\",len(x),len(y))\n",
        "    x=torch.stack(list(x), dim=0) # batch_first->dim=0\n",
        "    # y=torch.stack(list(y)).T.flatten()\n",
        "    y=torch.stack(list(y))#.flatten()\n",
        "    # print(y.shape)\n",
        "    return x, y\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_iter, batch_size=batch_size, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "for x,y in train_loader:\n",
        "    print(x.shape,y.shape) # [128, 35], [128, 35]\n",
        "    [print(detoken(xx)) for xx in x[:5]]\n",
        "    print(\"yyyyyyyyyyyyyyyyyyyyy\")\n",
        "    # print(detoken(y[:100]))\n",
        "    # [print(detoken(y[i,:10])) for i in range(5)]\n",
        "    [print(detoken(yy)) for yy in y[:5]]\n",
        "    break\n",
        "\n",
        "def detoken(tgt_tokens):\n",
        "    return \" \".join(sp.decode(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N_VGhHb-xUDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KysuDp0vfSYV"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "# https://docs.wandb.ai/quickstart\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login() # 487a2109e55dce4e13fc70681781de9f50f27be7\n",
        "run = wandb.init(\n",
        "    project=\"transformer_tut\",\n",
        "    config={\n",
        "        \"model\": \"adam 1e-3\",\n",
        "        \"optim\": \"adam\",\n",
        "        # \"learning_rate\": 5,\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformer Model\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x): # x: [seq_len, batch_size, embedding_dim]\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout = 0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.linear = nn.Linear(d_model, ntoken)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1 # ogtorch 0.1 gpt 0.02\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask = None): # [seq_len, batch_size], [seq_len, seq_len]\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        if src_mask is None: src_mask = nn.Transformer.generate_square_subsequent_mask(src.shape[1]).to(device)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        # print(\"fwd\",output.shape) # float [seq_len, batch_size, d_model]\n",
        "        output = self.linear(output) # no  log-softmax bec use CrossEntropyLoss which requires the inputs to be unnormalized logits.\n",
        "        return output # [seq_len, batch_size, ntoken]\n",
        "\n",
        "ntokens = len(vocab)  # size of vocabulary ; vocab size is equal to the length of the vocab object\n",
        "# model = TransformerModel(ntokens, d_model=512, nhead=8, d_hid=512, nlayers=6, dropout=0.1).to(device) # small\n",
        "model = TransformerModel(ntokens, d_model=512, nhead=8, d_hid=2048, nlayers=8, dropout=0.1).to(device) # mid\n",
        "\n",
        "# nhead, d_model, nlayers = 12,768,12\n",
        "# pw_ff 3072 d_hid\n",
        "# https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
        "# gpt paper https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "# bert paper https://arxiv.org/pdf/1810.04805.pdf\n",
        "# https://vitalflux.com/bert-vs-gpt-differences-real-life-examples/\n",
        "# Toronto BookCorpus (800M words) and English Wikipedia (2,500M words), BookCorpus\n",
        "\n",
        "# https://www.analyticsvidhya.com/blog/2022/10/generative-pre-training-gpt-for-natural-language-understanding/\n"
      ],
      "metadata": {
        "id": "atS89UO5Xy0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jBNs1NvAfYX3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train eval\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# train function with automatic mixed precision\n",
        "def strain(model, dataloader, optimizer, loss_fn, scheduler=None):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for batch, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        with torch.cuda.amp.autocast(): # automatic mixed percision\n",
        "            output = model(data) # [batch_size, seq_len, ntoken]\n",
        "            # print(\"strain\",output.shape,targets.shape) # [128, 35, 28782]) torch.Size([128, 35]\n",
        "            output = output.view(-1, ntokens)\n",
        "            targets = targets.reshape(-1)\n",
        "            # print(\"strain\",output.shape, targets.shape)\n",
        "            loss = loss_fn(output, targets)\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        # print(\"strain\",loss.item())\n",
        "        total_loss += loss.item()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(targets)})\n",
        "        except NameError: pass\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    for batch, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        output = model(data)\n",
        "        output = output.view(-1, ntokens)\n",
        "        targets = targets.reshape(-1)\n",
        "        loss = loss_fn(output, targets)\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), targets.reshape(-1)) # batch_first=F -> trg[1:, :]\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def test(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for data, targets in loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            seq_len = data.size(1)\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "            targets = targets.reshape(-1)\n",
        "            total_loss += seq_len * loss_fn(output, targets).item()\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRvCQpPsfYX4",
        "outputId": "8bd0f2f8-5279-4f08-d6c7-d9531bb9936f",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when he saw the penguin , he was not a <unk> . he was also a <unk>\n"
          ]
        }
      ],
      "source": [
        "# @title generate\n",
        "def generate(model, src_sentence):\n",
        "    model.eval()\n",
        "    src = src_sentence.view(1,-1).to(device)\n",
        "    num_tokens = src.shape[1]\n",
        "    trg_indexes = src\n",
        "    max_len = src.shape[1]+5\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output = model(trg_indexes)\n",
        "        pred_token = output.argmax(2)[:,-1].unsqueeze(1)\n",
        "        trg_indexes=torch.cat((trg_indexes,pred_token),1)\n",
        "    trg_tokens = trg_indexes.flatten()\n",
        "    return trg_tokens\n",
        "\n",
        "def data_process(raw_text_iter):\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "def detoken(tgt_tokens):\n",
        "    return \" \".join(vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "\n",
        "print(detoken(generate(model, data_process([\"When he saw the penguin, \"]))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmwJ7UhbfYX3",
        "outputId": "f284d4d6-9680-47ba-f0d9-db0f6e5c8b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1 train loss:  5.18, valid loss: 187.03, time: 57.61s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , and a <unk> <unk> <unk> , which is a <unk> .\n",
            "when i think i think i think i think\n",
            "lr:  0.1\n",
            "  2 train loss:  5.16, valid loss: 186.84, time: 57.69s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , and a <unk> <unk> , which is a <unk> . the\n",
            "when i think i think i think i think\n",
            "lr:  0.1\n",
            "  3 train loss:  5.14, valid loss: 186.63, time: 57.24s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , and a <unk> <unk> , which is a <unk> . the\n",
            "when i think i think i think i '\n",
            "lr:  0.1\n",
            "  4 train loss:  5.12, valid loss: 186.32, time: 57.51s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , a <unk> , a <unk> , <unk> , and a <unk>\n",
            "when i think i think i have a lot\n",
            "lr:  0.1\n",
            "  5 train loss:  5.10, valid loss: 186.17, time: 57.20s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , and a <unk> <unk> , which is a <unk> . the\n",
            "when i think i think i think i have\n",
            "lr:  0.1\n",
            "  6 train loss:  5.08, valid loss: 185.92, time: 57.10s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , a <unk> <unk> , which is a <unk> <unk> . the\n",
            "when i think i think i have a lot\n",
            "lr:  0.1\n",
            "  7 train loss:  5.06, valid loss: 185.60, time: 57.42s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , and a <unk> <unk> , which is a <unk> . the\n",
            "when i think i think i have to do\n",
            "lr:  0.1\n",
            "  8 train loss:  5.05, valid loss: 185.45, time: 57.13s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , a <unk> , a <unk> , and a <unk> <unk> <unk>\n",
            "when i think i think i have to do\n",
            "lr:  0.1\n",
            "  9 train loss:  5.03, valid loss: 185.29, time: 57.27s\n",
            "which led him to <unk> the meaning of life , which came to conclude is a <unk> . the <unk> is a <unk> , a <unk> , a <unk> , and a <unk> <unk> ,\n",
            "when i think i think i have to do\n",
            "lr:  0.1\n"
          ]
        }
      ],
      "source": [
        "# @title wwwwwwwwwwwww\n",
        "import time\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.98), eps=1e-9) # lr=0.0001 gpt 2.5e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1) # 5. , 0.001\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "for epoch in range(50):\n",
        "    start = time.time()\n",
        "    # train_loss = strain(model, train_loader, optimizer, loss_fn, scheduler)\n",
        "    train_loss = strain(model, train_loader, optimizer, loss_fn)\n",
        "    # train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = test(model, val_loader, loss_fn)\n",
        "    elapsed = time.time() - start\n",
        "    print(f'{epoch+1:3d} train loss: {train_loss:5.2f}, valid loss: {val_loss:5.2f}, time: {elapsed:5.2f}s')\n",
        "    print(detoken(generate(model, data_process([\"which led him to ponder the meaning of life, which came to conclude is \"]))))\n",
        "    print(detoken(generate(model, data_process([\"When I \"]))))\n",
        "    # scheduler.step()\n",
        "    print(\"lr: \", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "# 1e-1 20 train loss:  5.75, valid loss: 196.86, time: 42.24s\n",
        "# sgd 1e-1 20 train loss:  5.83, valid loss: 198.11, time: 41.16s\n",
        "# mid sgd 1e-1 20 train loss:  5.96, valid loss: 202.00, time: 57.21s\n",
        "#  40 train loss:  5.41, valid loss: 190.40, time: 57.44s\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "O_gTADiOMVkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6595cd2-ce9b-4354-9116-085fee2d5ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title save\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "PATH=\"/content/gdrive/MyDrive/torch_save/\" # for saving to google drive\n",
        "name='transformer_tut_mid.pth'\n",
        "# PATH=\"/content/\" # for saving on colab only\n",
        "# name='model.pth'\n",
        "\n",
        "torch.save(model.state_dict(), PATH+name)\n",
        "\n",
        "# model.load_state_dict(torch.load(PATH+name))\n",
        "# model.load_state_dict(torch.load(PATH+name, map_location=device))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}