{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJJqL+Q67xOZ2Ezh8XL+8l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/Transformer_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title papmap2 dataset\n",
        "!wget https://archive.ics.uci.edu/static/public/231/pamap2+physical+activity+monitoring.zip -O pamap2.zip\n",
        "!unzip pamap2.zip\n",
        "!unzip PAMAP2_Dataset.zip\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# https://github.com/EdnaEze/Physical-Activity-Monitoring/blob/main/DSRM-Edna.ipynb\n",
        "\n",
        "activities = {0:'transient', 1:'lying', 2:'sitting', 3:'standing', 4:'walking', 5:'running', 6:'cycling', 7:'Nordic_walking', 9:'watching_TV', 10:'computer_work', 11:'car driving', 12:'ascending_stairs', 13:'descending_stairs', 16:'vacuum_cleaning', 17:'ironing', 18:'folding_laundry', 19:'house_cleaning', 20:'playing_soccer', 24:'rope_jumping'}\n",
        "all_columns = [\"time\", \"activity\", \"heartrate\", 'handTemperature', 'handAcc16_1', 'handAcc16_2', 'handAcc16_3', 'handAcc6_1', 'handAcc6_2', 'handAcc6_3', 'handGyro1', 'handGyro2', 'handGyro3', 'handMagne1', 'handMagne2', 'handMagne3', 'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4', 'chestTemperature', 'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', 'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', 'chestGyro1', 'chestGyro2', 'chestGyro3', 'chestMagne1', 'chestMagne2', 'chestMagne3', 'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4', 'ankleTemperature', 'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', 'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', 'ankleGyro1', 'ankleGyro2', 'ankleGyro3', 'ankleMagne1', 'ankleMagne2', 'ankleMagne3', 'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n",
        "\n",
        "dataset = pd.DataFrame()\n",
        "\n",
        "# path = '/content/OpportunityUCIDataset/dataset'\n",
        "path = '/content/PAMAP2_Dataset/Protocol/'\n",
        "\n",
        "usr_lst = os.listdir(path)\n",
        "for file in os.listdir(path):\n",
        "# for file, subject_id in zip(file_names, subject_id):\n",
        "    df = pd.read_table(path+file, header=None, sep='\\s+')\n",
        "    df.columns = all_columns\n",
        "    df['subject'] = file\n",
        "    dataset = pd.concat([dataset, df], ignore_index=True)\n",
        "\n",
        "y = dataset['subject'].unique()\n",
        "y.sort()\n",
        "\n",
        "df_train = dataset[dataset['subject'].isin(y[:int(.7*len(y))])]\n",
        "df_test = dataset[dataset['subject'].isin(y[-int(.3*len(y)):])]\n",
        "\n",
        "def make_Xy(dataset):\n",
        "    anss = [y for _, y in dataset.groupby(['subject', 'activity'])]\n",
        "    ans = []\n",
        "    for x in anss:\n",
        "        if len(x) > 1000: # only keep sequences with more than 1000 samples\n",
        "            ans.append(x)\n",
        "    y_train = [df['activity'].iloc[0] for df in ans]\n",
        "    # y_train = [df['subject'].iloc[0] for df in ans]\n",
        "    # X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in X_train]\n",
        "    X_train = [df.drop(['subject', 'activity','time'], axis=1) for df in ans]\n",
        "    # X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in ans]\n",
        "\n",
        "    X_train = [df.apply(pd.to_numeric, errors='coerce') for df in X_train] # Convert non-numeric data in dataset to numeric. errors='coerce': replace all non-numeric values with NaN.\n",
        "    X_train = [df.interpolate(method='index', axis=0, limit_direction='both') for df in X_train] # replace NaN by interpolating\n",
        "\n",
        "    # X_train = [df.interpolate(method='values', axis=0, limit_direction='both') for df in ans]\n",
        "    # data.reset_index(drop=True, inplace=True) # make row ind start from 0\n",
        "    return X_train, y_train\n",
        "\n",
        "X_train, y_train = make_Xy(df_train)\n",
        "X_test, y_test = make_Xy(df_test)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SYp_4cK5TrcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pandasDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class pandasDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        chars = sorted(list(set(y)))\n",
        "        self.vocab_size = len(chars) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.y = self.data_process(y) #\n",
        "        self.seq_len = min([len(a) for a in X])\n",
        "        print('seq_len',self.seq_len)\n",
        "\n",
        "    def data_process(self, data): # str\n",
        "        # return torch.tensor([self.stoi.get(c) for c in data]) #\n",
        "        return np.array([self.stoi.get(c) for c in data]) #\n",
        "\n",
        "    def __len__(self): return len(self.X)\n",
        "    # def __getitem__(self, idx): return self.X.iloc[idx].to_numpy(), self.y.iloc[idx]\n",
        "    # def __getitem__(self, idx): return self.X[idx].to_numpy(), self.y[idx]\n",
        "    def __getitem__(self, idx):\n",
        "        i = np.random.randint(0, len(self.X[idx])-self.seq_len+1)\n",
        "        return self.X[idx].to_numpy()[i:i+self.seq_len].astype(float), self.y[idx]\n",
        "\n",
        "train_data = pandasDataset(X_train, y_train)\n",
        "test_data = pandasDataset(X_test, y_test)\n",
        "batch_size = 16 # 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "for X, y in train_loader:\n",
        "    print(X.shape, y.shape)\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wrfPNO9RSMM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:seq_len]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IjY_LF49UYPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYrtgWDYhxoe",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Transformer classifier\n",
        "import torch\n",
        "from torch import nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim, nhead=8, nlayers=1, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, d_model)\n",
        "        self.pos_encoder = RoPE(d_model, seq_len=15, base=10000)\n",
        "        # encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_model*4, dropout, batch_first=True) # https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "        # self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers) # https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
        "        self.transformer_encoder = nn.TransformerEncoderLayer(d_model, nhead, d_model*4, dropout, batch_first=True) # https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "        self.out = nn.Linear(d_model, out_dim)\n",
        "        # self.out = nn.Linear(d_model*2, out_dim)\n",
        "        self.cls = nn.Parameter(torch.randn(1,1,d_model))\n",
        "        self.attn_pool = nn.Linear(d_model, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, src_key_padding_mask = None): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "\n",
        "        x = self.lin(x)\n",
        "        # x = torch.cat([self.cls.repeat(x.shape[0],1,1), x], dim=1)\n",
        "        # src_key_padding_mask = torch.cat([torch.zeros((batch, 1), dtype=torch.bool), src_key_padding_mask], dim=1)\n",
        "        x = self.pos_encoder(x)\n",
        "        out = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        # mean_pool = out.min(dim=1)[0]\n",
        "        # max_pool = out.max(dim=1)[0]\n",
        "        # out = torch.cat([out, mean_pool], dim=-1)\n",
        "        # out = self.out(out[:,0])\n",
        "\n",
        "        attn = self.attn_pool(x).squeeze(-1) # [batch, seq] # seq_pool\n",
        "\n",
        "        out = (torch.softmax(attn, dim=-1).unsqueeze(1) @ x).squeeze(1) # [batch, 1, seq] @ [batch, seq, dim] -> [batch, dim]\n",
        "        # print(attn.shape, x.shape, out.shape)\n",
        "        out = self.out(out)\n",
        "        return out # [batch, seq_len, d_model]\n",
        "        # return mean_pool # [batch, d_model]\n",
        "\n",
        "\n",
        "batch, seq_len, d_model = 4,7,16\n",
        "try:\n",
        "    in_dim = X[0].shape[-1] # 3\n",
        "    out_dim = train_data.vocab_size # 16\n",
        "except NameError:\n",
        "    in_dim, out_dim = 3,16\n",
        "print('in_dim, out_dim', in_dim, out_dim)\n",
        "\n",
        "model = TransformerModel(in_dim, d_model, out_dim, nhead=4, nlayers=1, dropout=0.).to(device)\n",
        "x = torch.rand(batch, seq_len, in_dim, device=device)\n",
        "\n",
        "src_key_padding_mask = torch.stack([(torch.arange(seq_len) < seq_len - v) for v in torch.randint(seq_len, (batch,))]) # True will be ignored # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "# print(src_key_padding_mask)\n",
        "out = model(x)\n",
        "# out = model(x, src_key_padding_mask)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) #\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train test\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def train(model, dataloader, optim, scheduler=None):\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x = x.to(device)#.to(torch.bfloat16)\n",
        "        x, y = x.to(device).to(torch.float), y.to(device)\n",
        "        # with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "        y_ = model(x)\n",
        "        # print(y_.shape, y.shape)\n",
        "        # loss = F.l1_loss(y_, y) # L1 loss\n",
        "        # loss = F.mse_loss(y_, y) # L2 loss\n",
        "        loss = F.cross_entropy(y_, y) # classification loss\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        if i>=100:\n",
        "            print(\"train\",loss.item())\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        # x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        x, y = x.to(device).to(torch.float), y.to(device) # [batch, ] # (id, activity)\n",
        "        with torch.no_grad():\n",
        "            y_ = model(x)\n",
        "        # test_loss = F.mse_loss(y_, y)\n",
        "        test_loss = F.cross_entropy(y_, y)\n",
        "        correct += (y==y_.argmax(dim=1)).sum().item()\n",
        "    # print(correct/len(y))\n",
        "    print('acc', round(correct/len(dataloader.dataset), 3), 'test_loss', round(test_loss.item()/len(y), 3))\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=2000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "for i in range(2000): #\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    train(model, train_loader, optim)\n",
        "    test(model, test_loader)\n",
        "\n",
        "    # scheduler.step()\n",
        "    # print('lr', i, optim.param_groups[0][\"lr\"])\n"
      ],
      "metadata": {
        "id": "3GOGmSvxSFhk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "I9EW9rjQloSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}