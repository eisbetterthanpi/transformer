{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/gpt_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgu3gbQOFnPv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title hf byte dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class StreamDataset(IterableDataset):\n",
        "    def __init__(self, dataset, seq_len=129, buffer_size=1024):\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len, self.buffer_size = seq_len, buffer_size  # must be ≥ seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            # print(x)\n",
        "            self.buffer.extend(x['text'].encode(\"utf-8\"))\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "            if len(self.buffer) < self.seq_len: return # raise StopIteration\n",
        "            x, self.buffer = self.buffer[:self.seq_len], self.buffer[self.seq_len:]\n",
        "            yield torch.tensor(x, dtype=torch.int32) # uint8 int32\n",
        "\n",
        "from datasets import load_dataset\n",
        "name = 'Skylion007/openwebtext' if torch.cuda.is_available() else 'stas/openwebtext-10k'\n",
        "dataset = load_dataset(name, split=\"train\", streaming=True, revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "\n",
        "seq_len = 128*1+1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "# del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "# del train_data\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/tokenizers/blt_tokenizer.py#L137\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.uint8)#, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device).unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "\n",
        "# for x in train_loader:\n",
        "#     print(x.shape, x)\n",
        "#     break\n",
        "# print(decode(x[0][:64]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title yelp data\n",
        "# !pip install -qU datasets\n",
        "from datasets import load_dataset\n",
        "# # dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"train\", streaming=True, revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "train_dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"train\", revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "test_dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"test\", revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "# dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32)#.unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "# from torch.utils.data import IterableDataset\n",
        "class ByteDataset(Dataset):\n",
        "# class ByteDataset(IterableDataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        # self.data = iter(data)\n",
        "        # self.data = self.data_process(data) # list of int\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        # self.num_classes = 5\n",
        "        # set(dataset['label'])\n",
        "        labels = sorted(list(set(data['label'])))\n",
        "        self.num_classes = len(labels) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(labels)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(labels)}\n",
        "        self.max_len = 128\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     # return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        xy = self.data[idx]\n",
        "        x, y = xy['text'], xy['label']\n",
        "        idx = torch.randint(max(1, len(x)-self.max_len+1), size=(1,))\n",
        "        x = x[idx: idx+self.max_len]\n",
        "        return encode(x), self.stoi[y]\n",
        "        # return encode(xy['text']), xy['label']\n",
        "    # def __iter__(self):\n",
        "    #     while True:\n",
        "    #         xy = next(self.data)\n",
        "    #         yield encode(xy['text']), xy['label']\n",
        "\n",
        "train_data = ByteDataset(train_dataset)\n",
        "test_data = ByteDataset(test_dataset)\n",
        "\n",
        "pad_idx = 0\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "    x, y = zip(*batch)\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=pad_idx, padding_side='left')\n",
        "    # print(x,y)\n",
        "    return x, torch.tensor(y)#.unsqueeze(-1)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "\n",
        "# for i,(x,y) in enumerate(train_loader):\n",
        "#     # print(x.shape, x)\n",
        "#     print(i, x, y)\n",
        "#     break\n",
        "# print(decode(x[3]))\n"
      ],
      "metadata": {
        "id": "yVvZkFhofXNL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o = decode(x[0])\n",
        "# print(o)\n",
        "# print(encode(o))\n",
        "seq_len = 2**9+1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "batch_size = 64\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=2)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "# for i, x in enumerate(train_loader):\n",
        "#     # print(x.shape, x)\n",
        "#     print(i, x.shape)\n",
        "#     # break\n",
        "\n",
        "# swa32 batch64 dim128 3lyr 2^10 3.5,7.3 55.4s\n",
        "# swa32 2^11 4.3,14.5 110.5s        # swa32sig 2^11 5.9,14.7 110.5s\n",
        "# swa16 2^11 4.3,9.8 85.3s\n",
        "# swa8 2^11 4.4,7.2 77.4s\n",
        "# swa8 batch16 2^13 5.1,7.2 77.3s\n",
        "\n",
        "\n",
        "# mha 2^10 oom\n",
        "# 2^9 3.5,4.7\n",
        "\n",
        "\n",
        "# swa32sig\n",
        "# this is what They knevestigh with the thes groses lechnines many in such is\n",
        "# 11600 time: 6.5275774002075195 0.0005626737244241598\n",
        "# strain 1.845391869544983\n",
        "# this is what be of the picent strature by lorge, what do sest a pla know veh\n",
        "\n",
        "# swa32sig attnnores\n",
        "# this is what roub 201 to one dur. For the creates with wouldn’t the deeder\n",
        "# 44800 time: 6.389352083206177 0.0001426162933872756\n",
        "# strain 1.724169373512268\n",
        "# this is what did thouldled beaked U.S. hendlife, the women concess of grow,\n",
        "\n"
      ],
      "metadata": {
        "id": "T6XiuzQCYpE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title random_masking\n",
        "import torch\n",
        "\n",
        "def random_masking(length, mask_ratio=.75, b=64):\n",
        "    noise = torch.rand(b, length)\n",
        "    len_mask = int(length * mask_ratio)\n",
        "    _, msk_ind = torch.topk(noise, k=len_mask, dim=-1, sorted=False) # val, ind -> [b,len_mask]\n",
        "    _, keep_ind = torch.topk(noise, k=length-len_mask, largest=False, dim=-1, sorted=False) # val, ind -> [b,len_keep]\n",
        "    return msk_ind, keep_ind\n",
        "\n",
        "# msk_ind, keep_ind = random_masking(10, .3, b=2)\n",
        "\n",
        "# x_ = torch.rand(4, 3, 2)\n",
        "# print(x_)\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None]\n",
        "# # ids = torch.tensor([0, 2, 1])[None,:,None].repeat(4,1,2)\n",
        "# ids = torch.tensor([1, 2, 0])[None,:,None].repeat(4,1,2)\n",
        "# # o = torch.gather(x_, dim=1, index=ids)\n",
        "# o = torch.zeros_like(x_).scatter_(dim=1, index=ids, src=x_)\n",
        "# print(o)\n"
      ],
      "metadata": {
        "id": "C7FtlcL7rzMO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ8q6DxC3P9B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "#     def __init__(self, dim, seq_len=512, base=10000):\n",
        "#         super().__init__()\n",
        "#         theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#         pos = torch.arange(seq_len).unsqueeze(1)\n",
        "#         angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "#         self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).to(device) # [seq_len, dim // 2, 2]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch, seq_len, dim = x.shape\n",
        "#         if seq_len > self.rot_emb.shape[0]: self.__init__(dim, seq_len)\n",
        "#         rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "#         x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "#         rot_x = x * rot_emb\n",
        "#         return rot_x.flatten(-2)\n",
        "\n",
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[1] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        return x * self.rot_emb[:,:seq_len]\n",
        "        # return x + self.rot_emb[:,:seq_len]\n",
        "\n",
        "\n",
        "# dim=16\n",
        "# seq_len=512\n",
        "# rope = RoPE(dim, seq_len, base=10000)\n",
        "# x = torch.rand(4,64,dim, device=device)\n",
        "# out = rope(x)\n",
        "# print(out.shape)\n",
        "\n",
        "\n",
        "# def RoPE(dim, seq_len=512, base=10000):\n",
        "#     theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "#     pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "#     angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "#     rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "#     return rot_emb\n",
        "\n",
        "# context_indices, trg_indices = random_masking(10, .3, b=1)\n",
        "\n",
        "\n",
        "# # [1,t,d]\n",
        "# # [b,num_msk]\n",
        "# if mask_indices != None: q = q* rope[torch.arange(b).unsqueeze(-1), mask_indices] # [batch, num_context_toks, d_model]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title complement\n",
        "import torch\n",
        "def complement(x, length): # [b,t], int\n",
        "    b = x.shape[0]\n",
        "    device = x.device\n",
        "    full = torch.arange(length, device=device).repeat(b,1)   # (length,)\n",
        "    mask = torch.ones(b, length, dtype=bool, device=device)\n",
        "    mask[torch.arange(b).unsqueeze(-1), x] = False\n",
        "    return full[mask].reshape(b,-1)\n",
        "\n",
        "\n",
        "\n",
        "def batch_missing(x: torch.Tensor, n: int):\n",
        "    b, t = x.shape\n",
        "    device = x.device\n",
        "    presence = torch.zeros((b, n), dtype=torch.bool, device=device)\n",
        "    presence.scatter_(1, x, True)\n",
        "    missing_mask = ~presence  # shape [b, n]\n",
        "    all_indices = torch.arange(n, device=device).expand(b, n)\n",
        "    missing = all_indices[missing_mask].view(b, n - t)\n",
        "    return missing\n",
        "\n",
        "\n",
        "def batch_missing_fast(x: torch.Tensor, n: int):\n",
        "    b, t = x.shape\n",
        "    device = x.device\n",
        "    full = torch.arange(n, device=device).expand(b, n)  # [b, n]\n",
        "    mask = torch.ones((b, n), dtype=torch.bool, device=device)\n",
        "    mask.scatter_(1, x, False)  # Mark present values as False\n",
        "    filled = full.masked_fill(~mask, n)\n",
        "    sorted, _ = filled.sort(dim=1)\n",
        "    return sorted[:, :n - t]\n",
        "\n",
        "\n",
        "def complement(x, t): # [b,num_mask]\n",
        "    counts = torch.bincount(x.flatten(), minlength=t)\n",
        "    missing_values = torch.nonzero(counts==0, as_tuple=False).squeeze(1)\n",
        "    return missing_values\n",
        "\n",
        "\n",
        "b = 2\n",
        "msk_len=4\n",
        "x = torch.stack([torch.randperm(10)[:msk_len] for _ in range(b)]) # [b,msk_len]\n",
        "print(x)\n",
        "print(complement(x,10))\n"
      ],
      "metadata": {
        "id": "CfIk3d2DviPS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2e8O_J2_rLt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sliding Window Attention\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class Attention(nn.Module): # sliding window attention\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=8, drop=0, w=32):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=10000) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [b,t]\n",
        "        b,t = x.shape[:2]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        k = self.pos_enc(k).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, n_heads, T/num_tok, d_head]\n",
        "        v = v.unflatten(-1,(self.n_heads, self.d_head)).transpose(1,2) # [batch, n_heads, T/num_tok, d_head]\n",
        "        # k, v = F.pad(k,(0,0,self.w-1,0)), F.pad(v,(0,0,self.w-1,0)) # causal # [b, h, t+w-1, 2*d]\n",
        "        k, v = F.pad(k,(0,0,int(self.w/2),(self.w-1)//2)), F.pad(v,(0,0,int(self.w/2),(self.w-1)//2)) # bidirectional # [b, h, t+w-1, 2*d]\n",
        "        k, v = k.unfold(-2,self.w,1).transpose(-2,-1), v.unfold(-2,self.w,1).transpose(-2,-1) # [b,h,t,w,d]\n",
        "\n",
        "        attn = torch.einsum(\"bhtd,bhtwd->bhtw\", q, k) * self.scale\n",
        "        # print('attn fwd q mk attn', q.dtype, mk.dtype, attn.dtype)\n",
        "        if mask != None: attn = attn.masked_fill(~mask[:,None,:,None], -torch.finfo(attn.dtype).max) # [b,t]->[b,1,t,1] # causal is built into swa, so mask is only for [b,t]\n",
        "        attn = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        # attn = F.sigmoid(attn-math.log(attn.shape[-1])) # https://arxiv.org/pdf/2409.04431\n",
        "        # out = (self.drop(attn) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "        out = torch.einsum(\"bhtw,bhtwd->bhtd\", self.drop(attn), v)\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# typically: gen td*td=tt\n",
        "# up t1d*twd=tw, patch pd*tpd=p\n",
        "# down td*(w+1)d=t*(w+1)\n",
        "\n",
        "# t(2d+t)\n",
        "# t((w+1)d + w)\n",
        "# d+t >= wd+w\n",
        "# d~64-512; w~32-256; t~128*4^3=8192=2^13\n",
        "# 2^9; 2^8\n",
        "\n",
        "# b,t,d = 64,100,512\n",
        "\n",
        "# mask = torch.rand(b,t, device=device)>.5\n",
        "# model = Attention(d, w=3).to(device) # 257 ms\n",
        "# x = torch.rand(b,t,d, device=device)\n",
        "# out = model(x, mask=mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU7NsUHTZ9dl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title MultiHeadAttention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0):\n",
        "        super().__init__()\n",
        "        # self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_model // n_heads\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=10000) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.q.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # print('att fwd', x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        # k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        k = self.pos_enc(k).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        v = v.unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "\n",
        "        # # (quadratic) attention # Softmax(q @ k.T) @ v\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask.unsqueeze(1) if mask != None else None, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # # out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # attn = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(~mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        # attn = torch.softmax(attn, dim=-1)\n",
        "        # # attn = F.sigmoid(attn-torch.log(attn.shape[-1])) # https://arxiv.org/pdf/2409.04431\n",
        "        # out = self.drop(attn) @ v # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1,2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygkv7B71JHP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e28f96e-ea13-45c9-c672-0191783429ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 16])\n"
          ]
        }
      ],
      "source": [
        "# @title Attention\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "        # torch.nn.init.normal_(self.lin0.weight, std=.02)\n",
        "        torch.nn.init.normal_(self.lin0.weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "# 2048*2\n",
        "# 2048*7\n",
        "# ff: d_model*2 *ff_dim params\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, d_head, ff_dim=None, drop=0.):\n",
        "    def __init__(self, d_model, n_heads ,cond_dim=None, ff_dim=None, drop=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.cond_dim = cond_dim\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        if cond_dim!=None: self.norm2 = nn.RMSNorm(cond_dim)\n",
        "        # self.drop = nn.Dropout(drop)\n",
        "        self.attn = Attention(d_model, cond_dim or d_model, n_heads=n_heads, d_head=d_model//n_heads, drop=drop)\n",
        "        act = nn.SiLU() # GELU SiLU ReLU\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # torch.nn.init.normal_(self.ff[1].weight, std=.02)\n",
        "        torch.nn.init.normal_(self.ff[1].weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # print('attblk fwd', x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        # if self.cond_dim==None: x = x + self.attn(self.norm1(x), mask=mask)\n",
        "        # else: x = x + self.attn(self.norm1(x), self.norm2(cond), mask) # maybe no res for decoder\n",
        "        x = self.attn(self.norm1(x), self.norm2(cond)if cond!=None else self.norm1(x), mask) #\n",
        "        x = x + self.ff(x) # maybe no ff for decoder?\n",
        "        return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "b,t,d = 2,5,16\n",
        "# b,t,d = 2,5,256\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "mask = torch.rand(b,t,t, device=device)>0\n",
        "model = AttentionBlock(d_model=d, n_heads=4, ff_dim=16).to(device)\n",
        "# model = nn.Sequential(*[AttentionBlock(d_model=d, n_heads=4, ff_dim=16) for _ in range(2)])\n",
        "out =  model(x, mask=mask)\n",
        "# out =  model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n",
        "# # model = MultiHeadAttention(d).to(device) # 257 ms\n",
        "# x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "# # out = model(x)\n",
        "# out = model(x, mask=mask)\n",
        "# print(out.shape)\n",
        "# print(time.time()-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, i):\n",
        "        super().__init__()\n",
        "        self.i=i\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        print('attblk fwd', x, cond)\n",
        "        x=x+self.i+1\n",
        "        print(x)\n",
        "        return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "n_layers = 2\n",
        "enc = Seq(*[AttentionBlock(i) for i in range(n_layers)])\n",
        "x=3\n",
        "c=2\n",
        "out = enc(x)\n",
        "out = enc(x,c)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66xs0VNW2uEc",
        "outputId": "71fad1cc-1856-4ee4-a679-dc0f0954893c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attblk fwd 3 None\n",
            "4\n",
            "attblk fwd 4 None\n",
            "6\n",
            "attblk fwd 3 2\n",
            "4\n",
            "attblk fwd 4 2\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J73LuO9XUcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c9c071-5d57-4413-ae26-68343248d016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1644800\n"
          ]
        }
      ],
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        # self.d_model = d_model\n",
        "        # self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim+1, d_model)\n",
        "        # self.tok_emb = TTEmbedding([29, 1733], [64,1], rank=min(d_model,256))\n",
        "        # self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads) for _ in range(n_layers)])\n",
        "        # self.out = nn.Linear(d_model, out_dim)\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight[:-1].T # [vocab+1,d]->[d,vocab] # weight tying for nn.Embedding\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        # x = self.pos_enc(x)\n",
        "        x = self.tok_emb(x)\n",
        "        # print('gpt fwd', x.shape)\n",
        "        b,t,d = x.shape\n",
        "\n",
        "        causal_mask = torch.tril(torch.ones((b,t,t), dtype=bool, device=device)) # [b,q,cond]\n",
        "        if mask!=None: causal_mask = causal_mask & mask.unsqueeze(-2) & mask.unsqueeze(-1) # [b,t,t]\n",
        "\n",
        "        x = self.encoder(x, mask=causal_mask) # [b,t,d], [b,t,t]\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "\n",
        "# attnnores improve fast but slows down, gets overtaken\n",
        "# sigmoid ~< softmax\n",
        "\n",
        "\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# d1024 12lyr h16, 768 8 12, 512 6 8, 256 4 8, , 64 1 8\n",
        "# 768 16 12, 768 12 18\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "# model = GPT(vocab_size, d_model=512, out_dim=vocab_size, n_layers=3, n_heads=16).to(device)\n",
        "model = GPT(vocab_size, d_model=256, out_dim=vocab_size, n_layers=2, n_heads=8).to(device)\n",
        "# model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=1, n_heads=8).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3) # og betas=(0.9, 0.999), wd.01\n",
        "# optim = torch.optim.AdamW(model.parameters(), 1e-3, betas=(0.9, 0.95), weight_decay=1e-1) # og wd.01\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3, weight_decay=1e-1) # og wd.01\n",
        "\n",
        "# x = torch.randint(0, vocab_size, (64, 128), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# strain 5.660660743713379\n",
        "# this is what we've done it. I'm able to be something strong. I think their information can be highly specific to a busy intertwined. But why \"Effects\"/ who shouldn't do,\" suppose it would improve them.Earlier open in both countries is sent by documentation, who were involved by custom ways to cutting the carbon gas\n",
        "# strain 5.939525604248047\n",
        "# this is what I’d like he’s very important to learn, it’s a bug my liquid had nothing once his streaming user in their own. It’s enough to run the survey, if there’s also no very big interested in a very $2. It’s sufficient's\n",
        "# strain 5.96616792678833\n",
        "# this is what manyatform are neither even documented and looked down their intentions in writers in so far savigTS-ve nuite was never known that some dioxide the sizes celebrate was fighting, as guilty or innovation.672 Expressia lives to Islamic Cheney of your existence takes a free background rest: The book she probably read it or find\n",
        "# strain 5.811507225036621\n",
        "# this is what it has not seen in intelligence and have believed in a romantic order and possible and rapidly now. Many growing economic ear is unclear whether they can do that the country's job. According to a proposals, the majority of domestic trade on which are the greatest mission with rich law by people, violated stockrol, which they would\n",
        "\n",
        "\n",
        "# b64,l128 8.0ram,10.0gpu l128*2:oom\n",
        "\n",
        "# b64,l128 w64 8.2ram,6.8gpu\n",
        "# b64,l128*2 w64 8.1ram,14.0gpu\n",
        "\n",
        "# gpt 1lyr mha ropein100 @attn # low loss but not learning\n",
        "# gpt 1lyr mha ropeout10000 F.attn # 19.3s\n",
        "# gpt 1lyr mha ropein10000 @attn # not learning\n",
        "# gpt 1lyr mha ropein10000 F.attn # this is what you are something else all in course, you're simply went back over their news, the reaction of the French source that is proven as we did not change any life at each power because they have the which shall noteded in order to change.\n",
        "\n",
        "# gpt 1lyr mha ropein10000 @attn fix~mask # 18.5s 7.3,5.9\n",
        "\n",
        "# swa64 128*2 7.4,12.0\n",
        "\n",
        "# d256 6.9,6.7\n",
        "\n",
        "# dim128 3lyr 3.0,.7\n",
        "# this is whats more of ietre'soni, they asari, your very\n",
        "# they vate. This iat\n",
        "# 9600 time: 5.7726781368255615 0.0006012580827081866\n",
        "# strain 1.481871247291565\n",
        "# this is what line wasn't good to both feel\n",
        "# like a delieve able or so foll,\n",
        "\n",
        "# mha\n",
        "# seq128*4+1 3.0,4.3\n",
        "# seq128*8+1 oom\n",
        "\n",
        "# swa\n",
        "# seq128*4+1 w64: 2.4,9.1 crash? w32:2.5,3.8\n",
        "# this is whatusanintan]\n",
        "#  dacrstan mowainind Butun\n",
        "#  an����������������������\n",
        "# 1500 time: 39.80479311943054 0.026518849830957827\n",
        "# strain 2.2812118530273438\n",
        "# this is whathe oue suses\n",
        "# catedeintilly,27), afevonees, Whe theesereceempone\n",
        "\n",
        "# seq128*8+1 w64:oom w32:2.4,7.8\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gpt-bert\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim+1, d_model) # +1 for msk_idx\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight.T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight[:-1].T  # weight tying\n",
        "        # print(self.tok_emb.weight.shape) # [257, 64]\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        # x = self.pos_enc(self.tok_emb(x))\n",
        "        x = self.tok_emb(x)\n",
        "        b,t,d = x.shape\n",
        "\n",
        "        # causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "        # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "\n",
        "        # x = self.encoder(x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.encoder(x, x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    def bert_loss(self, x, mask_ratio=.15, m=32): # [b,t]\n",
        "        b,t = x.shape\n",
        "        mask = (torch.rand(m,t) < mask_ratio).repeat(b,1) # [b*m,t]\n",
        "        x = x.repeat_interleave(m, dim=0) # [b*m,t]\n",
        "        y = x[mask]\n",
        "        x[mask] = msk_idx\n",
        "        y_ = model(torch.cat([torch.full((b*m,1), msk_idx, device=device), x], dim=-1))[:,:-1] #output = [batch size, trg len - 1, output dim]\n",
        "        loss = F.cross_entropy(y_[mask], y.to(int)) # [b*t,d], [b*t]\n",
        "        return loss\n",
        "\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "msk_idx = 256\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "# model = GPT(vocab_size, d_model=128, out_dim=vocab_size, n_layers=3).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# x = torch.randint(0, vocab_size, (64, 128), device=device)\n",
        "# # out = model(x)\n",
        "# out = model.bert_loss(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "out_dim=train_data.num_classes\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(vocab_size, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3) # 1e-3\n",
        "\n"
      ],
      "metadata": {
        "id": "ChkMXlzar_R_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gpt-bert\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim+1, d_model) # +1 for msk_idx\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        self.decoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight.T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight[:-1].T  # weight tying\n",
        "        # print(self.tok_emb.weight.shape) # [257, 64]\n",
        "\n",
        "\n",
        "    def forward(self, x, context_indices, trg_indices): # [batch, seq_len, d_model], [batch, seq_len] # True will be ignored by the attention # https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        # x = self.pos_enc(self.tok_emb(x))\n",
        "        x = self.tok_emb(x)\n",
        "        b,t,d = x.shape\n",
        "\n",
        "        if context_indices != None: x = x[torch.arange(x.shape[0]).unsqueeze(-1), context_indices] # [batch, num_context_toks, d_model]\n",
        "\n",
        "        # causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "        # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "\n",
        "        # x = self.encoder(x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.encoder(x, x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    def bert_loss(self, x, mask_ratio=.15, m=32): # [b,t]\n",
        "        b,t = x.shape\n",
        "        mask = (torch.rand(m,t) < mask_ratio).repeat(b,1) # [b*m,t]\n",
        "        x = x.repeat_interleave(m, dim=0) # [b*m,t]\n",
        "        y = x[mask]\n",
        "        x[mask] = msk_idx\n",
        "        y_ = model(torch.cat([torch.full((b*m,1), msk_idx, device=device), x], dim=-1))[:,:-1] #output = [batch size, trg len - 1, output dim]\n",
        "        loss = F.cross_entropy(y_[mask], y.to(int)) # [b*t,d], [b*t]\n",
        "        return loss\n",
        "\n",
        "\n",
        "mae/ijepa enc:trans norm (lin if out!=dim); dec:trans norm lin\n",
        "\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "msk_idx = 256\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "# model = GPT(vocab_size, d_model=128, out_dim=vocab_size, n_layers=3).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# x = torch.randint(0, vocab_size, (64, 128), device=device)\n",
        "# # out = model(x)\n",
        "# out = model.bert_loss(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "out_dim=train_data.num_classes\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(vocab_size, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3) # 1e-3\n",
        "\n"
      ],
      "metadata": {
        "id": "nGklKm5iFnkN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNGU1V3XPFUk"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "e1393dfd-1e60-4ee3-94a2-9a7c712dbd29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>█▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train loss</td><td>0.05021</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ruby-water-85</strong> at: <a href='https://wandb.ai/bobdole/gpt/runs/htelrmd1' target=\"_blank\">https://wandb.ai/bobdole/gpt/runs/htelrmd1</a><br> View project at: <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">https://wandb.ai/bobdole/gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250806_145757-htelrmd1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250806_151052-lzpp2zwc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/gpt/runs/lzpp2zwc' target=\"_blank\">super-serenity-86</a></strong> to <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">https://wandb.ai/bobdole/gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/gpt/runs/lzpp2zwc' target=\"_blank\">https://wandb.ai/bobdole/gpt/runs/lzpp2zwc</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"gpt\", config={\"model\": \"res18\",}) #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "\n",
        "for i,(x,y) in enumerate(train_loader):\n",
        "    # print(x.shape, x)\n",
        "    print(i, x, y)\n",
        "    break\n",
        "# print(decode(x[3]))\n",
        "x = x[:0]\n",
        "\n",
        "mask_ratio=.25\n",
        "m=1\n",
        "mask = (torch.rand(m,t) < mask_ratio).repeat(b,1) # [b*m,t]\n",
        "x = x.repeat_interleave(m, dim=0) # [b*m,t]\n",
        "y = x#[mask]\n",
        "x[mask] = msk_idx\n",
        "output = model(torch.cat([torch.full((b*m,1), msk_idx, device=device), x], dim=-1))[:,:-1] #output = [batch size, trg len - 1, output dim]\n",
        "temperature=1\n",
        "output = output/temperature\n",
        "output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "# print(y)\n",
        "print(decode(y))\n",
        "x[mask] = ix\n",
        "# print(x)\n",
        "print(decode(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "uoe9IqyDnv0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title bert train test\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# # https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "# def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "#     log_probs = F.log_softmax(logits, dim=-1)\n",
        "#     nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "#     perplexity = nll.mean().exp()\n",
        "#     return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optim): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    start = begin = time.time()\n",
        "    for i, (x,_) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            b, t = x.shape\n",
        "            # print(x.shape)\n",
        "            loss = model.bert_loss(x)\n",
        "            # # loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(\"strain\",loss.item())\n",
        "            # print(generate(model, \"this is what\"))\n",
        "            # model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            start = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(x)})\n",
        "        except NameError: pass\n",
        "        if i>=500: break\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                # sx = model(x).detach()\n",
        "                sx = model(torch.cat([torch.full((x.shape[0],1), msk_idx, device=device), x], dim=-1))[:,:-1].mean(dim=-2).detach() # [b,t,d]->[b,d]\n",
        "            y_ = classifier(sx) # [b,vocab]\n",
        "            loss = F.cross_entropy(y_, y.flatten().to(int)) # [b,vocab], [b]\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        if i % 100 == 0:\n",
        "            print(\"classify\",loss.item()/len(y))\n",
        "        try: wandb.log({\"closs\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            # sx = model(x)\n",
        "            sx = model(torch.cat([torch.full((x.shape[0],1), msk_idx, device=device), x], dim=-1))[:,:-1].mean(dim=-2).detach() # [b,t,d]->[b,d]\n",
        "            y_ = classifier(sx)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y.flatten().to(int)) # [b,vocab], [b]\n",
        "        # acc = ((y.flatten()==y_.argmax(dim=1)).sum()/len(y)).item()\n",
        "        acc = ((y==y_.argmax(dim=1)).sum()/len(y)).item()\n",
        "        # print(y.shape, y_.shape, y_.argmax(dim=1).shape)\n",
        "        # print((y.flatten()==y_.argmax(dim=1)))\n",
        "        print\n",
        "        # if i>=100: break\n",
        "        if i % 100 == 0:\n",
        "            # print(\"test\",loss.item()/len(y))\n",
        "            print('acc', round(acc, 3), 'test_loss', round(loss.item()/len(y), 3))\n",
        "        try: wandb.log({\"correct\": acc})\n",
        "        # try: wandb.log({\"test loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "        # if i>=100: break\n",
        "        if i>=0: break\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=2000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "for i in range(10): #\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "\n",
        "    strain(model, train_loader, optim)\n",
        "    ctrain(model, classifier, train_loader, coptim)\n",
        "    test(model, classifier, test_loader)\n"
      ],
      "metadata": {
        "id": "xowu3wA9ms-q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title scheduler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import math\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "# total_steps=100\n",
        "# base_lr, max_lr = 3e-5, 3e-4\n",
        "\n",
        "# import torch\n",
        "# model=torch.nn.Linear(2,3)\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.999))\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=20 , num_training_steps=total_steps) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# # scheduler = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=max_lr, total_steps=total_steps, pct_start=0.45, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=div_factor, final_div_factor=100.0, three_phase=True,)\n",
        "\n",
        "# lr_lst=[]\n",
        "# import matplotlib.pyplot as plt\n",
        "# for t in range(total_steps):\n",
        "#     lr=optim.param_groups[0][\"lr\"]\n",
        "#     lr_lst.append(lr)\n",
        "#     scheduler.step()\n",
        "# plt.plot(lr_lst)\n",
        "\n",
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v6DaHoS2M2_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim.param_groups[0][\"lr\"] = 1e-3"
      ],
      "metadata": {
        "id": "grtjw7xPk0is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCN055Zr7Dq4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d900a97-9023-4f4a-902e-6a8a001dccca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 time: 0.10722780227661133 0.10722827911376953\n",
            "strain 5.53125\n",
            "this is what                                                                \n",
            "100 time: 6.49582839012146 0.06431513965719997\n",
            "strain 3.5779857635498047\n",
            "this is what seeees  ete e e eee eeeeet eeeee t eeeeete eee   eeeee      e s\n",
            "200 time: 6.609358310699463 0.03288238202754538\n",
            "strain 3.6150054931640625\n",
            "this is whatttnthtttttttttttttttttttttttttttttttttttttttttthtthttttttttttttt\n",
            "300 time: 6.5281007289886475 0.021688043873175435\n",
            "strain 3.2951087951660156\n",
            "this is whatttttotsttttttotttttttotottttttttttttttttttottttststtottttttotost\n",
            "400 time: 6.624863386154175 0.016520857513694098\n",
            "strain 3.1639251708984375\n",
            "this is what     o    t e       o a          tt      o   e    so   tt  t    \n",
            "500 time: 6.569174528121948 0.013112126234286798\n",
            "strain 3.160106658935547\n",
            "this is whatatt aottaoanottattttttotattttotatatttaaeta aetatttttttttttattoot\n",
            "600 time: 6.6285412311553955 0.011029187534097427\n",
            "strain 3.259089469909668\n",
            "this is what                             e                                  \n",
            "700 time: 6.568297386169434 0.009369897094160615\n",
            "strain 3.233912467956543\n",
            "this is what    t                                                     o     \n",
            "800 time: 6.6267616748809814 0.008273111002870863\n",
            "strain 3.34759521484375\n",
            "this is what        a     a               a a     a  a ae a    a   ea  a    \n",
            "900 time: 6.5367255210876465 0.007254968075852812\n",
            "strain 3.1963958740234375\n",
            "this is what                                        e                       \n",
            "1000 time: 6.597044467926025 0.006590454966633708\n",
            "strain 3.295339584350586\n",
            "this is what t teteeoteoeottoe ttt   et t tt o t t  ttoe t toto etettttet t \n",
            "1100 time: 6.534606218338013 0.005935155944754924\n",
            "strain 3.2427024841308594\n",
            "this is what                                                                \n",
            "1200 time: 6.570948839187622 0.005471231538389843\n",
            "strain 3.775177001953125\n",
            "this is whatooooaoooooooooooooooooooooooooooooooooaoooooooooooooooooooooooao\n",
            "1300 time: 6.639427900314331 0.005103327144942405\n",
            "strain 3.3278255462646484\n",
            "this is whateeeeeeee  eeeeee  eeeeeee eeeeeee eeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "1400 time: 6.508514642715454 0.004645621240521907\n",
            "strain 3.1905651092529297\n",
            "this is what e eee  eeeetee eeeeeee  e  eeee eeaeee  e  ee eeeee eee eee eee\n",
            "1500 time: 6.649338006973267 0.004429938871013888\n",
            "strain 3.103512763977051\n",
            "this is what          a               e          e  e e  e   e   eo e       \n",
            "1600 time: 6.515017509460449 0.00406934275320364\n",
            "strain 3.411602020263672\n",
            "this is what e    u ee  e a ea   a a e  ea     eeea            ee  e in  e e\n",
            "1700 time: 6.5793821811676025 0.0038679499685028455\n",
            "strain 3.239438056945801\n",
            "this is what oee e iooooeo oeooaoiieee oo oeoeieeoeeeieoi  i oieei ioeoeaete\n",
            "1800 time: 6.514698266983032 0.003617267486851855\n",
            "strain 3.206209182739258\n",
            "this is what  aee eeen eeee  e eeeee  eeeieeoaiei eeeece e e eeeo  eaeeaiee \n",
            "1900 time: 6.609225511550903 0.003476710146442957\n",
            "strain 3.1529159545898438\n",
            "this is whate a e ee ee eie i  eey oeoeaeooaeoeteoeae a   eeu eea eoe neeaen\n",
            "2000 time: 6.54286789894104 0.003269799407394691\n",
            "strain 3.2706546783447266\n",
            "this is what             e                                     e  a    e    \n",
            "2100 time: 6.562968730926514 0.0031237359389641463\n",
            "strain 3.1349010467529297\n",
            "this is what e                           e     \n",
            "                        e   \n",
            "2200 time: 6.499335527420044 0.002952901410384917\n",
            "strain 3.1465530395507812\n",
            "this is what        e    a ea     oe  e   ga    a  n  a   o      ee      aa \n",
            "2300 time: 6.57107949256897 0.002855749660758856\n",
            "strain 3.3446502685546875\n",
            "this is whatd t e e     tnaana te  oe e t et nc  ita aa ee   eeoa t  se  a  \n",
            "2400 time: 6.521494626998901 0.0027161578941424657\n",
            "strain 3.1681785583496094\n",
            "this is what Ss m te o  r Fkh   fteo t r   Hd  s e  clil-a fdh tmaona f s � \n",
            "2500 time: 6.555123567581177 0.002621001217280422\n",
            "strain 3.0831661224365234\n",
            "this is what toto uet  tgocg,toteooo tllc wyoe oecaooscsolooon oeac fgtoeos \n",
            "2600 time: 6.601896524429321 0.002538215009490603\n",
            "strain 3.132312774658203\n",
            "this is what a sa D  aearonovveaiaaa aneaa aaamconragaaai oiaiahenaaeaoaaaat\n",
            "2700 time: 6.530727386474609 0.0024178925817518577\n",
            "strain 3.5925912857055664\n",
            "this is whatt  t   to seg   dlh \n",
            "te ot�o ts   otetehen t r tt  ' thtttt eo  \n",
            "2800 time: 6.664326906204224 0.0023792671842346952\n",
            "strain 3.249929428100586\n",
            "this is whatfoobn otgaebn th,lt  r feum leeeao  fso f5alesa ei a osm  .t si \n",
            "2900 time: 6.510281085968018 0.0022441508317149867\n",
            "strain 3.2347335815429688\n",
            "this is whatt  a ei setW  o,te  o    teo  c d r    tas n t   f  ctrstttcots \n",
            "3000 time: 6.586654901504517 0.0021948201066054967\n",
            "strain 3.2108688354492188\n",
            "this is what                         a        e  a n ee.    i               \n",
            "3100 time: 6.512629985809326 0.002100170984917093\n",
            "strain 3.2834253311157227\n",
            "this is what                           e                                  e \n",
            "3200 time: 6.6998796463012695 0.0020930583832301933\n",
            "strain 3.1598997116088867\n",
            "this is what  noaoo ooa\n",
            "on i  o  o loib,e mnabc oe le oao  o c ceaoeo  ioooo\n",
            "3300 time: 6.537987470626831 0.001980608284599815\n",
            "strain 3.2259750366210938\n",
            "this is whatreantthoainrseaoacat bianttsterig w,ss taauassaitaaaiaitatoetaca\n",
            "3400 time: 6.567583084106445 0.0019310742607890632\n",
            "strain 3.105523109436035\n",
            "this is what   t   t t    t  tt  t           t   n     t      t             \n",
            "3500 time: 6.545636177062988 0.0018696477160525982\n",
            "strain 3.3443870544433594\n",
            "this is whatglcc,acccmeaarcaacccalc cceiccceaccceecmacccccvaae ccledcaaoearc\n",
            "3600 time: 6.627145528793335 0.001840362612388757\n",
            "strain 3.0407943725585938\n",
            "this is what   p                                      r       s             \n",
            "3700 time: 6.605048418045044 0.0017846660722239344\n",
            "strain 3.1727523803710938\n",
            "this is what                                               h          rr   e\n",
            "3800 time: 6.557515621185303 0.0017252080661941283\n",
            "strain 3.183534622192383\n",
            "this is what        it e       ttst t    tst  t            h  te   t  t    t\n",
            "3900 time: 6.538412809371948 0.001676086461473752\n",
            "strain 3.729947090148926\n",
            "this is whats s ,e inss  s e ae stit a  as   tss  ar st    ss    tdc   t    \n",
            "4000 time: 6.597867965698242 0.0016490548469221195\n",
            "strain 3.240114212036133\n",
            "this is whateaeeeeeeeeiebreieeageeeeeeeeeeeeeeaeeeeeeeeeeeeoaenereeieeeeee e\n",
            "4100 time: 6.56589937210083 0.0016010485460513453\n",
            "strain 3.121641159057617\n",
            "this is whatoeeooeoeeeeeooaeoeeoeoaoeeetloeeeooetooeoeooeeeeoeeoeooeeooeeese\n",
            "4200 time: 6.505690336227417 0.0015486052875659545\n",
            "strain 3.210590362548828\n",
            "this is whateeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "4300 time: 6.572726249694824 0.0015281856517352717\n",
            "strain 3.2192211151123047\n",
            "this is whateeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "4400 time: 6.53585958480835 0.0014850852760881815\n",
            "strain 3.3074417114257812\n",
            "this is whateeeeeeeeeeteeeteeeeeeeeeeeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeeeeeeeee\n",
            "4500 time: 6.5397984981536865 0.0014529657268545358\n",
            "strain 3.1483993530273438\n",
            "this is whatttttttetetettettttttttettttteettttteeteettettetteeeetetettttettt\n",
            "4600 time: 6.5603179931640625 0.0014258462743794392\n",
            "strain 3.1785736083984375\n",
            "this is whatbor bbwboeucusselbomfbwuwusogbbowbbuso,sobbwbbbbdosmbgsbbb ueoms\n",
            "4700 time: 6.569403171539307 0.0013974481277125004\n",
            "strain 3.2347002029418945\n",
            "this is whatsGtstacottn esa rtooaie.hu eohnlo.Dantanaeftndsttidtaa,agapoo  o\n",
            "4800 time: 6.504520416259766 0.0013548262142633503\n",
            "strain 3.118173599243164\n",
            "this is what. sdeoet dtme'ewo owele wa   densctc dihocuGitdtc clr awn\n",
            "laetm\n",
            "\n",
            "4900 time: 6.589832544326782 0.0013445894272077865\n",
            "strain 3.3217926025390625\n",
            "this is whatnatt,onlcotenoetotiiotii etaaotat� taosaanio,rtatsettotsatstta\n",
            "a\n",
            "5000 time: 6.510223865509033 0.001301784559241106\n",
            "strain 3.222050666809082\n",
            "this is whatdwb n5   ign s a Nathltete  hor     at   l  otc t a  l so e     \n",
            "5100 time: 6.565281867980957 0.0012870579461354036\n",
            "strain 3.192483901977539\n",
            "this is what nhpAogn srascsic  a  e e iyc  eu  aaa , o e oeib  su�eoso  ad h\n",
            "5200 time: 6.595111846923828 0.0012680469763047464\n",
            "strain 3.212839126586914\n",
            "this is whatsgiaa  an gddAeege l2 owhaerrs rbrt etc o s t n auiB dgeen  la  \n",
            "5300 time: 6.624224662780762 0.001249617977966297\n",
            "strain 3.19140625\n",
            "this is whateeeeeeeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "5400 time: 6.529727458953857 0.0012089849908889123\n",
            "strain 3.260669708251953\n",
            "this is whattteeoeeteeteeeneeeeeeeneenettsttoeeetnezeteoaeeeotoettateeoottEe\n",
            "5500 time: 6.570876598358154 0.0011944877431731163\n",
            "strain 3.1969051361083984\n",
            "this is whatoeh A    n o an f no o  s    n  isdansi Inonsanftur s olsee p cl\n",
            "5600 time: 6.610428333282471 0.0011802229194252901\n",
            "strain 3.143362045288086\n",
            "this is whato    losnosrhraadeasnhneaaaaease ssitho l syrha  sa do a so2ah  \n",
            "5700 time: 6.505427598953247 0.001141102935865958\n",
            "strain 3.153238296508789\n",
            "this is whatnCa io ms gt  unfboaaCl eescet \n",
            " eins foxde  tess   a   nos  ysw\n",
            "5800 time: 6.573957443237305 0.0011332457157234306\n",
            "strain 3.067075729370117\n",
            "this is whateks nehhte.eot ehh  ec anochnw nheaem\n",
            "snteeWTedpa aie hsntnuiaea\n",
            "5900 time: 6.510100841522217 0.0011032200166682553\n",
            "strain 3.137380599975586\n",
            "this is whatpopt e it e nof   aeat .mIinnp    e     hetdanfi re ,  d n  d nn\n",
            "6000 time: 6.647315263748169 0.0011077013398742422\n",
            "strain 3.1533775329589844\n",
            "this is whatth em haose�u  tetn h�gt oau nliw cdsa tchaylil ,iajr,gadynsitsu\n",
            "6100 time: 6.510916471481323 0.001067188446139961\n",
            "strain 3.227529525756836\n",
            "this is whatI eeh teameeae,eet edftthnepeiutwseuetst  edpeolwFtittHesBt o te\n",
            "6200 time: 6.575728178024292 0.0010604303587907515\n",
            "strain 3.2879867553710938\n",
            "this is what eui t :  n eiuan   sr.e fao i   Dofoyae   teaaBhh�te  na)i  va\n",
            "\n",
            "6300 time: 6.61126971244812 0.001049241382457816\n",
            "strain 3.2592315673828125\n",
            "this is whato\n",
            "\n",
            "\n",
            "p\n",
            "\n",
            "\n",
            "\n",
            "o\n",
            "\n",
            "\n",
            "\n",
            "o\n",
            "\n",
            "\n",
            "oop\n",
            "\n",
            "noo\n",
            "o\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "o\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "o\n",
            "\n",
            "\n",
            "\n",
            "noo\n",
            "\n",
            "o\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "o\n",
            "o\n",
            "\n",
            "6400 time: 6.564179182052612 0.0010254928011984364\n",
            "strain 3.3157644271850586\n",
            "this is whatrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n",
            "rrrrrrrrrrr.rrrrrrrrrrrrrrrr\n",
            "6500 time: 6.526081323623657 0.0010038581816458076\n",
            "strain 3.153622627258301\n",
            "this is whatrw\n",
            "hrrrr,r\n",
            ".\n",
            "rmr,rr\n",
            ".r\n",
            "rrrrr.\n",
            "r\n",
            "\n",
            "rr\n",
            "rrrr.n\n",
            "rrrr\n",
            "rrrurrrrrrr\n",
            "r.\n",
            "\n",
            "\n",
            "6600 time: 6.565855979919434 0.0009946759912646589\n",
            "strain 3.2295093536376953\n",
            "this is whatarr\n",
            "rrarrrrrrrrrrrrrrrrrrr\n",
            "rrrrrrrrrrrrrrrrrrrrrrrrrrr\n",
            "rr\n",
            "rrrrrr\n",
            "6700 time: 6.570476770401001 0.0009805218993043066\n",
            "strain 3.1201181411743164\n",
            "this is what dsnuhetat racss  asam eanaagwucrraoityt  sh  sogesasaetuls ioe \n",
            "6800 time: 6.5336058139801025 0.000960683212650469\n",
            "strain 3.3246383666992188\n",
            "this is what   a ase na tts  fsiys  kcln  oas ,m sona t t hn t tyid  aii i l\n",
            "6900 time: 6.555944204330444 0.0009499991947180152\n",
            "strain 3.1794958114624023\n",
            "this is whatwi beooeoi e eessth ao sce  remfgao   e e  syantyt8  f   ih hSs \n",
            "7000 time: 6.514797687530518 0.0009305524819238818\n",
            "strain 3.223538398742676\n",
            "this is what n s  g  ptors r   lro  ,e eenhniete enhs ni ,  maosas \n",
            "hs  et e\n",
            "7100 time: 6.562959909439087 0.0009242304444833468\n",
            "strain 3.175046920776367\n",
            "this is whate nu yo anogoe pe i   o .t ualwne T ed ttf set see r i ise aauso\n",
            "7200 time: 6.5044310092926025 0.0009032678081002306\n",
            "strain 3.127560615539551\n",
            "this is whatnyse@e yaswBs e c eoeeaeaoLa ertn n eS ra2aet nvoeoepeiiaw ia rt\n",
            "7300 time: 6.557739973068237 0.0008981976014688364\n",
            "strain 3.112725257873535\n",
            "this is what amenehedenits \n",
            "l m i  fnraiehhebah cethcyod ttuda xeeicnna  ats\n",
            "7400 time: 6.536263465881348 0.000883159565290975\n",
            "strain 3.0862388610839844\n",
            "this is whatd 0ku  et b tolsesetc atesetbean \n",
            "Du o iInf-ipnaa n  eeo nhceai \n",
            "7500 time: 6.555580139160156 0.0008739608555630897\n",
            "strain 3.2211151123046875\n",
            "this is whats2c teioeea eV e  GsI  srnhrm Fcba ra\"niet  CrlYu pi mnbbsoaenm \n",
            "7600 time: 6.522311210632324 0.000858086031560066\n",
            "strain 3.209996223449707\n",
            "this is whatsu Mha n  tvainhc sdseryhc LTeo,k2wsnneeteevtrn,  eillhn6pl   ih\n",
            "7700 time: 6.619639158248901 0.000859581794139395\n",
            "strain 3.212005615234375\n",
            "this is whatimaeiidmhuranraaa errnegaveaaaer ncr rgaaaaapmednea\n",
            "raar eaaaern\n",
            "7800 time: 6.495426893234253 0.0008326403805268543\n",
            "strain 3.1614112854003906\n",
            "this is whatradrcrrrrrlrdr\n",
            "rrrardrrraamrmrurrdrrrrrrernrrrmprrrrrrrrrmerrrar\n",
            "7900 time: 6.564000844955444 0.0008307810811027215\n",
            "strain 3.1978111267089844\n",
            "this is whatdrmr mlrrdagrrmrmlrrrrrdrdlglldld\n",
            "darmm\n",
            "rrrpmlrrmr\n",
            "dmclmgdrrerrb\n",
            "8000 time: 6.524157285690308 0.0008154177930917968\n",
            "strain 3.3823280334472656\n",
            "this is whatrrrfdbmluuemrmrrlgmrrdwmrmrrdlrrarelsccmmrdrrrdrdrrdmmrdrrreumr.\n",
            "8100 time: 6.599292755126953 0.0008146269588378635\n",
            "strain 3.2521419525146484\n",
            "this is whatemmmmrldlrerrmrmmrrlrd,mmmplarmlrmmrmdadmm,mmrpcmrremmrm,reg lmm\n",
            "8200 time: 6.528982877731323 0.0007961203944114719\n",
            "strain 3.1355152130126953\n",
            "this is whatmrrhuru,mclrrrrrrlrarrrlrrmrmlmrrrldlrlrrrrlllcm,,rulddclrmrrlrr\n",
            "8300 time: 6.506979465484619 0.0007838790150467653\n",
            "strain 3.2261276245117188\n",
            "this is whatdrmmemmmmemlmemlmmrrdemcemd,memcmmeeecmrm,rblrlmbmmmrmmmrmlcemgl\n",
            "8400 time: 6.577775955200195 0.0007829754115030775\n",
            "strain 3.3155651092529297\n",
            "this is whaty,yyyymvyywywwyywywy,p\n",
            "wyyyymyy\n",
            "ywygyywyyyy\n",
            "\n",
            "yyywy\n",
            "yyywywwyywwyw\n",
            "8500 time: 6.518543243408203 0.0007667972850541538\n",
            "strain 3.2617292404174805\n",
            "this is whatnnnnnninnnnnnonnnnnnnnnnnnnnnnnnnnnnnhnnnnnnnnnnnnnnnnnnnnnnnnnn\n",
            "8600 time: 6.544675827026367 0.0007609203657068883\n",
            "strain 3.1184587478637695\n",
            "this is whatnnnnnnnnnnhnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnhnnnnnnhn\n",
            "8700 time: 6.5136799812316895 0.0007486128557716179\n",
            "strain 3.2863006591796875\n",
            "this is whatnnnnnnonnnnnnnnnnnnnnthnnnnnnnnninnnnnnnnnnnhnnnnnnnnnnnnnnnnnnn\n",
            "8800 time: 6.672634124755859 0.0007581677494801089\n",
            "strain 3.213237762451172\n",
            "this is whatnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnhnnnnnnnnnon\n",
            "8900 time: 6.517653226852417 0.0007322383401045945\n",
            "strain 3.1930007934570312\n",
            "this is whaty         y      y    y          y    n         n               \n",
            "9000 time: 6.560232639312744 0.0007288337826609624\n",
            "strain 3.3385658264160156\n",
            "this is what#,pyyn0myncJtT,yy�y0t1KffnLyvmtcBtytzdiybC0yt,nctnyjtytya0fytmvm\n",
            "9100 time: 6.515735864639282 0.0007159363082602396\n",
            "strain 3.2193965911865234\n",
            "this is what                 n                y                 p           \n",
            "9200 time: 6.544918775558472 0.0007113269484181752\n",
            "strain 3.1088619232177734\n",
            "this is what                                                    y           \n",
            "9300 time: 6.598558664321899 0.0007094462037586343\n",
            "strain 3.2318649291992188\n",
            "this is what  hn n ny   n   yn y   y  n  n ynn  n    yn  nn    nn   n  nn   \n",
            "9400 time: 6.537681341171265 0.000695424084459996\n",
            "strain 3.1157569885253906\n",
            "this is whatnnnnnnnhnnnonnnnnnnnnnnnnnnnnnnnannnnnnnnnnntnnnnnnnnonnnnnnnnan\n",
            "9500 time: 6.523203611373901 0.0006865807902548215\n",
            "strain 3.1558780670166016\n",
            "this is whatnnnnnnnnnnonnnnnlhnntnnnnnnnnnnnnnnnnnnhnnnnnnnnnnnthntnnhnnnnnn\n",
            "9600 time: 6.565822601318359 0.0006838686428223038\n",
            "strain 3.214559555053711\n",
            "this is whatnhttmrnhhcctythitmnhtnthnhinttmimhinpnhitntnnhncntccntnnhhnhhtmn\n",
            "9700 time: 6.568698167800903 0.0006771155969713929\n",
            "strain 3.1351757049560547\n",
            "this is whatnnnnnntnonnnnnnnnnnhnnnnntnnnnnhhoonnnionhnnnnnnhnnnninnnnhnntot\n",
            "9800 time: 6.55126690864563 0.0006684284893277591\n",
            "strain 3.171746253967285\n",
            "this is whattnnnnhnnhnnnnnnnnnnnnnnnnnnnnnnnnnnsnnnnnhntnntnnnnnhnnnnnnnnnnh\n",
            "9900 time: 6.531937599182129 0.0006597251100331145\n",
            "strain 3.212923049926758\n",
            "this is whatnhnnhhnhnnnnhnnnnnnnnnooonnnnnnnnnnnnntnnninnnininnmitnnnnhnninn\n",
            "10000 time: 6.495273590087891 0.0006494624604464602\n",
            "strain 3.1350746154785156\n",
            "this is whatcgimremcmicicghclh tdhtcct.ccUtfnpcbtThetvc wtbmylhcgcce hscdlor\n",
            "10100 time: 6.5419042110443115 0.0006476492117494773\n",
            "strain 3.216733932495117\n",
            "this is whattcymTaen,i s letyatihn   sgtthetastgln fci llieh r taottpos5nses\n",
            "10200 time: 6.55912971496582 0.0006429889414570119\n",
            "strain 3.339278221130371\n",
            "this is whaty1fyct4m8tfrftWtfmyttTty\n",
            "yh1hnc@yy$otb�tAyr_y0pWmAfym_0fyhy2smA_\n",
            "10300 time: 6.57905387878418 0.0006386811103002387\n",
            "strain 3.732851028442383\n",
            "this is whattbtynhthnAnlmdfchmftitrshthgtniEhhcnhtfi=wtyvhfhhtcycimctthstns�\n",
            "10400 time: 6.480102062225342 0.0006230268761717624\n",
            "strain 3.1678428649902344\n",
            "this is whatniynntnnynynanyanhopnnhnndBhnynnnnhnyonntnnnhontnpmhbhnnnhhnthno\n",
            "10500 time: 6.570485591888428 0.0006257009646992674\n",
            "strain 3.1466598510742188\n",
            "this is whatanhhtahnAttlgtimygthynthmnsrnttmhlyp aialat�naytindiW0(fhimtthnf\n",
            "10600 time: 6.51388955116272 0.0006144599592491159\n",
            "strain 3.1858768463134766\n",
            "this is whatcttyopcrtmRztxse?ow whb0tdtmctnyahvttbyncZyot wbbreyn,cteBWThhyo\n",
            "10700 time: 6.600830554962158 0.0006168424476029638\n",
            "strain 3.1444835662841797\n",
            "this is whattCcnrmmytoni6dimchctvpp5dbyoft\n",
            "5mathhg? hch0gmdM�m\n",
            "thi tTvthAn2t\n",
            "10800 time: 6.523001432418823 0.0006039257392145155\n",
            "strain 3.253999710083008\n",
            "this is what m\n",
            "bmgmSyca ty�fpwp dsbtpt[ltyeckrbtssm0nfvpmfcdd�ykt,Lfemmhnbrn\n",
            "10900 time: 6.529433250427246 0.0005989756652843229\n",
            "strain 3.179943084716797\n",
            "this is whathftmtmfttek U\n",
            "ha1sbgsnticttfhtrhtcftitwdm ytittnhyyhtthiearipteh\n",
            "11000 time: 6.6026177406311035 0.0006001834576373295\n",
            "strain 3.407014846801758\n",
            "this is whatybcmhstielacboe btnhthyvh1ttnautaenl tPg hito.mcbs Tttphikth otm\n",
            "11100 time: 6.495995283126831 0.0005851721035533204\n",
            "strain 3.131380081176758\n",
            "this is whatthtmrtcechsi\n",
            "site yitnsahste taear tft tstotdetcnlghtnioiten otc\n",
            "11200 time: 6.530468702316284 0.0005830255707144024\n",
            "strain 3.210012435913086\n",
            "this is whata  togy hsahdhrt ttchte hnlhtthtd lhnintgee ht flseefsyhntslham \n",
            "11300 time: 6.503135919570923 0.0005754478715519053\n",
            "strain 3.119523048400879\n",
            "this is whatnic fniitehehthtet hgesbdeensainyeipacemcdthsitreosnt,hhhitcecty\n",
            "11400 time: 6.551709413528442 0.000574660985033383\n",
            "strain 3.5239486694335938\n",
            "this is whattkateeotytss ggctDgw\n",
            "kwnlntlnyittsep hbltfnhebhs t himltsaela tk\n",
            "11500 time: 6.49628758430481 0.0005648454762823571\n",
            "strain 3.1963462829589844\n",
            "this is whattnbtotnrofgam aiifiiacdBrr,et netiheoesoity isetnahen tndtniFn�l\n",
            "11600 time: 6.546419382095337 0.0005642979137445974\n",
            "strain 3.3475570678710938\n",
            "this is what a ee hst5etwhhie iht ctnetet rl s tssthsnefh 0tmenathhitgtTieei\n",
            "11700 time: 6.481703519821167 0.0005539444489067879\n",
            "strain 3.197053909301758\n",
            "this is whatrhecwpheteng ase dlienetnhogisanek f htJa euteisdemeioclttrl\n",
            " ci\n",
            "11800 time: 6.531919717788696 0.000553505651607987\n",
            "strain 3.2277297973632812\n",
            "this is whatw\n",
            "nyn\n",
            "ot el nlpindaRtsitdunm1fveeoyn vcohh he\n",
            "eaeLcesiillbwydtem\n",
            "11900 time: 6.485547780990601 0.0005449582604678396\n",
            "strain 3.217031478881836\n",
            "this is whatloaiuehnhslhehoi  lnhip ugs nigttlcrheliighei oobnta te i iesaev\n",
            "12000 time: 6.552290201187134 0.0005459787050750532\n",
            "strain 3.1724424362182617\n",
            "this is whatr rvFlgchhaen  get3etiholtmhesorgs lw inp\"fRm,rqttiwro as ncnmet\n",
            "12100 time: 6.522634506225586 0.0005390161956088541\n",
            "strain 3.264974594116211\n",
            "this is what ngllletb   oc nlth  nro  e mltrnmolslotiyiseeitaswtl  c\n",
            "e tmssi\n",
            "12200 time: 6.603060960769653 0.0005411901646740621\n",
            "strain 3.153186798095703\n",
            "this is whatreirtdhdctthelanstiseheg\n",
            " lmh rnsshhmsImibcsldtc sfur hr .fseifr\n",
            "12300 time: 6.507543325424194 0.0005290256109812155\n",
            "strain 3.201822280883789\n",
            "this is what iil tol\n",
            "uhtoe alene no meuitn naetngtrf on h   ienntdi-neB  se \n",
            "12400 time: 6.612457752227783 0.0005332197395892559\n",
            "strain 3.1537342071533203\n",
            "this is what e ddnrneo fwe e mriislrshe eadgswe tweht9enAo edny eeepaap3a9mk\n",
            "12500 time: 6.54962158203125 0.0005239278314094736\n",
            "strain 3.2219619750976562\n",
            "this is whatiesa usKEot iloe�rane1oetyislg ttbeiblvemesr adrlohysSn  5dii \n",
            " \n",
            "12600 time: 6.547617197036743 0.0005196109384537197\n",
            "strain 3.1788253784179688\n",
            "this is whatoanyr tcisdsita\n",
            "htlgcstuee ilns tebf.ean sn eehme ntkitaotd  cGi\n",
            "12700 time: 6.581211566925049 0.0005181648723535317\n",
            "strain 3.2018861770629883\n",
            "this is whattogwsflttsh  h hax\n",
            "eina .:ioo4earnt  ameg[mnhhgsha wf�,  rggopy \n",
            "12800 time: 6.510152578353882 0.0005085659570949505\n",
            "strain 3.118565559387207\n",
            "this is whateN�sa iyf iieo  ! b intohto oer� n\n",
            "aosdeccoefn ui*gieo e  glt gt\n",
            "12900 time: 6.541152238845825 0.0005070268160686429\n",
            "strain 3.217034339904785\n",
            "this is whatinhn ndteayrl h)ka.benAesgsp  suoeachaIottaeodtooswbeadLheheFaei\n",
            "13000 time: 6.512030839920044 0.0005008869378000633\n",
            "strain 3.1429996490478516\n",
            "this is what h5eMzo uoeMevaf ni  aerDtta  y 'yo1n roiaemdvd rt  nuc eiY  taF\n",
            "13100 time: 6.604809284210205 0.0005041454668382081\n",
            "strain 3.1228256225585938\n",
            "this is what r adtntmTd iH0 csoa�tl  umoaiaayRtmont.s.�ohstiiCapesew,mi hoai\n",
            "13200 time: 6.517181396484375 0.000493688480789558\n",
            "strain 3.2159957885742188\n",
            "this is whatdydalcot utni gent eoietri unse.mth�d t latahiecJ'nel   n-nBdrep\n",
            "13300 time: 6.55214524269104 0.0004926055152204178\n",
            "strain 3.2168045043945312\n",
            "this is what rnset s seugf eytseelr.hi saeogbphnre et Atn�oaa os satn ooipra\n",
            "13400 time: 6.497008562088013 0.0004848152405734774\n",
            "strain 3.2689266204833984\n",
            "this is what   a ntdosee  aeireM apt \"nhthaai mnehuhas cm iAeeep,igsuda  ted\n",
            "13500 time: 6.581240177154541 0.00048746319931795417\n",
            "strain 3.4008331298828125\n",
            "this is whatae htitisa  t�st  gee noh ewdeaa vkfi  . rceTeraoaeytl O o sihrh\n",
            "13600 time: 6.512883901596069 0.000478853347432779\n",
            "strain 3.150149345397949\n",
            "this is what ne bMrteorntaA uii ta�aeyod yromtortrr ocinhhsoereenoroytn[oe ,\n",
            "13700 time: 6.639065504074097 0.00048456798634488395\n",
            "strain 3.1836204528808594\n",
            "this is whatirs.ccealm ctcc1.neer.eoatmuesin\n",
            "na iwniy ioftnbael6lemtrwa seir\n",
            "13800 time: 6.558981895446777 0.0004752541389960101\n",
            "strain 3.226865768432617\n",
            "this is whatvwoo  aeI et �c so u ,ppaass apn s \n",
            "honl  -eortinmhi ydtngeniam \n",
            "13900 time: 6.56641960144043 0.0004723703730030031\n",
            "strain 3.154910087585449\n",
            "this is whatoih h scpia pnitfevudf,ieateaosilaye rwaeiwebe emShnb,mebbrsniro\n",
            "14000 time: 6.666159152984619 0.0004761202847410139\n",
            "strain 3.1758031845092773\n",
            "this is whatty ms srblcdS vaw0 aroven t screteprttloieu2 a  diebte nbutteott\n",
            "14100 time: 6.582488059997559 0.00046681005154490583\n",
            "strain 3.187624931335449\n",
            "this is whatifm  s thfnuhav yal�  nvlt se  e afe1ssc eiwwmityteha irs de yvn\n",
            "14200 time: 6.574798822402954 0.00046298144762049796\n",
            "strain 3.250345230102539\n",
            "this is what  w. a edtio  seo  tiehe  efureeaytwe'  ioeai  h ts-�sehti  ohb \n",
            "14300 time: 6.536018371582031 0.0004570323115053331\n",
            "strain 3.1458511352539062\n",
            "this is what hrahc� A S zsmdmsMirtcrcenep lfvovPlot nraenn paa rs otS\" e Etf\n",
            "14400 time: 6.577295780181885 0.00045672496750357914\n",
            "strain 3.298826217651367\n",
            "this is whatnclaaaabehtieoafHo'taatgtl\n",
            "ltntaAabrnvi,c bee panta\n",
            " ers�i's,Mtd\n",
            "14500 time: 6.5945470333099365 0.00045476500046400355\n",
            "strain 3.2338027954101562\n",
            "this is whatntRhirti h�y.aaw � nr ldca nsnivn�a  nliinonlstdatdte awlistaa e\n",
            "14600 time: 6.6368067264556885 0.00045454468631424337\n",
            "strain 3.165599822998047\n",
            "this is whatei ih hls uasnT li[  lnacelnby I\n",
            "  Bi I,vmoaooloihoaitne asip\n",
            "at\n",
            "14700 time: 6.521868467330933 0.00044363437481586903\n",
            "strain 3.1675519943237305\n",
            "this is whatebddnW,neac oU aahso' dcdrn iriac\n",
            "votkii ndi,rfnvmetnehsnhir ehS\n",
            "14800 time: 6.544987201690674 0.0004421990188857396\n",
            "strain 3.2745513916015625\n",
            "this is whatrgbnr ieie tiooweicilus unu nr lrrugdegtahtect ya fss0mc,X dg ed\n",
            "14900 time: 6.519275665283203 0.00043750598073669686\n",
            "strain 3.1547679901123047\n",
            "this is what� aod ny a  mies�doi es rnaer\n",
            "mimcoow tust�r�seoa  tvtnlwheyMrun\n",
            "15000 time: 6.571475267410278 0.0004380691942314523\n",
            "strain 3.2146425247192383\n",
            "this is whatwi\n",
            "s elrez :  Paaa tea entl hee mdg tllmtaCeestoMmtyrgoloro dti \n",
            "15100 time: 6.541510105133057 0.00043318391785654166\n",
            "strain 3.509309768676758\n",
            "this is whate� hhseIie lie h e a.evce.oaI mih f.pbpxls.sz taf o- ta trdop t \n",
            "15200 time: 6.555490493774414 0.00043125391304473345\n",
            "strain 3.244447708129883\n",
            "this is whate ea entasMoeu  oear�ei2lab�l-rmsi[ht    olot\n",
            " or'mwmofarbtmS A.\n",
            "15300 time: 6.588482141494751 0.0004305916357317763\n",
            "strain 3.269376754760742\n",
            "this is whattw eo. eudnk n iitd \"txJ BstiaepC srio ee3�eoe aa 6 sshrheufbbeh\n",
            "15400 time: 6.521922826766968 0.00042347401490839073\n",
            "strain 3.27703857421875\n",
            "this is whattChy.ohafoos ferQtB  anoleetouSPsd sheeita deons m ,sgtetauv'aDy\n",
            "15500 time: 6.618649005889893 0.00042698209681485394\n",
            "strain 3.2121686935424805\n",
            "this is whaten  asCrenBae w aedtcnlrrvg l� oees mg i lplop%tshtr tt alonsBdh\n",
            "15600 time: 6.596472263336182 0.0004228237125936376\n",
            "strain 3.1520490646362305\n",
            "this is whatnhiO9n si nfaet,vAorniherricns ot  gre esd sa ah t�rla)hnrnder�b\n",
            "15700 time: 6.596319198608398 0.00042012100591453636\n",
            "strain 3.0403757095336914\n",
            "this is whatoid c[ naovrp.y+boe y merar  e t pait aehstradriletn a� wee dhoe\n",
            "15800 time: 6.500406265258789 0.00041139209566972774\n",
            "strain 3.3203134536743164\n",
            "this is whato .anglSfaiilnt lBo tvl g aanoihlc2rastvolwi,oc ao oit0t votnhth\n",
            "15900 time: 6.560619592666626 0.0004125916652728624\n",
            "strain 3.174470901489258\n",
            "this is whatrce eeo  a u yem ei  ewg t mt. ri  ee afertett s oe nscmryo    e\n",
            "16000 time: 6.540785074234009 0.00040877354859516075\n",
            "strain 3.2850341796875\n",
            "this is what etr  heesf  dt slshvi clt osef  eeh rdtt aa ah oeuperta,et u d \n",
            "16100 time: 6.5893394947052 0.0004092503701609178\n",
            "strain 3.1848812103271484\n",
            "this is what t eogunh segres     p pha ltssu pt e yipa i   .gin  n t r.telh \n",
            "16200 time: 6.537113904953003 0.0004035006716739807\n",
            "strain 3.226167678833008\n",
            "this is whatact h i  ltteae ili,yy horls inee   ka r engsnoietrteynthesln  t\n",
            "16300 time: 6.5503997802734375 0.0004018403790376061\n",
            "strain 3.2808303833007812\n",
            "this is whatte t.enei hhpei  ne le ee eetao e s  wieeel eh ,i ilts.  es h  a\n",
            "16400 time: 6.54521632194519 0.0003990742805694474\n",
            "strain 3.212068557739258\n",
            "this is whatl roe fuhwltrs y   n ygn tmt p luo oyaaleapnnws s h c moinyrec.y\n",
            "16500 time: 6.546375751495361 0.00039672603044255014\n",
            "strain 3.525808334350586\n",
            "this is whatns rnsicgnl y nruusohw lnmypiohpmsfr vu ieiys oseposut itgi esr \n",
            "16600 time: 6.586654186248779 0.00039676252413022934\n",
            "strain 3.2354612350463867\n",
            "this is what ttstttieoetmtn ftiuaeten el seeeeiee wihi tn ph  seho  s e ghts\n",
            "16700 time: 6.5251569747924805 0.0003907046241821925\n",
            "strain 3.2747488021850586\n",
            "this is whatmnshoss eheaens eso rshgsapeiho haasttaeoseeha  itser ers  ee se\n",
            "16800 time: 6.58457088470459 0.00039191544322015047\n",
            "strain 3.092550277709961\n",
            "this is whatisoi ti euttatitbstpsihst eeerittttmbfames ds\n",
            "tfls\n",
            " ui\n",
            "ntlts taa\n",
            "16900 time: 6.561181306838989 0.0003882126232328009\n",
            "strain 3.3104076385498047\n",
            "this is what tinsnuetnst h tn retuepiirtp e ee thur thtiw uehfyisyneifi s  t\n",
            "17000 time: 6.607381105422974 0.00038864664327158\n",
            "strain 3.2170982360839844\n",
            "this is whatttie  sel etpvprueoogi.hk. edaoyo�jl,tsegytp lo d,roaccita0c dae\n",
            "17100 time: 6.564133167266846 0.000383845031432231\n",
            "strain 3.243109703063965\n",
            "this is whatrf oGe  ttl ea Eaeuspgoiae oe  te   afonn  t t  alraera tt  m e \n",
            "17200 time: 6.597632884979248 0.00038356103492915563\n",
            "strain 3.21975040435791\n",
            "this is whattytsekwap njugvesvtei khed.Bcso\n",
            "fmfspsaiaapmyea9nesvpagt tes\n",
            "ai.\n",
            "17300 time: 6.5811803340911865 0.00038039310151707554\n",
            "strain 3.1624269485473633\n",
            "this is what i i s \n",
            "lue a , tgteil e iiedchcniateacsrgnwyttdt ryyxe loec wkc\n",
            "17400 time: 6.583384275436401 0.00037833370221674386\n",
            "strain 3.4000062942504883\n",
            "this is whatstahl  eeacanoim e  e . j snscmt nans drlou�ihira o otfa   dycG \n",
            "17500 time: 6.570730686187744 0.0003754488979501116\n",
            "strain 3.287862777709961\n",
            "this is what saotaot ooletaatttipsthpotntt hdathtmtovtgtta tteoatnooointt tn\n",
            "17600 time: 6.591764211654663 0.0003745107920046158\n",
            "strain 3.251314163208008\n",
            "this is whathw lahaupmsuhws hpvulungfsn anactdhbwhnlyehtsh uayewhhpsh shmat \n",
            "17700 time: 6.6116650104522705 0.0003735193336934641\n",
            "strain 3.289825439453125\n",
            "this is whateuhgwn fa  wtfhbsn  mw\n",
            "afrnh to   f naypt unf wic ftc \n",
            " fnwnnmtu\n",
            "17800 time: 6.549315690994263 0.000367918454370541\n",
            "strain 3.2001724243164062\n",
            "this is what\n",
            "dwMp,  naihllym fn wt\n",
            "i ymf \n",
            "a  e\n",
            "tywl u uyudli  f\n",
            "t   \n",
            "yoa\n",
            "hha\n",
            "17900 time: 6.560526371002197 0.0003664894054990981\n",
            "strain 3.2559566497802734\n",
            "this is what  ti.htau\n",
            " bfo hhppane\n",
            "m yefudtd ea  de ,fupllgdss bs n i   d u \n",
            "18000 time: 6.552743911743164 0.0003640211182801924\n",
            "strain 3.2282562255859375\n",
            "this is whatlcy w h uihhunh doeuuui\n",
            "  ctisf du\n",
            "otsh tue  uie tdhfpeasn l nhl\n",
            "18100 time: 6.66760778427124 0.00036835579587362016\n",
            "strain 3.185606002807617\n",
            "this is what   letubgismrati lwn ftus  rf\n",
            "npfseu dr hnh stcnbhisee, hnttr hr\n",
            "18200 time: 6.5787763595581055 0.00036145139478024635\n",
            "strain 3.2844362258911133\n",
            "this is whataa i dl w h  e u ts hnm iiufte oa  ns tt caic e  a dnidpuawsceul\n",
            "18300 time: 6.587677001953125 0.00035996270579696647\n",
            "strain 3.2341136932373047\n",
            "this is whathsd  m\n",
            "es. cl hiadfga ntbfv lhv\n",
            "puemiimhT eversrf mduu� uehgehtf\n",
            "18400 time: 6.592809438705444 0.000358285401724038\n",
            "strain 3.0067367553710938\n",
            "this is whatasacibv ab hnle e fhu remieteher ge,rri rh  r    epift en, la  t\n",
            "18500 time: 6.563042402267456 0.0003547398994164972\n",
            "strain 3.2592830657958984\n",
            "this is whathu nletuhltes wndpv dirg,mehptdvewhb  anan   im f a tn nh  rd b,\n",
            "18600 time: 6.561160087585449 0.00035273161673249754\n",
            "strain 3.219898223876953\n",
            "this is whatli  v e oTMfv C1o itet  elt1iTwma te mBetf\n",
            "e   hhit  t  gfe\n",
            "trem\n",
            "18700 time: 6.568626642227173 0.0003512446991639127\n",
            "strain 3.264005661010742\n",
            "this is whatw haah  ut tiusaynLdnrlinhahu ytin    gratawe\n",
            " a uWcngs c  v tia\n",
            "18800 time: 6.542347431182861 0.00034797871964363703\n",
            "strain 3.225353240966797\n",
            "this is whatk   ee    fv �n    tT T anefo   spnl  patccienny . \n",
            "ltt, s\n",
            "   dl\n",
            "18900 time: 6.630818128585815 0.0003508183888156391\n",
            "strain 3.1885948181152344\n",
            "this is what ut . ba  eih r ,snsoft rtiim Minu oyuaAse,dn a wn au,\n",
            "  anan kg\n",
            "19000 time: 6.534377098083496 0.00034389650939006653\n",
            "strain 3.1826438903808594\n",
            "this is whattasduta h\n",
            " nb e oo ifhiH rsatdrywe mn l,nehhcn s haua csh oue, s\n",
            "19100 time: 6.568260431289673 0.00034387000199606464\n",
            "strain 3.1595726013183594\n",
            "this is whataect ncr mteafsi nmbeenh  r  Stt  th aomefe dimnoeus\n",
            "m d hrct \n",
            "e\n",
            "19200 time: 6.608560085296631 0.00034417793667693294\n",
            "strain 3.269899368286133\n",
            "this is whatiioc hr,  mrtdoocincdP a r c wt sttes hhao SnlignSst .txhhMbemea\n",
            "19300 time: 6.552987098693848 0.0003395154435278486\n",
            "strain 3.120096206665039\n",
            "this is what   o d tiettnh teet  hf wntaaa ts, reesatam Dtnt  etr  esifc o W\n",
            "19400 time: 6.574145793914795 0.0003388560523041056\n",
            "strain 3.4148006439208984\n",
            "this is whateenrity h\n",
            "ttdh hcrei,wth,e a seliuhetyop �tiodt eetyooeahu l pi \n",
            "19500 time: 6.539746284484863 0.0003353544311226102\n",
            "strain 3.294727325439453\n",
            "this is whatoaxn1 asn nthinaaa�hac  nial3 ua nfhfie atrsutcoznco    e iihusc\n",
            "19600 time: 6.5913331508636475 0.0003362753751186575\n",
            "strain 4.666019439697266\n",
            "this is whatea heeCgi,tsawr\n",
            "ai rieceonsiRiei  pga\n",
            "aresei\n",
            "i e ilt u i  owecr \n",
            "19700 time: 6.509209632873535 0.0003303999852652502\n",
            "strain 3.1729602813720703\n",
            "this is what r  da vc  bolnuwio ttayr\n",
            " nlnsmehehmnas-necebtlmod ia R urw ts \n",
            "19800 time: 6.563361883163452 0.00033146621879799956\n",
            "strain 3.3232364654541016\n",
            "this is whatmyg ensoEesftayth ecgog  Ht,�en,bua�mrfifFPsCs Jarlotle ethv�npy\n",
            "19900 time: 6.498523235321045 0.0003265425833162546\n",
            "strain 3.1989288330078125\n",
            "this is what\n",
            "hdrae oumnna eu 'Ka   tl tpr an tsvy/toe ee.cieahcmebegh , aiee\n",
            "20000 time: 6.575545310974121 0.00032876085134799654\n",
            "strain 3.3573131561279297\n",
            "this is whateda 1hmedTl oaxTavucEwe  bsa    hWa Latn N isn bhoeMiils aS H c \n",
            "20100 time: 6.517005681991577 0.00032421302026815365\n",
            "strain 3.192549705505371\n",
            "this is whatbdne  ae tal\n",
            "oeudtatlnkcfcay it e mp lhne ecrtswh e tstos  lrpAe\n",
            "20200 time: 6.555541276931763 0.00032451570485465673\n",
            "strain 3.077930450439453\n",
            "this is whatvrhhueuCtyrsanlbt�l lnnaa1 psno n rooeonfsuiw  CermAfedacn arlbh\n",
            "20300 time: 6.503594875335693 0.0003203583740787573\n",
            "strain 3.248250961303711\n",
            "this is whatilaiv�wh\n",
            "ai ek u ndael eySydnesr,Lde nsrbid nreM�gtedniarg A r o\n",
            "20400 time: 6.627552509307861 0.00032486412362849954\n",
            "strain 3.3648319244384766\n",
            "this is whattnw etsd irfnrseaei gadoneiw,f e\n",
            "rr9 mw ,Anrhssi t y \n",
            " sens re a\n",
            "20500 time: 6.5101563930511475 0.0003175531374024831\n",
            "strain 3.2405548095703125\n",
            "this is whatWsocode ev  ihP iaoetr�hsi�adsnsrrtds rciettdo ct an nIeyctso lt\n",
            "20600 time: 6.550676584243774 0.00031797859623712116\n",
            "strain 3.1852970123291016\n",
            "this is what apacaweted eoo  srti so slbo.h\n",
            "iaeson csoS lsa.aa eeaa,oucdartd\n",
            "20700 time: 6.542139053344727 0.00031603012077589896\n",
            "strain 3.3942623138427734\n",
            "this is what pdstaoa ed(itarrabaro,.e� miaueanusist]erwbegesa  ns  lntiaoori\n",
            "20800 time: 6.514791250228882 0.00031319607175844724\n",
            "strain 3.183502197265625\n",
            "this is whatcrgverpen tnubteMo dt ctuEt,eelhlgcaid)rn.cpuh-a)fy  ,rrd%dFwrii\n",
            "20900 time: 6.53480339050293 0.0003126550705191861\n",
            "strain 3.474580764770508\n",
            "this is whataeoge 2svht e�t- hie�o elfr   gwnBilvtLtpa[l vsals  owfe cfsufaa\n",
            "21000 time: 6.520620584487915 0.0003104909795402635\n",
            "strain 3.2192554473876953\n",
            "this is whatof elhao hnhonyhuho olceykmsierloynea  . eonvsee siAa   rntahieE\n",
            "21100 time: 6.613883018493652 0.0003134393278476011\n",
            "strain 3.494810104370117\n",
            "this is what0Aq�5�sx{Mcjpo� ?z Tz_jyP�M,?M�Q.ks9;C�C�(qu\"'tGBOCs5cjcZpMjI[ T\n",
            "21200 time: 6.613938093185425 0.0003119635191746891\n",
            "strain 3.2166175842285156\n",
            "this is whatckb  a oadncih..lePmyu  slp edh  tm iw s   ynasthalhdnrg frvw as\n",
            "21300 time: 6.57429838180542 0.0003086380273331768\n",
            "strain 3.1255197525024414\n",
            "this is whatNBpsi toetl5amet ia n e rpneki ehae  rifdBrdft,e\n",
            "rehMn\n",
            "onnHi   u\n",
            "21400 time: 6.511333465576172 0.0003042537237705402\n",
            "strain 3.0974245071411133\n",
            "this is what  w2wtwtB.\n",
            " wdt o mdus aroihejcos nit  alwiurog wpsesisan St  Ta\n",
            "21500 time: 6.5772693157196045 0.00030590528599312516\n",
            "strain 3.204050064086914\n",
            "this is what .ota  E ef   d3nTaMi,e\n",
            "�   hh  h L ss   WcL�o   hv      B     G\n",
            "21600 time: 6.5070624351501465 0.000301238966343563\n",
            "strain 3.197368621826172\n",
            "this is whatsdu      st o  d�i an. a  G y pSkp meeybi\" qk     TI  ns pub \n",
            "\n",
            " \n",
            "21700 time: 6.6255927085876465 0.000305312794203319\n",
            "strain 3.125436782836914\n",
            "this is what\n",
            " iiD�t  h v atarxv\n",
            "ettse  yi c e h\"b tk  gDk n T\n",
            " \"isek ito  ne\n",
            "21800 time: 6.512253999710083 0.00029871357804531077\n",
            "strain 3.1780834197998047\n",
            "this is whatot ca  mSsepv te  \n",
            "oi\n",
            "bCaa iu   arh Tt f\n",
            "�   rSh etefw igiaue \n",
            " \n",
            "21900 time: 6.532975912094116 0.0002982957924529791\n",
            "strain 3.198152542114258\n",
            "this is what   yvdpA nsw\n",
            "   tnlagHm iR  2tS ieuc ta'-uerAg..t tt  icon .t  d\n",
            "22000 time: 6.572373151779175 0.00029873067717905245\n",
            "strain 3.2538576126098633\n",
            "this is what�e D .n enp S shs kaiar gnoodsdmBiTSf �P cciicrPT c  soL    \"i..\n",
            "22100 time: 6.519353151321411 0.0002949800401147979\n",
            "strain 3.1711931228637695\n",
            "this is what \n",
            "  cotao t  d n eSs t p a\n",
            "hGde MaiCn h \n",
            "inwedsbT aa\"r  p ir�iwv\n",
            "22200 time: 6.686298370361328 0.00030117107723152405\n",
            "strain 3.1497020721435547\n",
            "this is whathpsGtrimn id-eadro-\"nhny pm n rl aieider n tf ire gn . gs'earoeT\n",
            "22300 time: 6.52196478843689 0.0002924516849852235\n",
            "strain 3.259033203125\n",
            "this is whatpoaI eeivwo9me l uc1,Igoidaoxvtaafatb iw1 \"gcve  sh r h  e2sgsc'\n",
            "22400 time: 6.551742315292358 0.0002924754605655782\n",
            "strain 3.1736459732055664\n",
            "this is whatrc n Pneaa  iy or tei Pth don kfda i. ahv eTTeeacotMuet eea odom\n",
            "22500 time: 6.504588842391968 0.0002890800106319331\n",
            "strain 3.2528228759765625\n",
            "this is whatelutfatttew  tlemnyih  h uI aas 1p  sen yh  i ie  in k ae \n",
            " ed  \n",
            "22600 time: 6.558024644851685 0.000290165253009613\n",
            "strain 3.1456127166748047\n",
            "this is whaturete hlu�m testsaedt\n",
            "ss sga ss loedh deethul uolat�mg st �y   C\n",
            "22700 time: 6.501295328140259 0.0002863880800395321\n",
            "strain 3.194051742553711\n",
            "this is whattphEirtfdarMiTprnepr t  ihsh e,olirehyiln3 taeea w eheete hasefa\n",
            "22800 time: 6.580112457275391 0.00028858878707567866\n",
            "strain 3.2575130462646484\n",
            "this is whatsbasrUe�esnas,tuelD s h  nh nnltoat et S�S  teauehcrseriA'lns od\n",
            "22900 time: 6.488858461380005 0.0002833439228258915\n",
            "strain 3.1838436126708984\n",
            "this is what xh\n",
            "oiwhnanil oa tfolteawi,noMcussvyentnIuae\n",
            "t,,dng eeua .ti n  \n",
            "23000 time: 6.680781364440918 0.0002904561679107532\n",
            "strain 3.1838245391845703\n",
            "this is whatadya n s w?nsehleva�euqoliw \n",
            "xnlop-yufuw lasei�dt�o e�aY   wsano\n",
            "23100 time: 6.489482164382935 0.0002809178338443648\n",
            "strain 3.140256881713867\n",
            "this is whatla  nrpnrune p  d As tiiIsElrcarus mniToieDedgepdF h-eeaw vs scb\n",
            "23200 time: 6.578174352645874 0.0002835298076764627\n",
            "strain 3.1320228576660156\n",
            "this is what,erer  ltrt.saw  qh  tneeey t tm  nikh c  oart ,lteilul gafsineu\n",
            "23300 time: 6.5152435302734375 0.00027961219555778795\n",
            "strain 3.4388389587402344\n",
            "this is whatnliatttwrr os t Siehteaasiinehe n�enmhve\n",
            "w aaauaeiBnmtnnp1\n",
            "sRret\n",
            "23400 time: 6.529882431030273 0.00027904289002388154\n",
            "strain 3.17569637298584\n",
            "this is whateart\" sa dorreiDarishufma weriffnuaal ic an naiieshmi crc a oas�\n",
            "23500 time: 6.609718084335327 0.00028125265142642806\n",
            "strain 3.163188934326172\n",
            "this is whatttitelnn c tgdpticle9eaabne sPace.owrtCt  siliaoleusrep ensgutwz\n",
            "23600 time: 6.505224704742432 0.0002756334554289899\n",
            "strain 3.3172712326049805\n",
            "this is what htac ilkafseoilha,tpgl tsc l  eserhgn�iaohgup  ypltn/hfiodsv m�\n",
            "23700 time: 6.558329343795776 0.00027671110166798595\n",
            "strain 3.13055419921875\n",
            "this is what e� wfdalu0it eg h oc P ,stnteh ey eg n ra stnositwmeeptpk  i la\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2302530062.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# train_loss = strain(model, train_loader, optim, scheduler=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mstrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m# strain(model, train_loader, optim, scheduler=scheduler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# print(generate(model, \"this is what\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2302530062.py\u001b[0m in \u001b[0;36mstrain\u001b[0;34m(model, dataloader, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title train generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optimizer, scheduler=None): # train function with automatic mixed precision\n",
        "    start = begin = time.time()\n",
        "    model.train()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x, y = x[:,:-1], x[:,1:]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits = model(x) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optim)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            print(\"strain\",loss.item())\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            start = begin = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            output = model(x) # gpt\n",
        "        output = output[:,-1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    # completion = decode(x)\n",
        "    return completion\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=40000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    # strain(model, train_loader, optim, scheduler=scheduler)\n",
        "    # print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33Grqz8SiKPN"
      },
      "outputs": [],
      "source": [
        "print(generate(model, \"this is what\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knt9jqUmgl97"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5CjHMaQE8MM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title GRU pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "    def forward(self, x, h0=None): # rnn/gru\n",
        "        x = self.emb(x)\n",
        "        if h0==None: h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        # print(x.shape, h0.shape)\n",
        "        x, h0 = self.rnn(x, h0)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        x = self.fc(x) # out: (n, 10)\n",
        "        return x, h0\n",
        "\n",
        "hidden_size = 128 #64 128\n",
        "num_layers = 3#2\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "\n",
        "model = RNN(vocab_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "# print(model)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "# 128,2\n",
        "# Test Loss: 6.360389362062727\n",
        "# this is what ween new york is well it a sign more directly into simply shares\n",
        "# 0 time: 5.910429954528809 5.910431623458862\n",
        "# 64,2\n",
        "# Test Loss: 6.167358561924526\n",
        "# this is what bull morp on its aftant opereals of a b. the hamally plans beces\n",
        "# 0 time: 5.655158996582031 5.655160903930664\n",
        "# 64,1\n",
        "# Test Loss: 5.823708357129778\n",
        "# this is what achan agrive\n",
        "#  the guinesst on of promjects cl funds that jound\n",
        "# 0 time: 4.788918495178223\n",
        "\n",
        "# Test Loss: 8.247572830745153\n",
        "# this is what that has months with unfinsings lide to N by well\n",
        "#  next offited\n",
        "# 29 time: 3.902247905731201 4.377542002995809\n",
        "\n",
        "# dim128 3lyr bptt25 2.8,.2\n",
        "# this is whatever from the every coddane\n",
        "# teets, in.]\n",
        "# [Footnote 21: Any ot\n",
        "# 9700 time: 4.124014616012573 0.00042511236912171237\n",
        "# strain loss, ppl 1.3780442476272583 3.953125\n",
        "# this is what seeming upon course in your drew\n",
        "# plant now, as a less ever aff\n",
        "\n",
        "b,t=2,5\n",
        "x = torch.randint(0, vocab_size, (b,t), device=device)\n",
        "h = torch.rand(num_layers, b, hidden_size, device=device)\n",
        "out, h = model(x, h)\n",
        "print(out.shape, h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SadMd02M_FK"
      },
      "outputs": [],
      "source": [
        "# @title bptt train, gen\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    # nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    nll = -log_probs.gather(dim=-1, index=target.to(torch.int64).unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optim, scheduler=None, bptt=25): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    h0 = None\n",
        "    start = begin = time.time()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        xs, ys = torch.split(x[:,:-1], bptt, dim=1), torch.split(x[:,1:], bptt, dim=1)\n",
        "        for (x, y) in zip(xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "                pred, h0 = model(x, h0)\n",
        "                loss = F.cross_entropy(pred.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "                # loss = F.cross_entropy(pred.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "            optim.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            # scaler.unscale_(optim)\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            h0 = h0.detach()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            perplexity = Perplexity(pred.detach(), y).item()\n",
        "            print(\"strain loss, ppl\",loss.item(), perplexity)\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            start = begin = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    hidden=None # rnn 1/3\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            # output = model(x) # gpt\n",
        "            output, hidden = model(x, hidden) # rnn 2/3\n",
        "        hidden = hidden[:,-1].unsqueeze(1) # rnn 3/3\n",
        "        output = output[:,-1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    # completion = decode(x)\n",
        "    return completion\n",
        "\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U_UuuGu35g49"
      },
      "outputs": [],
      "source": [
        "# @title pg19\n",
        "# https://console.cloud.google.com/storage/browser/deepmind-gutenberg/test\n",
        "# https://storage.cloud.google.com/deepmind-gutenberg/test/10146.txt\n",
        "\n",
        "# # !wget https://storage.googleapis.com/deepmind-gutenberg/train/10006.txt\n",
        "# !mkdir ./validation/\n",
        "# !wget https://storage.googleapis.com/deepmind-gutenberg/validation/42306.txt -O ./validation/42306.txt\n",
        "# # with open('/content/10006.txt', 'r') as f: text = f.read()\n",
        "\n",
        "# !gsutil -m cp -r gs://deepmind-gutenberg/train ./\n",
        "!gsutil -m cp -r gs://deepmind-gutenberg/test /content\n",
        "# !gsutil -m cp -r gs://deepmind-gutenberg/validation ./\n",
        "\n",
        "# # !mv -v ~/validation/.[!.]* ~/test/\n",
        "# !mv ~./validation/* ~./test\n",
        "!mv ~/content/validation/* ~/content/test\n",
        "\n",
        "# path='validation/' # 17,733,002 # 4200\n",
        "# path='test/' # 41,289,101\n",
        "# path='train/' # 749,219,628++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UFtNtKAF5rjv"
      },
      "outputs": [],
      "source": [
        "# @title bytedataset\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "\n",
        "def txt_iter(filepath, chunk_size=8192):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk: break\n",
        "            yield chunk\n",
        "\n",
        "\n",
        "    # class InfiniteDataset(IterableDataset):\n",
        "    #     def __init__(self, start_value=0):\n",
        "    #         self.current_value = start_value\n",
        "\n",
        "    #     def __iter__(self):\n",
        "    #         while True:\n",
        "    #             yield self.current_value\n",
        "    #             self.current_value += 1\n",
        "\n",
        "# class ByteDataset(Dataset):\n",
        "class ByteDataset(IterableDataset):\n",
        "    def __init__(self, path='train/', seq_len=129, buffer_size=1024):\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        self.seq_len, self.buffer_size = seq_len, buffer_size  # must be ≥ seq_len\n",
        "        file_list = [path+f for f in os.listdir(path)]\n",
        "        random.shuffle(file_list)\n",
        "        self.fileiter = iter(file_list)\n",
        "        # self.process()\n",
        "        self.textiter = txt_iter(next(self.fileiter))\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            try: x = next(self.textiter)\n",
        "            except StopIteration:\n",
        "                self.textiter = txt_iter(next(self.fileiter))\n",
        "                x = next(self.textiter)\n",
        "            self.buffer.extend(x)\n",
        "\n",
        "    # def __len__(self): return 128000 # too large will cause long load first batch\n",
        "    # # fs = sum([os.path.getsize(path+file) for file in os.listdir(path)])\n",
        "\n",
        "# import re\n",
        "# def strip_gutenberg_boilerplate(text):\n",
        "#     start_re = re.compile(r\"\\*\\*\\* START OF (.*?) \\*\\*\\*\", re.IGNORECASE)\n",
        "#     end_re = re.compile(r\"\\*\\*\\* END OF (.*?) \\*\\*\\*\", re.IGNORECASE)\n",
        "#     start_match = start_re.search(text)\n",
        "#     end_match = end_re.search(text)\n",
        "#     start = start_match.end() if start_match else 0\n",
        "#     end = end_match.start() if end_match else len(text)\n",
        "#     return text[start:end].strip()\n",
        "\n",
        "# def normalise_whitespace(text):\n",
        "#     return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "            # if len(self.buffer) < self.seq_len: raise StopIteration\n",
        "            if len(self.buffer) < self.seq_len: return\n",
        "            x, self.buffer = self.buffer[:self.seq_len], self.buffer[self.seq_len:]\n",
        "            # # return torch.tensor(x, dtype=torch.uint8)\n",
        "            # return torch.tensor(x, dtype=torch.int32)\n",
        "            yield torch.tensor(x, dtype=torch.int32)\n",
        "\n",
        "seq_len = 129 # 128\n",
        "train_data = ByteDataset('validation/', seq_len) # one line of poem is roughly 50 characters\n",
        "# train_data = ByteDataset('test/', seq_len) # one line of poem is roughly 50 characters\n",
        "# test_data = ByteDataset('test/', seq_len) # one line of poem is roughly 50 characters\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=2)\n",
        "# test_loader = DataLoader(test_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/tokenizers/blt_tokenizer.py#L137\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.uint8)#, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device).unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "# for x in train_loader:\n",
        "#     break\n",
        "# print(x)\n",
        "# s='恋 和 したい の わ'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khJg7EKtaFrz"
      },
      "outputs": [],
      "source": [
        "seq_len = 2**11+1 # 128\n",
        "print(seq_len)\n",
        "# train_data = ByteDataset('validation/', seq_len) # one line of poem is roughly 50 characters\n",
        "train_data = ByteDataset('test/', seq_len) # one line of poem is roughly 50 characters\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=2)\n",
        "\n",
        "for i, x in enumerate(train_loader):\n",
        "    print(i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q5bGqKAJzN7I"
      },
      "outputs": [],
      "source": [
        "# @title TTLinear\n",
        "# Tensor Train embedding https://arxiv.org/pdf/1901.10787\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def make_einsum(num_tensors):\n",
        "    a = 97\n",
        "    R = chr(a+25) # 'z'\n",
        "    lhs = [chr(a)+R]\n",
        "    for i in range(1, num_tensors-1):lhs.append(R+chr(a+i)+R)\n",
        "    lhs.append(R+chr(a+num_tensors-1))\n",
        "    return ','.join(lhs) + '->' + ''.join([chr(a+i) for i in range(num_tensors)]) # az,zbz,zcz,zd->abcd\n",
        "\n",
        "class TTLinear(nn.Module):\n",
        "    def __init__(self, in_features=None, out_features=None, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.lfeat = len(in_features)\n",
        "        if self.lfeat==1: lst = in_features + out_features\n",
        "        elif self.lfeat>=2: lst = [i*j for i, j in zip(in_features, out_features)]\n",
        "        last = len(lst)\n",
        "        var = last/rank**(1/(2*(std**.5)*last))\n",
        "        c=1/last\n",
        "        self.params = nn.ParameterList([nn.Parameter(torch.randn(lst[0], rank).clamp(-c,c)*var),\n",
        "            *[nn.Parameter(torch.randn(rank, ij, rank).clamp(-c,c)*var) for ij in lst[1:-1]],\n",
        "            nn.Parameter(torch.randn(rank, lst[-1]).clamp(-c,c)*var)])\n",
        "        self.einsum_str = make_einsum(last)\n",
        "        self.shape = [p for ij in zip(in_features, out_features) for p in ij]\n",
        "        self.permute = list(range(0, 2*self.lfeat - 1, 2)) + list(range(1, 2*self.lfeat, 2))\n",
        "    def weight(self): return torch.einsum(self.einsum_str, *self.params).reshape(self.shape).permute(self.permute).flatten(0,self.lfeat-1).flatten(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.weight()\n",
        "        return x.to(weight.dtype) @ weight\n",
        "\n",
        "def one_hot(x, in_dim):\n",
        "    return torch.zeros((*x.shape,in_dim), dtype=torch.int8, device=x.device).scatter_(-1, x.unsqueeze(-1).to(int), 1)\n",
        "\n",
        "def one_hot(x, in_dim):\n",
        "    b,t = x.shape\n",
        "    o = torch.zeros((b,t,in_dim), dtype=bool, device=x.device)\n",
        "    o[torch.arange(b).unsqueeze(-1),torch.arange(t).unsqueeze(0),x] = True\n",
        "    return o\n",
        "\n",
        "import math\n",
        "class TTEmbedding(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.ttlin = TTLinear(in_dim, d_model, rank, std) # https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "        self.weight = self.ttlin.weight\n",
        "        self.num_classes = math.prod(in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return self.ttlin(F.one_hot(x, self.num_classes))\n",
        "        return self.ttlin(one_hot(x, self.num_classes))\n",
        "# self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "\n",
        "# # in_features=(3,4,5,6); out_features=(2,3,4,5)\n",
        "# in_features=[120]; out_features=[300]\n",
        "# in_features=[12]; out_features=[30]\n",
        "# rank=16\n",
        "# # std=.5\n",
        "# lin = TTLinear(in_features, out_features, rank, std).to(device)\n",
        "# # x = torch.rand(4,math.prod((3,4,5,6)))\n",
        "# x = torch.rand(4,7,math.prod(in_features), device=device)\n",
        "# print(lin.params[0].device)\n",
        "# out = lin(x)\n",
        "# print(out.shape)\n",
        "# print(lin.ttlin.params[0].device)\n",
        "\n",
        "# emb = TTEmbedding(in_features, out_features, rank).to(device)\n",
        "# x = torch.randint(0, math.prod(in_features), (2, 5), device=device)\n",
        "# out = emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# o=lin.weight\n",
        "# print(o.mean().item(), o.std().item(), o.min().item(), o.max().item())\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# # plt.hist(o.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# # plt.hist(o[:100,:100].flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# x = torch.randn(100,100)*std\n",
        "# # plt.hist(x.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# plt.hist([o[:100,:100].flatten().tolist(), x.flatten().tolist()], bins=20, alpha=.5, label='context mask')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UCgV-efIfzne"
      },
      "outputs": [],
      "source": [
        "# @title mask translate\n",
        "PAD_IDX=0\n",
        "def make_src_mask(src):\n",
        "    # return (src != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device) # [batch_size, 1, 1, src_len]?\n",
        "    return (src != PAD_IDX)[:,None,None,:].to(device) # [batch_size, 1, 1, src_len]?\n",
        "\n",
        "# attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "def make_trg_mask(trg):\n",
        "    # trg_pad_mask = (trg != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device)\n",
        "    trg_pad_mask = (trg != PAD_IDX)[:,None,None,:].to(device) # [batch, 1, 1, trg_len]\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
        "    # print('make_trg_mask', trg_pad_mask.shape, trg_sub_mask.shape) # [64, 1, 1, 10], [10, 10]\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask # [batch, 1, trg_len, trg_len]?\n",
        "    return trg_mask\n",
        "\n",
        "def translate(model, src_sentence):\n",
        "    model.eval()\n",
        "    src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "    num_tokens = src.shape[1]\n",
        "    trg_indexes = [BOS_IDX]\n",
        "    max_len = src.shape[1]+5\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output = model(src, trg_tensor, src_mask, trg_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == EOS_IDX: break\n",
        "    trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "    return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "def translate_fast(model, src_sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "        num_tokens = src.shape[1]\n",
        "        trg_indexes = [BOS_IDX]\n",
        "        max_len = src.shape[1]+5\n",
        "        src_mask = make_src_mask(src)\n",
        "        output = model.encode(src, src_mask)\n",
        "        for i in range(max_len):\n",
        "            trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "            trg_mask = make_trg_mask(trg_tensor)\n",
        "            output = model.decode(src, trg_tensor, src_mask, trg_mask)\n",
        "            pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "            trg_indexes.append(pred_token)\n",
        "            if pred_token == EOS_IDX: break\n",
        "        trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "        return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "# UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "src, trg = torch.randint(0, 100, (64, 10)), torch.randint(0, 100, (64, 10))\n",
        "sm = make_src_mask(src)\n",
        "tm = make_trg_mask(trg)\n",
        "# print(sm.shape, tm.shape) # [64, 1, 1, 10], [64, 1, 10, 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N9R4hVUCxGiu"
      },
      "outputs": [],
      "source": [
        "# @title translate train test\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "        trg_input = trg[:,:-1]\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "        print('train', src.shape, trg.shape, src_mask.shape, trg_mask.shape)\n",
        "        output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(list(dataloader))\n",
        "\n",
        "def test(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "            trg_input = trg[:,:-1]\n",
        "            src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "            output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(list(dataloader))\n",
        "\n",
        "# @title run\n",
        "import time\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) # lr=0.0001\n",
        "\n",
        "# for epoch in range(20):\n",
        "for epoch in range(1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = test(model, val_loader, loss_fn)\n",
        "    end_time = time.time()\n",
        "    print((f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    # print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people standing in front of an igloo .\n",
        "    # @title inference\n",
        "    print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people stand in front of an igloo .\n",
        "    print(translate(model, \"Ein Koch in weißer Uniform bereitet Essen in einer Restaurantküche zu .\")) # A chef in a white uniform prepares food in a restaurant kitchen .\n",
        "    print(translate(model, \"Zwei junge Mädchen spielen Fußball auf einem Feld. .\")) # Two young girls play soccer on a field. .\n",
        "    print(translate(model, \"Eine Frau mit Hut und Sonnenbrille steht am Strand .\")) # A woman wearing a hat and sunglasses stands on the beach .\n",
        "    print(translate(model, \"Zwei Freunde lachen und genießen ein Eis auf einer wunderschönen Wiese .\")) # Two friends laugh and enjoy ice cream on a beautiful meadow .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-bHqmV4ReUv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Epoch: 1, Train loss: 5.402, Val loss: 4.186, Epoch time = 41.608s\n",
        "A group of people are are are are are are in a .\n",
        "Epoch: 2, Train loss: 3.898, Val loss: 3.545, Epoch time = 41.068s\n",
        "A group of people are standing in a crowd of people .\n",
        "Epoch: 3, Train loss: 3.353, Val loss: 3.125, Epoch time = 41.566s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 4, Train loss: 2.944, Val loss: 2.830, Epoch time = 40.756s\n",
        "A group of people standing in front of a building .\n",
        "Epoch: 5, Train loss: 2.630, Val loss: 2.596, Epoch time = 41.468s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 6, Train loss: 2.375, Val loss: 2.429, Epoch time = 41.023s\n",
        "A group of people standing in front of a house .\n",
        "Epoch: 7, Train loss: 2.166, Val loss: 2.307, Epoch time = 41.604s\n",
        "A group of people stand in front of a house .\n",
        "Epoch: 8, Train loss: 1.984, Val loss: 2.210, Epoch time = 40.876s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 9, Train loss: 1.834, Val loss: 2.131, Epoch time = 41.496s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 10, Train loss: 1.698, Val loss: 2.079, Epoch time = 41.052s\n",
        "A group of people are standing in front of an empty house .\n",
        "Epoch: 11, Train loss: 1.576, Val loss: 2.038, Epoch time = 41.570s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 12, Train loss: 1.475, Val loss: 2.033, Epoch time = 41.067s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 13, Train loss: 1.381, Val loss: 2.017, Epoch time = 41.576s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 14, Train loss: 1.292, Val loss: 1.977, Epoch time = 40.779s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 15, Train loss: 1.213, Val loss: 1.948, Epoch time = 41.461s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 16, Train loss: 1.139, Val loss: 1.940, Epoch time = 40.800s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 17, Train loss: 1.073, Val loss: 1.940, Epoch time = 41.533s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 18, Train loss: 1.006, Val loss: 1.939, Epoch time = 40.856s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 19, Train loss: 0.944, Val loss: 1.947, Epoch time = 41.345s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 20, Train loss: 0.893, Val loss: 1.956, Epoch time = 40.935s\n",
        "A group of people stand in front of an igloo\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3vBEfUjzuobW"
      },
      "outputs": [],
      "source": [
        "# @title bleu\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    for datum in data:\n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "    return bleu_score(pred_trgs, trgs)\n",
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')\n",
        "# 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model.\n",
        "\n",
        "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "\n",
        "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    translations_done = [0] * len(src_tensor)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_tokens = output.argmax(2)[:,-1]\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\n",
        "            trg_indexes[i].append(pred_token_i)\n",
        "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                translations_done[i] = 1\n",
        "        if all(translations_done):\n",
        "            break\n",
        "\n",
        "    # Iterate through each predicted example one by one;\n",
        "    # Cut-off the portion including the after the <eos> token\n",
        "    pred_sentences = []\n",
        "    for trg_sentence in trg_indexes:\n",
        "        pred_sentence = []\n",
        "        for i in range(1, len(trg_sentence)):\n",
        "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                break\n",
        "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
        "        pred_sentences.append(pred_sentence)\n",
        "    return pred_sentences, attention\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            _trgs = []\n",
        "            for sentence in trg:\n",
        "                tmp = []\n",
        "                # Start from the first token which skips the <start> token\n",
        "                for i in sentence[1:]:\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
        "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
        "                        break\n",
        "                    tmp.append(trg_field.vocab.itos[i])\n",
        "                _trgs.append([tmp])\n",
        "            trgs += _trgs\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
        "            pred_trgs += pred_trg\n",
        "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF9atY_nDOoV"
      },
      "source": [
        "## old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAyqkMjq6T2C"
      },
      "outputs": [],
      "source": [
        "# Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://www.mihaileric.com/posts/transformers-attention-in-disguise/\n",
        "# https://jalammar.github.io/illustrated-transformer/\n",
        "# http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "# position embedding <-> \"vocabulary\" size 100 <-> model can accept sentences up to 100 tokens long\n",
        "# learned positional encoding, warm-up and cool-down steps, label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V1teyZuwff9_"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64c91cf87c13c0e83586b8e66e4d74e/translation_transformer.ipynb\n",
        "\n",
        "# https://github.com/pytorch/data\n",
        "# %pip install portalocker\n",
        "# %pip install torchdata\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -qU torchdata torchtext\n",
        "!pip install -qU spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "!git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset\n",
        "\n",
        "# !pip list | grep torch\n",
        "!pip list | grep python\n",
        "\n",
        "import torchtext.datasets as datasets\n",
        "\n",
        "# Load the Multi30k dataset\n",
        "train_iter, valid_iter, test_iter = datasets.Multi30k(split=('train', 'valid', 'test'))\n",
        "\n",
        "# Iterate through the dataset\n",
        "for src, tgt in train_iter:\n",
        "    print(f\"Source: {src}\")\n",
        "    print(f\"Target: {tgt}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5BFat7RgKSwR"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "# modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "de_vocab.set_default_index(UNK_IDX)\n",
        "en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# vocab_transform = {SRC_LANGUAGE:de_vocab, TRG_LANGUAGE:en_vocab}\n",
        "# text_transform = {SRC_LANGUAGE:de_transform, TRG_LANGUAGE:en_transform}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gURN1FYyM-A2"
      },
      "outputs": [],
      "source": [
        "# !pip install -q datasets\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "!python -m spacy download de_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J-zgIl9FMgPV"
      },
      "outputs": [],
      "source": [
        "# @title hf data\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a translation dataset (e.g., WMT English-German)\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "\n",
        "# Access train and validation splits\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "\n",
        "# # Example of accessing data\n",
        "# # for example in train_data:\n",
        "# #     print(example['translation'])\n",
        "# batch_size = 128 # 128\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "# from torchtext.datasets import multi30k, Multi30k\n",
        "# # modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "# de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "# en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# # Create a blank Tokenizer with just the English vocab\n",
        "# tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# # Construction 2\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# tokenizer = nlp.tokenizer\n",
        "\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "import spacy\n",
        "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "de_tokenizer = spacy.load(\"de_core_news_sm\") # https://spacy.io/models/de\n",
        "\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# from torchtext.vocab import build_vocab_from_iterator\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "# de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# de_vocab.set_default_index(UNK_IDX)\n",
        "# en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "\n",
        "print(\"en_tokens\", en_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "# val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KeuV4EZyqXj5"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_seq_length=512):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "class LearntPosEnc(nn.Module): # learnt positional embeddings\n",
        "    def __init__(self, d_model, dropout=0.1, max_length=512):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, src_len = x.shape\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # [batch size, src len]\n",
        "        return self.drop(x + self.pos_embedding(pos))\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        # theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "        theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
        "        # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = pos * theta # [seq_len, 1] * [dim // 2] = [seq_len, dim // 2]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: print(\"rot_emb.shape[0] < seq_len\")\n",
        "        rot_emb = self.rot_emb[:seq_len, :].unsqueeze(0).expand(batch, -1, -1)\n",
        "        x = x.view(batch, seq_len, dim // 2, 2)\n",
        "        rot_emb = rot_emb.view(batch, seq_len, dim // 2, 2)\n",
        "        # rot_x = torch.einsum('...ij,...ij->...ij', x, rot_emb)\n",
        "        rot_x = x * rot_emb\n",
        "        # return rot_x.view(*rot_x.shape[:-2], dim)\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        x = self.drop(attention) @ V # x = torch.matmul(self.drop(attention), V)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.norm1(src + self.drop(self.self_attn(src, src, src, src_mask)[0]))\n",
        "        src = self.norm2(src + self.drop(self.ff(src)))\n",
        "        return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.norm3 = nn.RMSNorm(d_model)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.enc_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.norm1(trg + self.drop(self.self_attn(trg, trg, trg, trg_mask)[0]))\n",
        "        trg = self.norm2(trg + self.drop(self.enc_attn(trg, enc_src, enc_src, src_mask)[0]))\n",
        "        trg = self.norm3(trg + self.drop(self.ff(trg)))\n",
        "        return trg\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        return trg\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, enc_layers, nhead, ff_dim, dropout)\n",
        "        self.decoder = Decoder(d_model, dec_layers, nhead, ff_dim, dropout)\n",
        "        # self.pos_enc = PositionalEncoder(d_model, dropout=dropout)\n",
        "        # self.pos_enc = LearntPosEnc(d_model, dropout=dropout)\n",
        "        self.pos_enc = RoPE(d_model)\n",
        "        self.src_tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.trg_tok_emb = nn.Embedding(out_dim, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src = self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model))\n",
        "        trg = self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model))\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.lin(output)\n",
        "        return output\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        return self.encoder(self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model)), src_mask)\n",
        "\n",
        "    def decode(self, trg, memory, trg_mask=None, src_mask=None):\n",
        "        trg = self.decoder(self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model)), memory, trg_mask, src_mask)\n",
        "        return self.lin(trg)\n",
        "\n",
        "\n",
        "in_dim = 50#\n",
        "out_dim = 50\n",
        "model = Seq2Seq(in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zqXMOvhvV1CQ"
      },
      "outputs": [],
      "source": [
        "# @title Attention with kvcache window\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# mask = torch.rand(b,t,t)<.2\n",
        "def to_midx(mask): # [b,t,t+p] bool mask\n",
        "    b,t = mask.shape[:2]\n",
        "    idx = mask.nonzero(as_tuple=True)\n",
        "    tk = torch.stack(idx[:-1], dim=-1).to(device)\n",
        "    tk = torch.cat([torch.tensor([True], device=device), (tk[:-1]!=tk[1:]).any(-1)], dim=0)\n",
        "    cc = len(idx[0])\n",
        "    positions = torch.arange(cc, device=device)\n",
        "    last_reset_pos = torch.zeros(cc, dtype=int, device=device)\n",
        "    last_reset_pos[tk] = positions[tk]\n",
        "    out = positions - last_reset_pos.cummax(0).values\n",
        "    c=out.max()+1\n",
        "    y = torch.full((b,t,c), -1, device=device)\n",
        "    y[*idx[:-1],out] = idx[-1]\n",
        "    return y # [b,t,c]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=8, drop=0, w=64):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "        t=128\n",
        "        self.midx = (torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)).to(device) # [t,w,1]\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [b,t,t(+p)]\n",
        "        b,t = x.shape[:2]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # kv = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2)#.chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        kv = torch.cat([self.pos_enc(k),v], dim=-1).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        import time\n",
        "        start = time.time()\n",
        "\n",
        "        # if mask != None:\n",
        "            # midx = to_midx(mask) # [b,t,w]\n",
        "            # # print('mask', q.shape, mask.shape, midx.shape)\n",
        "            # kv = kv[torch.arange(b)[:,None,None,None], torch.arange(self.n_heads)[None,:,None,None], midx.unsqueeze(1)] # [b,h,t,w,d]\n",
        "\n",
        "            # b,t,d = x.shape\n",
        "            # if t>len(self.midx): self.midx = (torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)).to(device) # [t,w,1]\n",
        "            # midx = self.midx[:t,-min(self.w,t):] # [t,w,1]\n",
        "            # kv = kv[torch.arange(b)[:,None,None,None], torch.arange(self.n_heads)[None,:,None,None], midx[None,None,...]]\n",
        "\n",
        "\n",
        "        kv = F.pad(kv, (0,0,self.w-1,0)) # [b, h, t+w-1, d]\n",
        "        # kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "        kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "\n",
        "\n",
        "            # mmask = midx>=0 # F->mask # [t,c]\n",
        "            # mmask = (midx>=0).unsqueeze(0) # F->mask # [1,t,c]\n",
        "        # else: kv = kv.unsqueeze(3).repeat(1,1,1,t,1) # [b,h,t,t(w),d]\n",
        "        print('midx', time.time()-start)\n",
        "\n",
        "# [batch, n_heads, T, d_head]\n",
        "# [b,h,t,d] @ [b,h,d,t] = [b,h,t,t]\n",
        "\n",
        "# [batch, n_heads, T, d_head]\n",
        "# [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "\n",
        "        mk, mv = kv.chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "        # del kv\n",
        "        # attn fwd q mk attn torch.Size([64, 8, 127, 8]) torch.Size([64, 8, 100, 64, 8])\n",
        "        # print('attn fwd q mk attn', q.shape, mk.shape, kv.shape)\n",
        "        # attn = q.unsqueeze(3) @ mk.transpose(-2,-1) * self.scale # [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "        # print('attn fwd q mk attn', q.shape, mk.shape, attn.shape, mmask.shape)\n",
        "        # q mk attn [64, 8, 100, 8], [64, 8, 100, 3, 8], [64, 8, 100, 1, 3], [64, 100, 3])\n",
        "        # q mk attn [64, 8, 100, 8], [64, 8, 100, 64, 8], [64, 8, 100, 1, 64], [100, 64])\n",
        "# attn fwd q mk attn torch.Size([64, 8, 100, 8]) torch.Size([64, 8, 100, 3, 8]) torch.Size([64, 8, 100, 1, 3]) torch.Size([100, 3])\n",
        "\n",
        "        attn = torch.einsum(\"bhtd,bhtwd->bhtw\", q, mk) * self.scale\n",
        "        # print('attn fwd q mk attn', q.dtype, mk.dtype, attn.dtype)\n",
        "\n",
        "        # if mask != None:\n",
        "        #     # attn = attn.masked_fill(~mmask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        #     attn = attn.masked_fill(~mmask[:,None,:,None], -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,1,t]\n",
        "        attention = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        # out = (self.drop(attention) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "        out = torch.einsum(\"bhtw,bhtwd->bhtd\", self.drop(attention), mv)\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# td*dt->tt t(2t+d)\n",
        "# td*twd->tw t(~wd-(w+2)d)\n",
        "# iff 2t>wd\n",
        "# gen: xqkv, patch: ~xkv ~pq, gen: xqkv pkv\n",
        "# calc xqkv, calc ~xkv ~pq, get xqkv calc pkv\n",
        "\n",
        "# typically: gen td*td=tt\n",
        "# gen t1d*twd=tw, patch pd*tpd=p\n",
        "\n",
        "\n",
        "b,t,d = 64,100,512\n",
        "# mask = torch.rand(b,t,t, device=device)<.2\n",
        "\n",
        "\n",
        "# causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "# # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "# mask = causal_mask * ~torch.tril(torch.ones_like(causal_mask, dtype=bool, device=device), diagonal=-64) # sliding window mask\n",
        "\n",
        "\n",
        "msk = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(msk, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(ind.shape, ind)\n",
        "mask = torch.zeros_like(msk, dtype=bool)\n",
        "mask[torch.arange(b)[:,None,None], torch.arange(t)[None,:,None], ind] = True\n",
        "\n",
        "\n",
        "# model = Attention(d).to(device) # 257 ms\n",
        "model = Attention(d, w=3).to(device) # 257 ms\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "# out = model(x)\n",
        "out = model(x, mask=mask)\n",
        "print(out.shape)\n",
        "print(time.time()-start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gLnVZ-hYf97r"
      },
      "outputs": [],
      "source": [
        "# @title torch.profiler  as_strided unfold\n",
        "\n",
        "import torch\n",
        "b,t = 2,8\n",
        "mask = torch.rand(b,t,t)<.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        # to_midx(mask)\n",
        "        midx = midx[:t,-min(w,t):] # [t,w,1]\n",
        "        h=4\n",
        "        d=5\n",
        "        kv = torch.rand(b,h,t,d)\n",
        "        kv = kv[torch.arange(b)[:,None,None,None], torch.arange(h)[None,:,None,None], midx[None,None,...]]\n",
        "\n",
        "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "print(prof.key_averages().table())\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "# print(time.time()-start)\n",
        "\n",
        "x = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(x, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(c)\n",
        "b,h,t,d = 2,4,8,5\n",
        "k = torch.rand(b,h,t,d)\n",
        "k = torch.rand(b,h,t,d)\n",
        "q = torch.rand(b,h,t,d)\n",
        "\n",
        "k = F.pad(k, (0, 0, w - 1, 0))  # (B, H, T + W - 1, D)\n",
        "v = F.pad(v, (0, 0, w - 1, 0))\n",
        "k = F.pad(x, (0,0,w-1,0)) # [b, h, t+w-1, d]\n",
        "\n",
        "b,h,l,d = k.shape\n",
        "k_strided = k.as_strided(size=(b,h,t,w,d), stride=(k.stride(0), k.stride(1), k.stride(2), k.stride(2), k.stride(3)))\n",
        "v_strided = v.as_strided(size=(b,h,t,w,d), stride=(v.stride(0), v.stride(1), v.stride(2), v.stride(2), v.stride(3)))\n",
        "\n",
        "o = k.as_strided((b,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "\n",
        "attn_scores = torch.einsum(\"bhtd,bhtwd->bhtw\", q, k_strided) / math.sqrt(d)\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "out = torch.einsum(\"bhtw,bhtwd->bhtd\", attn_weights, v_strided)\n",
        "\n",
        "# # b,h,t,d = 2,4,8,5\n",
        "# k = torch.rand(b,h,t,d)\n",
        "# # # print(k.stride(0), k.stride(1), k.stride(2), k.stride(2), k.stride(3))\n",
        "# print(k.stride())\n",
        "# # print(torch.rand(2,3,5,7).stride())\n",
        "# # 3*5*7,5*7,7,1\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.rand(2,3,5)\n",
        "print(x)\n",
        "# # o = torch.as_strided(x, (2, 2), (1, 2))\n",
        "# o = torch.as_strided(x, (2, 5), (1, 3))\n",
        "# print(o)\n",
        "# o = torch.as_strided(x, (2, 5), (2, 3))\n",
        "b,t,d = x.shape\n",
        "w=5\n",
        "k = F.pad(x, (0,0,w-1,0)) # [b, t, t+w-1, d]\n",
        "print(k.shape)\n",
        "# o = k.as_strided(size=(b,t,w,d), stride=(k.stride(0), k.stride(1), k.stride(1), k.stride(2)))\n",
        "# o = k.as_strided((b,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "# o = k.as_strided((b,h,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "\n",
        "# o = k.view(b*h,t+w-1,d).unfold(dimension=1, size=w, step=1).view(b,h,t,w,d)\n",
        "# o = k.view(b,t+w-1,d).unfold(dimension=1, size=w, step=1).view(b,t,w,d)\n",
        "# o = k.unfold(dimension=1, size=w, step=1)#.view(b,t,w,d)\n",
        "o = k.unfold(dimension=-2, size=w, step=1).transpose(-2,-1)\n",
        "\n",
        "print(o.shape, o)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NF9atY_nDOoV"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}