{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/gpt_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hf stream dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "class StreamDataset(Dataset):\n",
        "    def __init__(self, dataset, seq_len, buffer_size):\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.dataset = dataset\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len = seq_len\n",
        "        self.buffer_size = buffer_size  # must be â‰¥ seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            tokens = self.enc.encode(x[\"text\"]) # tiktoken\n",
        "            self.buffer.extend(tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        # /4.5/(4/3)\n",
        "        return 128000000\n",
        "        # return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print('get', idx)\n",
        "        if idx == 0: self.data = iter(self.dataset)\n",
        "        if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "        if len(self.buffer) < self.seq_len:\n",
        "            raise StopIteration\n",
        "        x = self.buffer[:self.seq_len]\n",
        "        self.buffer = self.buffer[self.seq_len:]\n",
        "        # return torch.tensor(x)\n",
        "        return torch.tensor(x, dtype=torch.int32)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # print(batch)\n",
        "    return torch.stack(batch)\n",
        "\n",
        "name = 'Skylion007/openwebtext' if torch.cuda.is_available() else 'stas/openwebtext-10k'\n",
        "\n",
        "dataset = load_dataset(name, trust_remote_code=True, split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "# dataset = load_dataset(\"Skylion007/openwebtext\", trust_remote_code=True, split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "# dataset = load_dataset(\"deepmind/pg19\", trust_remote_code=True, split=\"train\", streaming=True, cache_dir=\"/content/hf\") # 8.7,3.8\n",
        "\n",
        "seq_len = 128*1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "# del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=2)\n",
        "del train_data\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "# print(train_data.vocab_size)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bgu3gbQOFnPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 128*2 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "# del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=2)\n",
        "del train_data\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n"
      ],
      "metadata": {
        "id": "tzT3b50IasRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).to(device) # [seq_len, dim // 2, 2]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "        rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "        x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "        rot_x = x * rot_emb\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "dim=16\n",
        "seq_len=512\n",
        "rope = RoPE(dim, seq_len, base=10000)\n",
        "x = torch.rand(4,64,dim, device=device)\n",
        "out = rope(x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "aZ8q6DxC3P9B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q5bGqKAJzN7I"
      },
      "outputs": [],
      "source": [
        "# @title TTLinear\n",
        "# Tensor Train embedding https://arxiv.org/pdf/1901.10787\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def make_einsum(num_tensors):\n",
        "    a = 97\n",
        "    R = chr(a+25) # 'z'\n",
        "    lhs = [chr(a)+R]\n",
        "    for i in range(1, num_tensors-1):lhs.append(R+chr(a+i)+R)\n",
        "    lhs.append(R+chr(a+num_tensors-1))\n",
        "    return ','.join(lhs) + '->' + ''.join([chr(a+i) for i in range(num_tensors)]) # az,zbz,zcz,zd->abcd\n",
        "\n",
        "class TTLinear(nn.Module):\n",
        "    def __init__(self, in_features=None, out_features=None, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.lfeat = len(in_features)\n",
        "        if self.lfeat==1: lst = in_features + out_features\n",
        "        elif self.lfeat>=2: lst = [i*j for i, j in zip(in_features, out_features)]\n",
        "        last = len(lst)\n",
        "        var = last/rank**(1/(2*(std**.5)*last))\n",
        "        c=1/last\n",
        "        self.params = nn.ParameterList([nn.Parameter(torch.randn(lst[0], rank).clamp(-c,c)*var),\n",
        "            *[nn.Parameter(torch.randn(rank, ij, rank).clamp(-c,c)*var) for ij in lst[1:-1]],\n",
        "            nn.Parameter(torch.randn(rank, lst[-1]).clamp(-c,c)*var)])\n",
        "        self.einsum_str = make_einsum(last)\n",
        "        self.shape = [p for ij in zip(in_features, out_features) for p in ij]\n",
        "        self.permute = list(range(0, 2*self.lfeat - 1, 2)) + list(range(1, 2*self.lfeat, 2))\n",
        "    def weight(self): return torch.einsum(self.einsum_str, *self.params).reshape(self.shape).permute(self.permute).flatten(0,self.lfeat-1).flatten(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.weight()\n",
        "        return x.to(weight.dtype) @ weight\n",
        "\n",
        "def one_hot(x, in_dim):\n",
        "    return torch.zeros((*x.shape,in_dim), dtype=torch.int8, device=x.device).scatter_(-1, x.unsqueeze(-1).to(int), 1)\n",
        "\n",
        "def one_hot(x, in_dim):\n",
        "    b,t = x.shape\n",
        "    o = torch.zeros((b,t,in_dim), dtype=bool, device=x.device)\n",
        "    o[torch.arange(b).unsqueeze(-1),torch.arange(t).unsqueeze(0),x] = True\n",
        "    return o\n",
        "\n",
        "import math\n",
        "class TTEmbedding(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.ttlin = TTLinear(in_dim, d_model, rank, std) # https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "        self.weight = self.ttlin.weight\n",
        "        self.num_classes = math.prod(in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return self.ttlin(F.one_hot(x, self.num_classes))\n",
        "        return self.ttlin(one_hot(x, self.num_classes))\n",
        "# self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "\n",
        "# # in_features=(3,4,5,6); out_features=(2,3,4,5)\n",
        "# in_features=[120]; out_features=[300]\n",
        "# in_features=[12]; out_features=[30]\n",
        "# rank=16\n",
        "# # std=.5\n",
        "# lin = TTLinear(in_features, out_features, rank, std).to(device)\n",
        "# # x = torch.rand(4,math.prod((3,4,5,6)))\n",
        "# x = torch.rand(4,7,math.prod(in_features), device=device)\n",
        "# print(lin.params[0].device)\n",
        "# out = lin(x)\n",
        "# print(out.shape)\n",
        "# print(lin.ttlin.params[0].device)\n",
        "\n",
        "# emb = TTEmbedding(in_features, out_features, rank).to(device)\n",
        "# x = torch.randint(0, math.prod(in_features), (2, 5), device=device)\n",
        "# out = emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# o=lin.weight\n",
        "# print(o.mean().item(), o.std().item(), o.min().item(), o.max().item())\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# # plt.hist(o.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# # plt.hist(o[:100,:100].flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# x = torch.randn(100,100)*std\n",
        "# # plt.hist(x.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# plt.hist([o[:100,:100].flatten().tolist(), x.flatten().tolist()], bins=20, alpha=.5, label='context mask')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sliding window Attention as_strided\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=8, drop=0, w=64):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [b,t,t(+p)]\n",
        "        b,t = x.shape[:2]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # kv = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2)#.chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        kv = torch.cat([self.pos_enc(k),v], dim=-1).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        kv = F.pad(kv, (0,0,self.w-1,0)) # [b, h, t+w-1, d]\n",
        "        # kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "        kv = kv.unfold(dimension=-2, size=self.w, step=1).transpose(-2,-1)\n",
        "        # print('attn fwd kv', kv.shape)\n",
        "\n",
        "        mk, mv = kv.chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "\n",
        "        attn = torch.einsum(\"bhtd,bhtwd->bhtw\", q, mk) * self.scale\n",
        "        # print('attn fwd q mk attn', q.dtype, mk.dtype, attn.dtype)\n",
        "\n",
        "        # if mask != None:\n",
        "        #     # attn = attn.masked_fill(~mmask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        #     attn = attn.masked_fill(~mmask[:,None,:,None], -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,1,t]\n",
        "        attention = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        # out = (self.drop(attention) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "        out = torch.einsum(\"bhtw,bhtwd->bhtd\", self.drop(attention), mv)\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# typically: gen td*td=tt\n",
        "# gen t1d*twd=tw, patch pd*tpd=p\n",
        "\n",
        "b,t,d = 64,100,512\n",
        "# mask = torch.rand(b,t,t, device=device)<.2\n",
        "\n",
        "# causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "# # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "# mask = causal_mask * ~torch.tril(torch.ones_like(causal_mask, dtype=bool, device=device), diagonal=-64) # sliding window mask\n",
        "\n",
        "\n",
        "msk = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(msk, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(ind.shape, ind)\n",
        "mask = torch.zeros_like(msk, dtype=bool)\n",
        "mask[torch.arange(b)[:,None,None], torch.arange(t)[None,:,None], ind] = True\n",
        "\n",
        "\n",
        "# model = Attention(d).to(device) # 257 ms\n",
        "model = Attention(d, w=3).to(device) # 257 ms\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "# out = model(x)\n",
        "out = model(x, mask=mask)\n",
        "print(out.shape)\n",
        "print(time.time()-start)\n",
        "\n",
        "# midx 0.04\n",
        "# swa no mask ie bht1d 8e-5\n",
        "# unfold .0005\n",
        "\n",
        "# unfold 23 + error? 6.8,7.6\n",
        "# as strided 24.518250703811646 # 6.8,8.8\n",
        "\n",
        "# unfold\n",
        "# this is what they just improve thesan that the characteristic is huge behavior down the artists by our treative origins of delivery real specialist is instant connected to- (which is never symptoms, or limited nationals when established surgery for developers to help for apps clearlyosc ripple shows without the choice of documentary about them by growing opportunities to work out about\n",
        "# 12300 time: 23.157159328460693 0.0018825428862463565\n",
        "# strain 6.319660663604736\n",
        "# this is what is not closer further is in technology should not be about desk, such firmware our comic situation it haseeper moment on that is never for duty to deal with real- weaknesses.\n",
        "\n",
        "# â€œThe difference among Jitt natural code super departure testing is not seem to be loaded devices to allow information and then such as ï¿½\n",
        "# 12400 time: 23.128992080688477 0.0018650909247258797\n",
        "# strain 6.408341884613037\n",
        "# this is what these moreilitating our light and love your benefit.\n",
        "\n",
        "# What is from all in imaryichn, which are more than your regular factors can see both takes aHTp hydro Beyonees include, intoEngine frequently potential attackers to genBOOK to own up the ha portrait higher and promptingieine itemiques athlete.\n",
        "\n"
      ],
      "metadata": {
        "id": "W2e8O_J2_rLt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "b,h,t,d = 2,3,4,5\n",
        "# q = torch.rand(b,h,t,d, device=device)\n",
        "# k = torch.rand(b,h,t,d, device=device)\n",
        "# v = torch.rand(b,h,t,d, device=device)\n",
        "# mask = torch.rand(b,t,t, device=device)>0.5\n",
        "print(mask)\n",
        "\n",
        "out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask.unsqueeze(1) if mask != None else None, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "# out = F.scaled_dot_product_attention(q, k, v, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "# out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "print(out)\n",
        "scale = d**-0.5\n",
        "attn = q @ k.transpose(-2,-1) * scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "# if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "# print(attn.shape, mask.shape)\n",
        "if mask != None: attn = attn.masked_fill(~mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "# print(attn, mask)\n",
        "print(attn)\n",
        "# if mask != None: attn = attn.masked_fill(mask.unsqueeze(1), 0) # [b,t,t]->[b,1,t,t]\n",
        "attention = torch.softmax(attn, dim=-1)\n",
        "# out = self.drop(attention) @ v # [batch, n_heads, T, d_head]\n",
        "out = attention @ v # [batch, n_heads, T, d_head]\n",
        "print(out)\n",
        "\n",
        "# [0.5788, 0.1438, 0.5807, 0.4964, 0.9362],\n",
        "# [0.5788, 0.1438, 0.5807, 0.4964, 0.9362],\n",
        "# [0.7777, 0.0903, 0.5304, 0.6868, 0.6610],\n",
        "# [0.6023, 0.4530, 0.6115, 0.6318, 0.7990]\n",
        "\n",
        "# [0.7057, 0.3609, 0.6557, 0.6586, 0.6254],\n",
        "# [0.6914, 0.3631, 0.6448, 0.6534, 0.6553],\n",
        "# [0.7140, 0.3456, 0.6367, 0.6716, 0.6285],\n",
        "# [0.7121, 0.3514, 0.6353, 0.6733, 0.6311]"
      ],
      "metadata": {
        "id": "JL-UyGidrr6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(attn[0])\n",
        "attention = torch.softmax(attn, dim=-1)\n",
        "print(attention)"
      ],
      "metadata": {
        "id": "sRF3-foZv4JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Attention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0):\n",
        "        super().__init__()\n",
        "        # self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_model // n_heads\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=10000) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.q.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        # k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        k = self.pos_enc(k).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        v = v.unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "\n",
        "        # # (quadratic) attention # Softmax(q @ k.T) @ v\n",
        "        # out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask.unsqueeze(1) if mask != None else None, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # # out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        attn = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        if mask != None: attn = attn.masked_fill(~mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        out = self.drop(attention) @ v # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1,2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "        # torch.nn.init.normal_(self.lin0.weight, std=.02)\n",
        "        torch.nn.init.normal_(self.lin0.weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "# 2048*2\n",
        "# 2048*7\n",
        "# ff: d_model*2 *ff_dim params\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, d_head, ff_dim=None, drop=0.):\n",
        "    def __init__(self, d_model, n_heads ,cond_dim=None, ff_dim=None, drop=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.cond_dim = cond_dim\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        if cond_dim!=None: self.norm2 = nn.RMSNorm(cond_dim)\n",
        "        # self.drop = nn.Dropout(drop)\n",
        "        self.attn = MultiHeadAttention(d_model, cond_dim, n_heads=n_heads, d_head=d_model//n_heads, drop=drop)\n",
        "        # self.attn = Attention(d_model, cond_dim, n_heads=n_heads, d_head=d_model//n_heads, drop=drop, w=64)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # torch.nn.init.normal_(self.ff[1].weight, std=.02)\n",
        "        torch.nn.init.normal_(self.ff[1].weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # print('attblk fwd', x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        if self.cond_dim==None: x = x + self.attn(self.norm1(x), mask=mask)\n",
        "        else: x = x + self.attn(self.norm1(x), self.norm2(cond), mask) # maybe no res for decoder\n",
        "        x = x + self.ff(x) # maybe no ff for decoder?\n",
        "        return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "b,t,d = 2,5,16\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "mask = torch.rand(b,t,t, device=device)>0\n",
        "model = AttentionBlock(d_model=d, n_heads=4, ff_dim=16).to(device)\n",
        "# model = nn.Sequential(*[AttentionBlock(d_model=d, n_heads=4, ff_dim=16) for _ in range(2)])\n",
        "out =  model(x, mask)\n",
        "# out =  model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n",
        "# # model = MultiHeadAttention(d).to(device) # 257 ms\n",
        "# x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "# # out = model(x)\n",
        "# out = model(x, mask=mask)\n",
        "# print(out.shape)\n",
        "# print(time.time()-start)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ygkv7B71JHP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        # self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.tok_emb = TTEmbedding([29, 1733], [64,1], rank=min(d_model,256))\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        # self.out = nn.Linear(d_model, out_dim)\n",
        "        self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "        self.w = 64\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        # x = self.pos_enc(self.tok_emb(x))\n",
        "        x = self.tok_emb(x)\n",
        "        b,t,d = x.shape\n",
        "\n",
        "        causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "        mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "\n",
        "        x = self.encoder(x, x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "# model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=3).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# x = torch.randint(0, input_size, (2, 5), device=device)\n",
        "# x = torch.randint(0, input_size, (64, 128), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# strain 5.660660743713379\n",
        "# this is what we've done it. I'm able to be something strong. I think their information can be highly specific to a busy intertwined. But why \"Effects\"/ who shouldn't do,\" suppose it would improve them.Earlier open in both countries is sent by documentation, who were involved by custom ways to cutting the carbon gas\n",
        "# strain 5.939525604248047\n",
        "# this is what Iâ€™d like heâ€™s very important to learn, itâ€™s a bug my liquid had nothing once his streaming user in their own. Itâ€™s enough to run the survey, if thereâ€™s also no very big interested in a very $2. Itâ€™s sufficient's\n",
        "# strain 5.96616792678833\n",
        "# this is what manyatform are neither even documented and looked down their intentions in writers in so far savigTS-ve nuite was never known that some dioxide the sizes celebrate was fighting, as guilty or innovation.672 Expressia lives to Islamic Cheney of your existence takes a free background rest: The book she probably read it or find\n",
        "# strain 5.811507225036621\n",
        "# this is what it has not seen in intelligence and have believed in a romantic order and possible and rapidly now. Many growing economic ear is unclear whether they can do that the country's job. According to a proposals, the majority of domestic trade on which are the greatest mission with rich law by people, violated stockrol, which they would\n",
        "\n",
        "\n",
        "# b64,l128 8.0ram,10.0gpu l128*2:oom\n",
        "\n",
        "# b64,l128 w64 8.2ram,6.8gpu\n",
        "# b64,l128*2 w64 8.1ram,14.0gpu\n",
        "\n",
        "# gpt 1lyr mha ropein100 @attn # low loss but not learning\n",
        "# gpt 1lyr mha ropeout10000 F.attn # 19.3s\n",
        "# gpt 1lyr mha ropein10000 @attn # not learning\n",
        "# gpt 1lyr mha ropein10000 F.attn # this is what you are something else all in course, you're simply went back over their news, the reaction of the French source that is proven as we did not change any life at each power because they have the which shall noteded in order to change.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4J73LuO9XUcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JNGU1V3XPFUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"gpt\", config={\"model\": \"res18\",}) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCN055Zr7Dq4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train test generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "\n",
        "def strain(model, dataloader, optimizer, scheduler=None): # train function with automatic mixed precision\n",
        "    start = begin = time.time()\n",
        "    model.train()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x, y = x[:,:-1], x[:,1:]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            # causal_mask = torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1) # for F.scaled_dot_product_attention\n",
        "            # # causal_mask = ~torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1)\n",
        "            # logits = model(x, mask=causal_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            logits = model(x) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "            # loss = F.cross_entropy(logits.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(\"strain\",loss.item())\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            start = begin = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            output = model(x)\n",
        "        output = output[:, -1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    return completion\n",
        "\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    # print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(model, \"this is what\"))"
      ],
      "metadata": {
        "id": "33Grqz8SiKPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## drawer"
      ],
      "metadata": {
        "id": "knt9jqUmgl97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RNN pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.emb = nn.Embedding(in_dim, d_model)\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # for p in self.parameters():\n",
        "        #     if p.dim() > 1:\n",
        "        #         nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "        x = self.emb(x)\n",
        "        if hc is None:\n",
        "            h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "            c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        else: h0,c0 = hc\n",
        "        # print(x.shape, h0.shape,c0.shape)\n",
        "        out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, (h, c)\n",
        "\n",
        "    def forward(self, x, h=None): # rnn/gru\n",
        "        x = self.emb(x)# * self.d_model**.5\n",
        "        if h is None: h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        else: h0 = h\n",
        "        # print(x.shape, h0.shape)\n",
        "        out, h = self.rnn(x, h0)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h\n",
        "\n",
        "hidden_size = 64 #128\n",
        "num_layers = 1#2\n",
        "input_size = num_classes = 50#train_data.vocab_size#65\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# print(model)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# 128,2\n",
        "# Test Loss: 6.360389362062727\n",
        "# this is what ween new york is well it a sign more directly into simply shares\n",
        "# 0 time: 5.910429954528809 5.910431623458862\n",
        "# 64,2\n",
        "# Test Loss: 6.167358561924526\n",
        "# this is what bull morp on its aftant opereals of a b. the hamally plans beces\n",
        "# 0 time: 5.655158996582031 5.655160903930664\n",
        "# 64,1\n",
        "# Test Loss: 5.823708357129778\n",
        "# this is what achan agrive\n",
        "#  the guinesst on of promjects cl funds that jound\n",
        "# 0 time: 4.788918495178223\n",
        "\n",
        "# Test Loss: 8.247572830745153\n",
        "# this is what that has months with unfinsings lide to N by well\n",
        "#  next offited\n",
        "# 29 time: 3.902247905731201 4.377542002995809\n",
        "\n",
        "b,t=2,5\n",
        "\n",
        "x = torch.randint(0,input_size, (b,t), device=device)\n",
        "h = torch.rand(num_layers, b, hidden_size, device=device)\n",
        "out, h = model(x, h)\n",
        "print(out.shape, h.shape)\n"
      ],
      "metadata": {
        "id": "Q5CjHMaQE8MM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCgV-efIfzne",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title mask translate\n",
        "PAD_IDX=0\n",
        "def make_src_mask(src):\n",
        "    # return (src != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device) # [batch_size, 1, 1, src_len]?\n",
        "    return (src != PAD_IDX)[:,None,None,:].to(device) # [batch_size, 1, 1, src_len]?\n",
        "\n",
        "# attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "def make_trg_mask(trg):\n",
        "    # trg_pad_mask = (trg != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device)\n",
        "    trg_pad_mask = (trg != PAD_IDX)[:,None,None,:].to(device) # [batch, 1, 1, trg_len]\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
        "    # print('make_trg_mask', trg_pad_mask.shape, trg_sub_mask.shape) # [64, 1, 1, 10], [10, 10]\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask # [batch, 1, trg_len, trg_len]?\n",
        "    return trg_mask\n",
        "\n",
        "def translate(model, src_sentence):\n",
        "    model.eval()\n",
        "    src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "    num_tokens = src.shape[1]\n",
        "    trg_indexes = [BOS_IDX]\n",
        "    max_len = src.shape[1]+5\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output = model(src, trg_tensor, src_mask, trg_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == EOS_IDX: break\n",
        "    trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "    return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "def translate_fast(model, src_sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "        num_tokens = src.shape[1]\n",
        "        trg_indexes = [BOS_IDX]\n",
        "        max_len = src.shape[1]+5\n",
        "        src_mask = make_src_mask(src)\n",
        "        output = model.encode(src, src_mask)\n",
        "        for i in range(max_len):\n",
        "            trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "            trg_mask = make_trg_mask(trg_tensor)\n",
        "            output = model.decode(src, trg_tensor, src_mask, trg_mask)\n",
        "            pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "            trg_indexes.append(pred_token)\n",
        "            if pred_token == EOS_IDX: break\n",
        "        trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "        return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "# UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "src, trg = torch.randint(0, 100, (64, 10)), torch.randint(0, 100, (64, 10))\n",
        "sm = make_src_mask(src)\n",
        "tm = make_trg_mask(trg)\n",
        "# print(sm.shape, tm.shape) # [64, 1, 1, 10], [64, 1, 10, 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9R4hVUCxGiu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title translate train test\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "        trg_input = trg[:,:-1]\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "        print('train', src.shape, trg.shape, src_mask.shape, trg_mask.shape)\n",
        "        output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(list(dataloader))\n",
        "\n",
        "def test(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "            trg_input = trg[:,:-1]\n",
        "            src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "            output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(list(dataloader))\n",
        "\n",
        "# @title run\n",
        "import time\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) # lr=0.0001\n",
        "\n",
        "# for epoch in range(20):\n",
        "for epoch in range(1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = test(model, val_loader, loss_fn)\n",
        "    end_time = time.time()\n",
        "    print((f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    # print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people standing in front of an igloo .\n",
        "    # @title inference\n",
        "    print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people stand in front of an igloo .\n",
        "    print(translate(model, \"Ein Koch in weiÃŸer Uniform bereitet Essen in einer RestaurantkÃ¼che zu .\")) # A chef in a white uniform prepares food in a restaurant kitchen .\n",
        "    print(translate(model, \"Zwei junge MÃ¤dchen spielen FuÃŸball auf einem Feld. .\")) # Two young girls play soccer on a field. .\n",
        "    print(translate(model, \"Eine Frau mit Hut und Sonnenbrille steht am Strand .\")) # A woman wearing a hat and sunglasses stands on the beach .\n",
        "    print(translate(model, \"Zwei Freunde lachen und genieÃŸen ein Eis auf einer wunderschÃ¶nen Wiese .\")) # Two friends laugh and enjoy ice cream on a beautiful meadow .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Epoch: 1, Train loss: 5.402, Val loss: 4.186, Epoch time = 41.608s\n",
        "A group of people are are are are are are in a .\n",
        "Epoch: 2, Train loss: 3.898, Val loss: 3.545, Epoch time = 41.068s\n",
        "A group of people are standing in a crowd of people .\n",
        "Epoch: 3, Train loss: 3.353, Val loss: 3.125, Epoch time = 41.566s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 4, Train loss: 2.944, Val loss: 2.830, Epoch time = 40.756s\n",
        "A group of people standing in front of a building .\n",
        "Epoch: 5, Train loss: 2.630, Val loss: 2.596, Epoch time = 41.468s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 6, Train loss: 2.375, Val loss: 2.429, Epoch time = 41.023s\n",
        "A group of people standing in front of a house .\n",
        "Epoch: 7, Train loss: 2.166, Val loss: 2.307, Epoch time = 41.604s\n",
        "A group of people stand in front of a house .\n",
        "Epoch: 8, Train loss: 1.984, Val loss: 2.210, Epoch time = 40.876s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 9, Train loss: 1.834, Val loss: 2.131, Epoch time = 41.496s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 10, Train loss: 1.698, Val loss: 2.079, Epoch time = 41.052s\n",
        "A group of people are standing in front of an empty house .\n",
        "Epoch: 11, Train loss: 1.576, Val loss: 2.038, Epoch time = 41.570s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 12, Train loss: 1.475, Val loss: 2.033, Epoch time = 41.067s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 13, Train loss: 1.381, Val loss: 2.017, Epoch time = 41.576s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 14, Train loss: 1.292, Val loss: 1.977, Epoch time = 40.779s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 15, Train loss: 1.213, Val loss: 1.948, Epoch time = 41.461s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 16, Train loss: 1.139, Val loss: 1.940, Epoch time = 40.800s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 17, Train loss: 1.073, Val loss: 1.940, Epoch time = 41.533s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 18, Train loss: 1.006, Val loss: 1.939, Epoch time = 40.856s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 19, Train loss: 0.944, Val loss: 1.947, Epoch time = 41.345s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 20, Train loss: 0.893, Val loss: 1.956, Epoch time = 40.935s\n",
        "A group of people stand in front of an igloo\n",
        "'''"
      ],
      "metadata": {
        "id": "x-bHqmV4ReUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title bleu\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    for datum in data:\n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "    return bleu_score(pred_trgs, trgs)\n",
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')\n",
        "# 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model.\n",
        "\n",
        "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "\n",
        "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    translations_done = [0] * len(src_tensor)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_tokens = output.argmax(2)[:,-1]\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\n",
        "            trg_indexes[i].append(pred_token_i)\n",
        "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                translations_done[i] = 1\n",
        "        if all(translations_done):\n",
        "            break\n",
        "\n",
        "    # Iterate through each predicted example one by one;\n",
        "    # Cut-off the portion including the after the <eos> token\n",
        "    pred_sentences = []\n",
        "    for trg_sentence in trg_indexes:\n",
        "        pred_sentence = []\n",
        "        for i in range(1, len(trg_sentence)):\n",
        "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                break\n",
        "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
        "        pred_sentences.append(pred_sentence)\n",
        "    return pred_sentences, attention\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            _trgs = []\n",
        "            for sentence in trg:\n",
        "                tmp = []\n",
        "                # Start from the first token which skips the <start> token\n",
        "                for i in sentence[1:]:\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
        "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
        "                        break\n",
        "                    tmp.append(trg_field.vocab.itos[i])\n",
        "                _trgs.append([tmp])\n",
        "            trgs += _trgs\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
        "            pred_trgs += pred_trg\n",
        "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3vBEfUjzuobW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## old"
      ],
      "metadata": {
        "id": "NF9atY_nDOoV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAyqkMjq6T2C"
      },
      "outputs": [],
      "source": [
        "# Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://www.mihaileric.com/posts/transformers-attention-in-disguise/\n",
        "# https://jalammar.github.io/illustrated-transformer/\n",
        "# http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "# position embedding <-> \"vocabulary\" size 100 <-> model can accept sentences up to 100 tokens long\n",
        "# learned positional encoding, warm-up and cool-down steps, label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V1teyZuwff9_"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64c91cf87c13c0e83586b8e66e4d74e/translation_transformer.ipynb\n",
        "\n",
        "# https://github.com/pytorch/data\n",
        "# %pip install portalocker\n",
        "# %pip install torchdata\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -qU torchdata torchtext\n",
        "!pip install -qU spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "!git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset\n",
        "\n",
        "# !pip list | grep torch\n",
        "!pip list | grep python\n",
        "\n",
        "import torchtext.datasets as datasets\n",
        "\n",
        "# Load the Multi30k dataset\n",
        "train_iter, valid_iter, test_iter = datasets.Multi30k(split=('train', 'valid', 'test'))\n",
        "\n",
        "# Iterate through the dataset\n",
        "for src, tgt in train_iter:\n",
        "    print(f\"Source: {src}\")\n",
        "    print(f\"Target: {tgt}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BFat7RgKSwR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "# modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "de_vocab.set_default_index(UNK_IDX)\n",
        "en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# vocab_transform = {SRC_LANGUAGE:de_vocab, TRG_LANGUAGE:en_vocab}\n",
        "# text_transform = {SRC_LANGUAGE:de_transform, TRG_LANGUAGE:en_transform}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q datasets\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "!python -m spacy download de_core_news_sm\n"
      ],
      "metadata": {
        "id": "gURN1FYyM-A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hf data\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a translation dataset (e.g., WMT English-German)\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "\n",
        "# Access train and validation splits\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "\n",
        "# # Example of accessing data\n",
        "# # for example in train_data:\n",
        "# #     print(example['translation'])\n",
        "# batch_size = 128 # 128\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "# from torchtext.datasets import multi30k, Multi30k\n",
        "# # modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "# de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "# en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# # Create a blank Tokenizer with just the English vocab\n",
        "# tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# # Construction 2\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# tokenizer = nlp.tokenizer\n",
        "\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "import spacy\n",
        "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "de_tokenizer = spacy.load(\"de_core_news_sm\") # https://spacy.io/models/de\n",
        "\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# from torchtext.vocab import build_vocab_from_iterator\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "# de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# de_vocab.set_default_index(UNK_IDX)\n",
        "# en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "\n",
        "print(\"en_tokens\", en_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "# val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J-zgIl9FMgPV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_seq_length=512):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "class LearntPosEnc(nn.Module): # learnt positional embeddings\n",
        "    def __init__(self, d_model, dropout=0.1, max_length=512):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, src_len = x.shape\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # [batch size, src len]\n",
        "        return self.drop(x + self.pos_embedding(pos))\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        # theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "        theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
        "        # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = pos * theta # [seq_len, 1] * [dim // 2] = [seq_len, dim // 2]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: print(\"rot_emb.shape[0] < seq_len\")\n",
        "        rot_emb = self.rot_emb[:seq_len, :].unsqueeze(0).expand(batch, -1, -1)\n",
        "        x = x.view(batch, seq_len, dim // 2, 2)\n",
        "        rot_emb = rot_emb.view(batch, seq_len, dim // 2, 2)\n",
        "        # rot_x = torch.einsum('...ij,...ij->...ij', x, rot_emb)\n",
        "        rot_x = x * rot_emb\n",
        "        # return rot_x.view(*rot_x.shape[:-2], dim)\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        x = self.drop(attention) @ V # x = torch.matmul(self.drop(attention), V)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.norm1(src + self.drop(self.self_attn(src, src, src, src_mask)[0]))\n",
        "        src = self.norm2(src + self.drop(self.ff(src)))\n",
        "        return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.norm3 = nn.RMSNorm(d_model)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.enc_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.norm1(trg + self.drop(self.self_attn(trg, trg, trg, trg_mask)[0]))\n",
        "        trg = self.norm2(trg + self.drop(self.enc_attn(trg, enc_src, enc_src, src_mask)[0]))\n",
        "        trg = self.norm3(trg + self.drop(self.ff(trg)))\n",
        "        return trg\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        return trg\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, enc_layers, nhead, ff_dim, dropout)\n",
        "        self.decoder = Decoder(d_model, dec_layers, nhead, ff_dim, dropout)\n",
        "        # self.pos_enc = PositionalEncoder(d_model, dropout=dropout)\n",
        "        # self.pos_enc = LearntPosEnc(d_model, dropout=dropout)\n",
        "        self.pos_enc = RoPE(d_model)\n",
        "        self.src_tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.trg_tok_emb = nn.Embedding(out_dim, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src = self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model))\n",
        "        trg = self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model))\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.lin(output)\n",
        "        return output\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        return self.encoder(self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model)), src_mask)\n",
        "\n",
        "    def decode(self, trg, memory, trg_mask=None, src_mask=None):\n",
        "        trg = self.decoder(self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model)), memory, trg_mask, src_mask)\n",
        "        return self.lin(trg)\n",
        "\n",
        "\n",
        "in_dim = 50#\n",
        "out_dim = 50\n",
        "model = Seq2Seq(in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1).to(device)\n"
      ],
      "metadata": {
        "id": "KeuV4EZyqXj5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Attention with kvcache window\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# mask = torch.rand(b,t,t)<.2\n",
        "def to_midx(mask): # [b,t,t+p] bool mask\n",
        "    b,t = mask.shape[:2]\n",
        "    idx = mask.nonzero(as_tuple=True)\n",
        "    tk = torch.stack(idx[:-1], dim=-1).to(device)\n",
        "    tk = torch.cat([torch.tensor([True], device=device), (tk[:-1]!=tk[1:]).any(-1)], dim=0)\n",
        "    cc = len(idx[0])\n",
        "    positions = torch.arange(cc, device=device)\n",
        "    last_reset_pos = torch.zeros(cc, dtype=int, device=device)\n",
        "    last_reset_pos[tk] = positions[tk]\n",
        "    out = positions - last_reset_pos.cummax(0).values\n",
        "    c=out.max()+1\n",
        "    y = torch.full((b,t,c), -1, device=device)\n",
        "    y[*idx[:-1],out] = idx[-1]\n",
        "    return y # [b,t,c]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=8, drop=0, w=64):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "        t=128\n",
        "        self.midx = (torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)).to(device) # [t,w,1]\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [b,t,t(+p)]\n",
        "        b,t = x.shape[:2]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # kv = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2)#.chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        kv = torch.cat([self.pos_enc(k),v], dim=-1).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        import time\n",
        "        start = time.time()\n",
        "\n",
        "        # if mask != None:\n",
        "            # midx = to_midx(mask) # [b,t,w]\n",
        "            # # print('mask', q.shape, mask.shape, midx.shape)\n",
        "            # kv = kv[torch.arange(b)[:,None,None,None], torch.arange(self.n_heads)[None,:,None,None], midx.unsqueeze(1)] # [b,h,t,w,d]\n",
        "\n",
        "            # b,t,d = x.shape\n",
        "            # if t>len(self.midx): self.midx = (torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)).to(device) # [t,w,1]\n",
        "            # midx = self.midx[:t,-min(self.w,t):] # [t,w,1]\n",
        "            # kv = kv[torch.arange(b)[:,None,None,None], torch.arange(self.n_heads)[None,:,None,None], midx[None,None,...]]\n",
        "\n",
        "\n",
        "        kv = F.pad(kv, (0,0,self.w-1,0)) # [b, h, t+w-1, d]\n",
        "        # kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "        kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "\n",
        "\n",
        "            # mmask = midx>=0 # F->mask # [t,c]\n",
        "            # mmask = (midx>=0).unsqueeze(0) # F->mask # [1,t,c]\n",
        "        # else: kv = kv.unsqueeze(3).repeat(1,1,1,t,1) # [b,h,t,t(w),d]\n",
        "        print('midx', time.time()-start)\n",
        "\n",
        "# [batch, n_heads, T, d_head]\n",
        "# [b,h,t,d] @ [b,h,d,t] = [b,h,t,t]\n",
        "\n",
        "# [batch, n_heads, T, d_head]\n",
        "# [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "\n",
        "        mk, mv = kv.chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "        # del kv\n",
        "        # attn fwd q mk attn torch.Size([64, 8, 127, 8]) torch.Size([64, 8, 100, 64, 8])\n",
        "        # print('attn fwd q mk attn', q.shape, mk.shape, kv.shape)\n",
        "        # attn = q.unsqueeze(3) @ mk.transpose(-2,-1) * self.scale # [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "        # print('attn fwd q mk attn', q.shape, mk.shape, attn.shape, mmask.shape)\n",
        "        # q mk attn [64, 8, 100, 8], [64, 8, 100, 3, 8], [64, 8, 100, 1, 3], [64, 100, 3])\n",
        "        # q mk attn [64, 8, 100, 8], [64, 8, 100, 64, 8], [64, 8, 100, 1, 64], [100, 64])\n",
        "# attn fwd q mk attn torch.Size([64, 8, 100, 8]) torch.Size([64, 8, 100, 3, 8]) torch.Size([64, 8, 100, 1, 3]) torch.Size([100, 3])\n",
        "\n",
        "        attn = torch.einsum(\"bhtd,bhtwd->bhtw\", q, mk) * self.scale\n",
        "        # print('attn fwd q mk attn', q.dtype, mk.dtype, attn.dtype)\n",
        "\n",
        "        # if mask != None:\n",
        "        #     # attn = attn.masked_fill(~mmask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        #     attn = attn.masked_fill(~mmask[:,None,:,None], -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,1,t]\n",
        "        attention = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        # out = (self.drop(attention) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "        out = torch.einsum(\"bhtw,bhtwd->bhtd\", self.drop(attention), mv)\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# td*dt->tt t(2t+d)\n",
        "# td*twd->tw t(~wd-(w+2)d)\n",
        "# iff 2t>wd\n",
        "# gen: xqkv, patch: ~xkv ~pq, gen: xqkv pkv\n",
        "# calc xqkv, calc ~xkv ~pq, get xqkv calc pkv\n",
        "\n",
        "# typically: gen td*td=tt\n",
        "# gen t1d*twd=tw, patch pd*tpd=p\n",
        "\n",
        "\n",
        "b,t,d = 64,100,512\n",
        "# mask = torch.rand(b,t,t, device=device)<.2\n",
        "\n",
        "\n",
        "# causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "# # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "# mask = causal_mask * ~torch.tril(torch.ones_like(causal_mask, dtype=bool, device=device), diagonal=-64) # sliding window mask\n",
        "\n",
        "\n",
        "msk = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(msk, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(ind.shape, ind)\n",
        "mask = torch.zeros_like(msk, dtype=bool)\n",
        "mask[torch.arange(b)[:,None,None], torch.arange(t)[None,:,None], ind] = True\n",
        "\n",
        "\n",
        "# model = Attention(d).to(device) # 257 ms\n",
        "model = Attention(d, w=3).to(device) # 257 ms\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "# out = model(x)\n",
        "out = model(x, mask=mask)\n",
        "print(out.shape)\n",
        "print(time.time()-start)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zqXMOvhvV1CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title torch.profiler  as_strided unfold\n",
        "\n",
        "import torch\n",
        "b,t = 2,8\n",
        "mask = torch.rand(b,t,t)<.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        # to_midx(mask)\n",
        "        midx = midx[:t,-min(w,t):] # [t,w,1]\n",
        "        h=4\n",
        "        d=5\n",
        "        kv = torch.rand(b,h,t,d)\n",
        "        kv = kv[torch.arange(b)[:,None,None,None], torch.arange(h)[None,:,None,None], midx[None,None,...]]\n",
        "\n",
        "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "print(prof.key_averages().table())\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "# print(time.time()-start)\n",
        "\n",
        "x = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(x, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(c)\n",
        "b,h,t,d = 2,4,8,5\n",
        "k = torch.rand(b,h,t,d)\n",
        "k = torch.rand(b,h,t,d)\n",
        "q = torch.rand(b,h,t,d)\n",
        "\n",
        "k = F.pad(k, (0, 0, w - 1, 0))  # (B, H, T + W - 1, D)\n",
        "v = F.pad(v, (0, 0, w - 1, 0))\n",
        "k = F.pad(x, (0,0,w-1,0)) # [b, h, t+w-1, d]\n",
        "\n",
        "b,h,l,d = k.shape\n",
        "k_strided = k.as_strided(size=(b,h,t,w,d), stride=(k.stride(0), k.stride(1), k.stride(2), k.stride(2), k.stride(3)))\n",
        "v_strided = v.as_strided(size=(b,h,t,w,d), stride=(v.stride(0), v.stride(1), v.stride(2), v.stride(2), v.stride(3)))\n",
        "\n",
        "o = k.as_strided((b,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "\n",
        "attn_scores = torch.einsum(\"bhtd,bhtwd->bhtw\", q, k_strided) / math.sqrt(d)\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "out = torch.einsum(\"bhtw,bhtwd->bhtd\", attn_weights, v_strided)\n",
        "\n",
        "# # b,h,t,d = 2,4,8,5\n",
        "# k = torch.rand(b,h,t,d)\n",
        "# # # print(k.stride(0), k.stride(1), k.stride(2), k.stride(2), k.stride(3))\n",
        "# print(k.stride())\n",
        "# # print(torch.rand(2,3,5,7).stride())\n",
        "# # 3*5*7,5*7,7,1\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.rand(2,3,5)\n",
        "print(x)\n",
        "# # o = torch.as_strided(x, (2, 2), (1, 2))\n",
        "# o = torch.as_strided(x, (2, 5), (1, 3))\n",
        "# print(o)\n",
        "# o = torch.as_strided(x, (2, 5), (2, 3))\n",
        "b,t,d = x.shape\n",
        "w=5\n",
        "k = F.pad(x, (0,0,w-1,0)) # [b, t, t+w-1, d]\n",
        "print(k.shape)\n",
        "# o = k.as_strided(size=(b,t,w,d), stride=(k.stride(0), k.stride(1), k.stride(1), k.stride(2)))\n",
        "# o = k.as_strided((b,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "# o = k.as_strided((b,h,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "\n",
        "# o = k.view(b*h,t+w-1,d).unfold(dimension=1, size=w, step=1).view(b,h,t,w,d)\n",
        "# o = k.view(b,t+w-1,d).unfold(dimension=1, size=w, step=1).view(b,t,w,d)\n",
        "# o = k.unfold(dimension=1, size=w, step=1)#.view(b,t,w,d)\n",
        "o = k.unfold(dimension=-2, size=w, step=1).transpose(-2,-1)\n",
        "\n",
        "print(o.shape, o)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gLnVZ-hYf97r",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "knt9jqUmgl97",
        "NF9atY_nDOoV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}