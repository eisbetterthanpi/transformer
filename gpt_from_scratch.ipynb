{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/gpt_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgu3gbQOFnPv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title hf byte dataset me\n",
        "!pip install -qU datasets # restart?\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class StreamDataset(IterableDataset):\n",
        "    def __init__(self, dataset, seq_len=129, buffer_size=1024):\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        self.data = iter(dataset)\n",
        "        self.seq_len, self.buffer_size = seq_len, buffer_size  # must be ≥ seq_len\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            x = next(self.data)\n",
        "            # print(x)\n",
        "            self.buffer.extend(x['text'].encode(\"utf-8\"))\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "            if len(self.buffer) < self.seq_len: return # raise StopIteration\n",
        "            x, self.buffer = self.buffer[:self.seq_len], self.buffer[self.seq_len:]\n",
        "            yield torch.tensor(x, dtype=torch.int32) # uint8 int32\n",
        "\n",
        "from datasets import load_dataset\n",
        "name = 'Skylion007/openwebtext' if torch.cuda.is_available() else 'stas/openwebtext-10k'\n",
        "dataset = load_dataset(name, split=\"train\", streaming=True, revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "\n",
        "seq_len = 128*1+1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "# del dataset\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "# del train_data\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/tokenizers/blt_tokenizer.py#L137\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.uint8)#, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device).unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "\n",
        "# for x in train_loader:\n",
        "#     print(x.shape, x)\n",
        "#     break\n",
        "# print(decode(x[0][:64]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets"
      ],
      "metadata": {
        "id": "W960cRyrWxfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title yelp data\n",
        "# !pip install -qU datasets\n",
        "from datasets import load_dataset\n",
        "# # dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"train\", streaming=True, revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "train_dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"train\", revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "test_dataset = load_dataset(\"Yelp/yelp_review_full\", split=\"test\", revision='refs/convert/parquet', cache_dir=\"/content/hf\")\n",
        "# dataset = load_dataset(\"yelp_polarity\") # yelp_polarity yelp_review_full\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32)#.unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "# from torch.utils.data import IterableDataset\n",
        "class ByteDataset(Dataset):\n",
        "# class ByteDataset(IterableDataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        # self.data = iter(data)\n",
        "        # self.data = self.data_process(data) # list of int\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        # self.num_classes = 5\n",
        "        # set(dataset['label'])\n",
        "        labels = sorted(list(set(data['label'])))\n",
        "        self.num_classes = len(labels) #\n",
        "        self.stoi = {ch:i for i,ch in enumerate(labels)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(labels)}\n",
        "        self.max_len = 128\n",
        "    # def data_process(self, data): # str 10780437\n",
        "    #     # return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        xy = self.data[idx]\n",
        "        x, y = xy['text'], xy['label']\n",
        "        idx = torch.randint(max(1, len(x)-self.max_len+1), size=(1,))\n",
        "        x = x[idx: idx+self.max_len]\n",
        "        return encode(x), self.stoi[y]\n",
        "        # return encode(xy['text']), xy['label']\n",
        "    # def __iter__(self):\n",
        "    #     while True:\n",
        "    #         xy = next(self.data)\n",
        "    #         yield encode(xy['text']), xy['label']\n",
        "\n",
        "train_data = ByteDataset(train_dataset)\n",
        "test_data = ByteDataset(test_dataset)\n",
        "\n",
        "pad_idx = 0\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def collate_fn(batch):\n",
        "    x, y = zip(*batch)\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=pad_idx, padding_side='left')\n",
        "    # print(x,y)\n",
        "    return x, torch.tensor(y).unsqueeze(-1)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 if torch.cuda.is_available() else 16 #64 512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "\n",
        "# for i,(x,y) in enumerate(train_loader):\n",
        "#     # print(x.shape, x)\n",
        "#     print(i, x, y)\n",
        "#     break\n",
        "# print(decode(x[3]))\n"
      ],
      "metadata": {
        "id": "yVvZkFhofXNL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# o = decode(x[0])\n",
        "# print(o)\n",
        "# print(encode(o))\n",
        "seq_len = 2**9+1 # 128\n",
        "buffer_size = seq_len*1\n",
        "train_data = StreamDataset(dataset, seq_len, buffer_size) # train_data = StreamDataset(dataset[\"train\"], seq_len, buffer_size)\n",
        "batch_size = 64\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=2)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=0)\n",
        "# for i, x in enumerate(train_loader):\n",
        "#     # print(x.shape, x)\n",
        "#     print(i, x.shape)\n",
        "#     # break\n",
        "\n",
        "# swa32 batch64 dim128 3lyr 2^10 3.5,7.3 55.4s\n",
        "# swa32 2^11 4.3,14.5 110.5s        # swa32sig 2^11 5.9,14.7 110.5s\n",
        "# swa16 2^11 4.3,9.8 85.3s\n",
        "# swa8 2^11 4.4,7.2 77.4s\n",
        "# swa8 batch16 2^13 5.1,7.2 77.3s\n",
        "\n",
        "\n",
        "# mha 2^10 oom\n",
        "# 2^9 3.5,4.7\n",
        "\n",
        "\n",
        "# swa32sig\n",
        "# this is what They knevestigh with the thes groses lechnines many in such is\n",
        "# 11600 time: 6.5275774002075195 0.0005626737244241598\n",
        "# strain 1.845391869544983\n",
        "# this is what be of the picent strature by lorge, what do sest a pla know veh\n",
        "\n",
        "# swa32sig attnnores\n",
        "# this is what roub 201 to one dur. For the creates with wouldn’t the deeder\n",
        "# 44800 time: 6.389352083206177 0.0001426162933872756\n",
        "# strain 1.724169373512268\n",
        "# this is what did thouldled beaked U.S. hendlife, the women concess of grow,\n",
        "\n"
      ],
      "metadata": {
        "id": "T6XiuzQCYpE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aZ8q6DxC3P9B"
      },
      "outputs": [],
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).to(device) # [seq_len, dim // 2, 2]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        if seq_len > self.rot_emb.shape[0]: self.__init__(dim, seq_len)\n",
        "        rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "        x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "        rot_x = x * rot_emb\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "dim=16\n",
        "seq_len=512\n",
        "rope = RoPE(dim, seq_len, base=10000)\n",
        "x = torch.rand(4,64,dim, device=device)\n",
        "out = rope(x)\n",
        "\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2e8O_J2_rLt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Sliding Window Attention\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class Attention(nn.Module): # sliding window attention\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=8, drop=0, w=32):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=10000) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [b,t]\n",
        "        b,t = x.shape[:2]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        k = self.pos_enc(k).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, n_heads, T/num_tok, d_head]\n",
        "        v = v.unflatten(-1,(self.n_heads, self.d_head)).transpose(1,2) # [batch, n_heads, T/num_tok, d_head]\n",
        "        # k, v = F.pad(k,(0,0,self.w-1,0)), F.pad(v,(0,0,self.w-1,0)) # causal # [b, h, t+w-1, 2*d]\n",
        "        k, v = F.pad(k,(0,0,int(self.w/2),(self.w-1)//2)), F.pad(v,(0,0,int(self.w/2),(self.w-1)//2)) # bidirectional # [b, h, t+w-1, 2*d]\n",
        "        k, v = k.unfold(-2,self.w,1).transpose(-2,-1), v.unfold(-2,self.w,1).transpose(-2,-1) # [b,h,t,w,d]\n",
        "\n",
        "        attn = torch.einsum(\"bhtd,bhtwd->bhtw\", q, k) * self.scale\n",
        "        # print('attn fwd q mk attn', q.dtype, mk.dtype, attn.dtype)\n",
        "        if mask != None: attn = attn.masked_fill(~mask[:,None,:,None], -torch.finfo(attn.dtype).max) # [b,t]->[b,1,t,1] # causal is built into swa, so mask is only for [b,t]\n",
        "        attn = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        # attn = F.sigmoid(attn-math.log(attn.shape[-1])) # https://arxiv.org/pdf/2409.04431\n",
        "        # out = (self.drop(attn) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "        out = torch.einsum(\"bhtw,bhtwd->bhtd\", self.drop(attn), v)\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# typically: gen td*td=tt\n",
        "# up t1d*twd=tw, patch pd*tpd=p\n",
        "# down td*(w+1)d=t*(w+1)\n",
        "\n",
        "# t(2d+t)\n",
        "# t((w+1)d + w)\n",
        "# d+t >= wd+w\n",
        "# d~64-512; w~32-256; t~128*4^3=8192=2^13\n",
        "# 2^9; 2^8\n",
        "\n",
        "# b,t,d = 64,100,512\n",
        "\n",
        "# mask = torch.rand(b,t, device=device)>.5\n",
        "# model = Attention(d, w=3).to(device) # 257 ms\n",
        "# x = torch.rand(b,t,d, device=device)\n",
        "# out = model(x, mask=mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zU7NsUHTZ9dl"
      },
      "outputs": [],
      "source": [
        "# @title MultiHeadAttention\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=64, drop=0):\n",
        "        super().__init__()\n",
        "        # self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_model // n_heads\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=10000) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.q.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        # k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        k = self.pos_enc(k).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        v = v.unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "\n",
        "        # # (quadratic) attention # Softmax(q @ k.T) @ v\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask.unsqueeze(1) if mask != None else None, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # # out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # attn = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(~mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        # attn = torch.softmax(attn, dim=-1)\n",
        "        # # attn = F.sigmoid(attn-torch.log(attn.shape[-1])) # https://arxiv.org/pdf/2409.04431\n",
        "        # out = self.drop(attn) @ v # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1,2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygkv7B71JHP1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Attention\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "        # torch.nn.init.normal_(self.lin0.weight, std=.02)\n",
        "        torch.nn.init.normal_(self.lin0.weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "# 2048*2\n",
        "# 2048*7\n",
        "# ff: d_model*2 *ff_dim params\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, d_head, ff_dim=None, drop=0.):\n",
        "    def __init__(self, d_model, n_heads ,cond_dim=None, ff_dim=None, drop=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.cond_dim = cond_dim\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        if cond_dim!=None: self.norm2 = nn.RMSNorm(cond_dim)\n",
        "        # self.drop = nn.Dropout(drop)\n",
        "        # self.attn = MultiHeadAttention(d_model, cond_dim, n_heads=n_heads, d_head=d_model//n_heads, drop=drop)\n",
        "        # self.attn = Attention(d_model, cond_dim, n_heads=n_heads, d_head=d_model//n_heads, drop=drop, w=64)\n",
        "        self.attn = Attention(d_model, cond_dim, n_heads=n_heads, d_head=d_model//n_heads, drop=drop)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(drop), zero_module(nn.Linear(ff_dim, d_model))\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, zero_module(nn.Linear(ff_dim, d_model))\n",
        "        )\n",
        "        # torch.nn.init.normal_(self.ff[1].weight, std=.02)\n",
        "        torch.nn.init.normal_(self.ff[1].weight, std=1/(math.sqrt(d_model)+math.sqrt(ff_dim)))\n",
        "        # self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        # print('attblk fwd', x.shape, cond.shape if cond!=None else None, mask.shape if mask!=None else None)\n",
        "        if self.cond_dim==None: x = x + self.attn(self.norm1(x), mask=mask)\n",
        "        else: x = x + self.attn(self.norm1(x), self.norm2(cond), mask) # maybe no res for decoder\n",
        "        x = x + self.ff(x) # maybe no ff for decoder?\n",
        "        return x\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "b,t,d = 2,5,16\n",
        "# b,t,d = 2,5,256\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "mask = torch.rand(b,t,t, device=device)>0\n",
        "model = AttentionBlock(d_model=d, n_heads=4, ff_dim=16).to(device)\n",
        "# model = nn.Sequential(*[AttentionBlock(d_model=d, n_heads=4, ff_dim=16) for _ in range(2)])\n",
        "out =  model(x, mask)\n",
        "# out =  model(x)\n",
        "print(out.shape)\n",
        "\n",
        "\n",
        "# # model = MultiHeadAttention(d).to(device) # 257 ms\n",
        "# x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "# # out = model(x)\n",
        "# out = model(x, mask=mask)\n",
        "# print(out.shape)\n",
        "# print(time.time()-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J73LuO9XUcX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        # self.tok_emb = TTEmbedding([29, 1733], [64,1], rank=min(d_model,256))\n",
        "        # self.tok_emb = TTEmbedding([29, 1733], [128,2], rank=min(d_model,256))\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        # self.out = nn.Linear(d_model, out_dim)\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight.T  # weight tying\n",
        "        # self.w = 64\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        # x = self.pos_enc(self.tok_emb(x))\n",
        "        x = self.tok_emb(x)\n",
        "        b,t,d = x.shape\n",
        "\n",
        "        # causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "        # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "\n",
        "        # x = self.encoder(x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.encoder(x, x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "\n",
        "# attnnores improve fast but slows down, gets overtaken\n",
        "sigmoid ~< softmax\n",
        "\n",
        "\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "# model = GPT(vocab_size, d_model=128, out_dim=vocab_size, n_layers=3).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# x = torch.randint(0, vocab_size, (64, 128), device=device)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# strain 5.660660743713379\n",
        "# this is what we've done it. I'm able to be something strong. I think their information can be highly specific to a busy intertwined. But why \"Effects\"/ who shouldn't do,\" suppose it would improve them.Earlier open in both countries is sent by documentation, who were involved by custom ways to cutting the carbon gas\n",
        "# strain 5.939525604248047\n",
        "# this is what I’d like he’s very important to learn, it’s a bug my liquid had nothing once his streaming user in their own. It’s enough to run the survey, if there’s also no very big interested in a very $2. It’s sufficient's\n",
        "# strain 5.96616792678833\n",
        "# this is what manyatform are neither even documented and looked down their intentions in writers in so far savigTS-ve nuite was never known that some dioxide the sizes celebrate was fighting, as guilty or innovation.672 Expressia lives to Islamic Cheney of your existence takes a free background rest: The book she probably read it or find\n",
        "# strain 5.811507225036621\n",
        "# this is what it has not seen in intelligence and have believed in a romantic order and possible and rapidly now. Many growing economic ear is unclear whether they can do that the country's job. According to a proposals, the majority of domestic trade on which are the greatest mission with rich law by people, violated stockrol, which they would\n",
        "\n",
        "\n",
        "# b64,l128 8.0ram,10.0gpu l128*2:oom\n",
        "\n",
        "# b64,l128 w64 8.2ram,6.8gpu\n",
        "# b64,l128*2 w64 8.1ram,14.0gpu\n",
        "\n",
        "# gpt 1lyr mha ropein100 @attn # low loss but not learning\n",
        "# gpt 1lyr mha ropeout10000 F.attn # 19.3s\n",
        "# gpt 1lyr mha ropein10000 @attn # not learning\n",
        "# gpt 1lyr mha ropein10000 F.attn # this is what you are something else all in course, you're simply went back over their news, the reaction of the French source that is proven as we did not change any life at each power because they have the which shall noteded in order to change.\n",
        "\n",
        "# gpt 1lyr mha ropein10000 @attn fix~mask # 18.5s 7.3,5.9\n",
        "\n",
        "# swa64 128*2 7.4,12.0\n",
        "\n",
        "# d256 6.9,6.7\n",
        "\n",
        "# dim128 3lyr 3.0,.7\n",
        "# this is whats more of ietre'soni, they asari, your very\n",
        "# they vate. This iat\n",
        "# 9600 time: 5.7726781368255615 0.0006012580827081866\n",
        "# strain 1.481871247291565\n",
        "# this is what line wasn't good to both feel\n",
        "# like a delieve able or so foll,\n",
        "\n",
        "# mha\n",
        "# seq128*4+1 3.0,4.3\n",
        "# seq128*8+1 oom\n",
        "\n",
        "# swa\n",
        "# seq128*4+1 w64: 2.4,9.1 crash? w32:2.5,3.8\n",
        "# this is whatusanintan]\n",
        "#  dacrstan mowainind Butun\n",
        "#  an����������������������\n",
        "# 1500 time: 39.80479311943054 0.026518849830957827\n",
        "# strain 2.2812118530273438\n",
        "# this is whathe oue suses\n",
        "# catedeintilly,27), afevonees, Whe theesereceempone\n",
        "\n",
        "# seq128*8+1 w64:oom w32:2.4,7.8\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title gpt-bert\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=256, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim+1, d_model) # +1 for msk_idx\n",
        "        self.encoder = Seq(*[AttentionBlock(d_model, n_heads=n_heads, cond_dim=d_model) for _ in range(n_layers)])\n",
        "        # self.out = lambda x: x @ self.tok_emb.weight.T  # weight tying\n",
        "        self.out = lambda x: x @ self.tok_emb.weight[:-1].T  # weight tying\n",
        "        # print(self.tok_emb.weight.shape) # [257, 64]\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        # x = self.pos_enc(self.tok_emb(x))\n",
        "        x = self.tok_emb(x)\n",
        "        b,t,d = x.shape\n",
        "\n",
        "        # causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "        # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "\n",
        "        # x = self.encoder(x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.encoder(x, x, mask=mask) # [b,t,d], [nlyr,b,d]\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "    def bert_loss(self, x, mask_ratio=.25, m=32): # [b,t]\n",
        "        b,t = x.shape\n",
        "        mask = (torch.rand(m,t) < mask_ratio).repeat(b,1) # [b*m,t]\n",
        "        x = x.repeat_interleave(m, dim=0) # [b*m,t]\n",
        "        y = x[mask]\n",
        "        x[mask] = msk_idx\n",
        "        y_ = model(torch.cat([torch.full((b*m,1), msk_idx, device=device), x], dim=-1))[:,:-1] #output = [batch size, trg len - 1, output dim]\n",
        "        loss = F.cross_entropy(y_[mask], y.to(int)) # [b*t,d], [b*t]\n",
        "        return loss\n",
        "\n",
        "\n",
        "# gpt 2\n",
        "# Parameters Layers dmodel\n",
        "# 117M 12 768 gpt1\n",
        "# 345M 24 1024\n",
        "# 762M 36 1280\n",
        "# 1542M 48 1600\n",
        "\n",
        "msk_idx = 256\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "# model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "# model = GPT(vocab_size, d_model=128, out_dim=vocab_size, n_layers=3).to(device)\n",
        "model = GPT(vocab_size, d_model=64, out_dim=vocab_size, n_layers=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# x = torch.randint(0, vocab_size, (64, 128), device=device)\n",
        "# # out = model(x)\n",
        "# out = model.bert_loss(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "out_dim=vocab_size+1\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_dim, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(in_dim, num_classes)\n",
        "    def forward(self, x): return self.classifier(x)\n",
        "classifier = Classifier(vocab_size, out_dim).to(device)\n",
        "coptim = torch.optim.SGD(classifier.parameters(), lr=1e-3) # 1e-3\n",
        "\n"
      ],
      "metadata": {
        "id": "ChkMXlzar_R_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNGU1V3XPFUk"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nd-sGe6Ku4S",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"bert\", config={\"model\": \"res18\",}) #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title bert train test\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# # https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "# def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "#     log_probs = F.log_softmax(logits, dim=-1)\n",
        "#     nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "#     perplexity = nll.mean().exp()\n",
        "#     return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optim): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    start = begin = time.time()\n",
        "    for i, (x,_) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            b, t = x.shape\n",
        "            # print(x.shape)\n",
        "            loss = model.bert_loss(x)\n",
        "            # # loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "        optim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(\"strain\",loss.item())\n",
        "            # print(generate(model, \"this is what\"))\n",
        "            # model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            start = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(x)})\n",
        "        except NameError: pass\n",
        "        if i>=500: break\n",
        "\n",
        "def ctrain(model, classifier, dataloader, coptim, scheduler=None): # train function with automatic mixed precision\n",
        "    model.eval()\n",
        "    classifier.train()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # bfloat16 float16\n",
        "            with torch.no_grad():\n",
        "                # sx = model(x).detach()\n",
        "                sx = model(torch.cat([torch.full((x.shape[0],1), msk_idx, device=device), x], dim=-1))[:,:-1].mean(dim=-2).detach() # [b,t,d]->[b,d]\n",
        "            y_ = classifier(sx) # [b,vocab]\n",
        "            loss = F.cross_entropy(y_, y.flatten().to(int)) # [b,vocab], [b]\n",
        "        coptim.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5) # .5\n",
        "        scaler.step(coptim)\n",
        "        scaler.update()\n",
        "        if i % 100 == 0:\n",
        "            print(\"classify\",loss.item()/len(y))\n",
        "        try: wandb.log({\"closs\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "\n",
        "def test(model, classifier, dataloader):\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device) # [batch, ]\n",
        "        with torch.no_grad():\n",
        "            # sx = model(x)\n",
        "            sx = model(torch.cat([torch.full((x.shape[0],1), msk_idx, device=device), x], dim=-1))[:,:-1].mean(dim=-2).detach() # [b,t,d]->[b,d]\n",
        "            y_ = classifier(sx)\n",
        "\n",
        "        loss = F.cross_entropy(y_, y.flatten().to(int)) # [b,vocab], [b]\n",
        "        acc = (y==y_.argmax(dim=1)).mean().item()\n",
        "        # if i>=100: break\n",
        "        if i % 100 == 0:\n",
        "            # print(\"test\",loss.item()/len(y))\n",
        "            print('acc', round(acc, 3), 'test_loss', round(loss.item()/len(y), 3))\n",
        "        try: wandb.log({\"correct\": acc})\n",
        "        # try: wandb.log({\"test loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "        if i>=100: break\n",
        "\n",
        "# scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=400, num_training_steps=2000) # https://docs.pytorch.org/torchtune/0.2/generated/torchtune.modules.get_cosine_schedule_with_warmup.html\n",
        "for i in range(10): #\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, pin_memory=True, num_workers=0)\n",
        "\n",
        "    strain(model, train_loader, optim)\n",
        "    ctrain(model, classifier, train_loader, coptim)\n",
        "    test(model, classifier, test_loader)\n"
      ],
      "metadata": {
        "id": "xowu3wA9ms-q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCN055Zr7Dq4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title train generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "\n",
        "def strain(model, dataloader, optimizer, scheduler=None): # train function with automatic mixed precision\n",
        "    start = begin = time.time()\n",
        "    model.train()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x, y = x[:,:-1], x[:,1:]\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            # causal_mask = torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1) # for F.scaled_dot_product_attention\n",
        "            # # causal_mask = ~torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1)\n",
        "            # logits = model(x, mask=causal_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            logits = model(x) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = F.cross_entropy(logits.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "            # loss = F.cross_entropy(logits.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        # scaler.unscale_(optim)\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            print(\"strain\",loss.item())\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            # perplexity = Perplexity(logits.detach(), y).item()\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            start = begin = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            output = model(x) # gpt\n",
        "        output = output[:,-1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    # completion = decode(x)\n",
        "    return completion\n",
        "\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33Grqz8SiKPN"
      },
      "outputs": [],
      "source": [
        "print(generate(model, \"this is what\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knt9jqUmgl97"
      },
      "source": [
        "## drawer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5CjHMaQE8MM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title GRU pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.emb = nn.Embedding(in_dim, d_model)\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "    def forward(self, x, h0=None): # rnn/gru\n",
        "        x = self.emb(x)\n",
        "        if h0==None: h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        # print(x.shape, h0.shape)\n",
        "        x, h0 = self.rnn(x, h0)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        x = self.fc(x) # out: (n, 10)\n",
        "        return x, h0\n",
        "\n",
        "hidden_size = 128 #64 128\n",
        "num_layers = 3#2\n",
        "try: vocab_size=train_loader.dataset.vocab_size#50\n",
        "except NameError: vocab_size=50\n",
        "\n",
        "model = RNN(vocab_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "# print(model)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "# 128,2\n",
        "# Test Loss: 6.360389362062727\n",
        "# this is what ween new york is well it a sign more directly into simply shares\n",
        "# 0 time: 5.910429954528809 5.910431623458862\n",
        "# 64,2\n",
        "# Test Loss: 6.167358561924526\n",
        "# this is what bull morp on its aftant opereals of a b. the hamally plans beces\n",
        "# 0 time: 5.655158996582031 5.655160903930664\n",
        "# 64,1\n",
        "# Test Loss: 5.823708357129778\n",
        "# this is what achan agrive\n",
        "#  the guinesst on of promjects cl funds that jound\n",
        "# 0 time: 4.788918495178223\n",
        "\n",
        "# Test Loss: 8.247572830745153\n",
        "# this is what that has months with unfinsings lide to N by well\n",
        "#  next offited\n",
        "# 29 time: 3.902247905731201 4.377542002995809\n",
        "\n",
        "# dim128 3lyr bptt25 2.8,.2\n",
        "# this is whatever from the every coddane\n",
        "# teets, in.]\n",
        "# [Footnote 21: Any ot\n",
        "# 9700 time: 4.124014616012573 0.00042511236912171237\n",
        "# strain loss, ppl 1.3780442476272583 3.953125\n",
        "# this is what seeming upon course in your drew\n",
        "# plant now, as a less ever aff\n",
        "\n",
        "b,t=2,5\n",
        "x = torch.randint(0, vocab_size, (b,t), device=device)\n",
        "h = torch.rand(num_layers, b, hidden_size, device=device)\n",
        "out, h = model(x, h)\n",
        "print(out.shape, h.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SadMd02M_FK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title bptt train, gen\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    # nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    nll = -log_probs.gather(dim=-1, index=target.to(torch.int64).unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "import time\n",
        "def strain(model, dataloader, optim, scheduler=None, bptt=25): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    h0 = None\n",
        "    start = begin = time.time()\n",
        "    for i, x in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        xs, ys = torch.split(x[:,:-1], bptt, dim=1), torch.split(x[:,1:], bptt, dim=1)\n",
        "        for (x, y) in zip(xs, ys): # https://discuss.pytorch.org/t/how-to-train-a-many-to-many-lstm-with-bptt/13005/10\n",
        "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "                pred, h0 = model(x, h0)\n",
        "                loss = F.cross_entropy(pred.flatten(0,1), y.flatten().to(int)) # [b*t,d], [b*t]\n",
        "                # loss = F.cross_entropy(pred.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "            optim.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            # scaler.unscale_(optim)\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            scaler.step(optim)\n",
        "            scaler.update()\n",
        "            h0 = h0.detach()\n",
        "\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % 100 == 0:\n",
        "            perplexity = Perplexity(pred.detach(), y).item()\n",
        "            print(\"strain loss, ppl\",loss.item(), perplexity)\n",
        "            print(generate(model, \"this is what\"))\n",
        "            model.train()\n",
        "            print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "            start = begin = time.time()\n",
        "        try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        except NameError: pass\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    hidden=None # rnn 1/3\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            # output = model(x) # gpt\n",
        "            output, hidden = model(x, hidden) # rnn 2/3\n",
        "        hidden = hidden[:,-1].unsqueeze(1) # rnn 3/3\n",
        "        output = output[:,-1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    # completion = decode(x)\n",
        "    return completion\n",
        "\n",
        "# import time\n",
        "# start = begin = time.time()\n",
        "for i in range(1):\n",
        "    # train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    strain(model, train_loader, optim, scheduler=None)\n",
        "    print(generate(model, \"this is what\"))\n",
        "    # print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    # start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U_UuuGu35g49"
      },
      "outputs": [],
      "source": [
        "# @title pg19\n",
        "# https://console.cloud.google.com/storage/browser/deepmind-gutenberg/test\n",
        "# https://storage.cloud.google.com/deepmind-gutenberg/test/10146.txt\n",
        "\n",
        "# # !wget https://storage.googleapis.com/deepmind-gutenberg/train/10006.txt\n",
        "# !mkdir ./validation/\n",
        "# !wget https://storage.googleapis.com/deepmind-gutenberg/validation/42306.txt -O ./validation/42306.txt\n",
        "# # with open('/content/10006.txt', 'r') as f: text = f.read()\n",
        "\n",
        "# !gsutil -m cp -r gs://deepmind-gutenberg/train ./\n",
        "!gsutil -m cp -r gs://deepmind-gutenberg/test /content\n",
        "# !gsutil -m cp -r gs://deepmind-gutenberg/validation ./\n",
        "\n",
        "# # !mv -v ~/validation/.[!.]* ~/test/\n",
        "# !mv ~./validation/* ~./test\n",
        "!mv ~/content/validation/* ~/content/test\n",
        "\n",
        "# path='validation/' # 17,733,002 # 4200\n",
        "# path='test/' # 41,289,101\n",
        "# path='train/' # 749,219,628++\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UFtNtKAF5rjv"
      },
      "outputs": [],
      "source": [
        "# @title bytedataset\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "\n",
        "def txt_iter(filepath, chunk_size=8192):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        while True:\n",
        "            chunk = f.read(chunk_size)\n",
        "            if not chunk: break\n",
        "            yield chunk\n",
        "\n",
        "\n",
        "    # class InfiniteDataset(IterableDataset):\n",
        "    #     def __init__(self, start_value=0):\n",
        "    #         self.current_value = start_value\n",
        "\n",
        "    #     def __iter__(self):\n",
        "    #         while True:\n",
        "    #             yield self.current_value\n",
        "    #             self.current_value += 1\n",
        "\n",
        "# class ByteDataset(Dataset):\n",
        "class ByteDataset(IterableDataset):\n",
        "    def __init__(self, path='train/', seq_len=129, buffer_size=1024):\n",
        "        self.vocab_size = 256 # utf-8 # self.enc.n_vocab # gpt2:50257\n",
        "        self.seq_len, self.buffer_size = seq_len, buffer_size  # must be ≥ seq_len\n",
        "        file_list = [path+f for f in os.listdir(path)]\n",
        "        random.shuffle(file_list)\n",
        "        self.fileiter = iter(file_list)\n",
        "        # self.process()\n",
        "        self.textiter = txt_iter(next(self.fileiter))\n",
        "        self.buffer = []  # token buffer\n",
        "        self.fill_buffer()\n",
        "\n",
        "    def fill_buffer(self):\n",
        "        while len(self.buffer) < self.buffer_size:\n",
        "            try: x = next(self.textiter)\n",
        "            except StopIteration:\n",
        "                self.textiter = txt_iter(next(self.fileiter))\n",
        "                x = next(self.textiter)\n",
        "            self.buffer.extend(x)\n",
        "\n",
        "    # def __len__(self): return 128000 # too large will cause long load first batch\n",
        "    # # fs = sum([os.path.getsize(path+file) for file in os.listdir(path)])\n",
        "\n",
        "# import re\n",
        "# def strip_gutenberg_boilerplate(text):\n",
        "#     start_re = re.compile(r\"\\*\\*\\* START OF (.*?) \\*\\*\\*\", re.IGNORECASE)\n",
        "#     end_re = re.compile(r\"\\*\\*\\* END OF (.*?) \\*\\*\\*\", re.IGNORECASE)\n",
        "#     start_match = start_re.search(text)\n",
        "#     end_match = end_re.search(text)\n",
        "#     start = start_match.end() if start_match else 0\n",
        "#     end = end_match.start() if end_match else len(text)\n",
        "#     return text[start:end].strip()\n",
        "\n",
        "# def normalise_whitespace(text):\n",
        "#     return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # def __getitem__(self, idx):\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            if len(self.buffer) < self.seq_len: self.fill_buffer()\n",
        "            # if len(self.buffer) < self.seq_len: raise StopIteration\n",
        "            if len(self.buffer) < self.seq_len: return\n",
        "            x, self.buffer = self.buffer[:self.seq_len], self.buffer[self.seq_len:]\n",
        "            # # return torch.tensor(x, dtype=torch.uint8)\n",
        "            # return torch.tensor(x, dtype=torch.int32)\n",
        "            yield torch.tensor(x, dtype=torch.int32)\n",
        "\n",
        "seq_len = 129 # 128\n",
        "train_data = ByteDataset('validation/', seq_len) # one line of poem is roughly 50 characters\n",
        "# train_data = ByteDataset('test/', seq_len) # one line of poem is roughly 50 characters\n",
        "# test_data = ByteDataset('test/', seq_len) # one line of poem is roughly 50 characters\n",
        "batch_size = 64 #512\n",
        "# train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=2)\n",
        "# test_loader = DataLoader(test_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# https://github.com/facebookresearch/blt/blob/main/bytelatent/tokenizers/blt_tokenizer.py#L137\n",
        "# def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.uint8)#, device=device)#.unsqueeze(0)\n",
        "def encode(c): return torch.tensor(list(c.encode(\"utf-8\")), dtype=torch.int32, device=device).unsqueeze(0)\n",
        "def decode(x): return bytes(x.tolist()).decode(\"utf-8\", errors='replace') # replace ignore\n",
        "# for x in train_loader:\n",
        "#     break\n",
        "# print(x)\n",
        "# s='恋 和 したい の わ'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khJg7EKtaFrz"
      },
      "outputs": [],
      "source": [
        "seq_len = 2**11+1 # 128\n",
        "print(seq_len)\n",
        "# train_data = ByteDataset('validation/', seq_len) # one line of poem is roughly 50 characters\n",
        "train_data = ByteDataset('test/', seq_len) # one line of poem is roughly 50 characters\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, pin_memory=True, num_workers=2)\n",
        "\n",
        "for i, x in enumerate(train_loader):\n",
        "    print(i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q5bGqKAJzN7I"
      },
      "outputs": [],
      "source": [
        "# @title TTLinear\n",
        "# Tensor Train embedding https://arxiv.org/pdf/1901.10787\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def make_einsum(num_tensors):\n",
        "    a = 97\n",
        "    R = chr(a+25) # 'z'\n",
        "    lhs = [chr(a)+R]\n",
        "    for i in range(1, num_tensors-1):lhs.append(R+chr(a+i)+R)\n",
        "    lhs.append(R+chr(a+num_tensors-1))\n",
        "    return ','.join(lhs) + '->' + ''.join([chr(a+i) for i in range(num_tensors)]) # az,zbz,zcz,zd->abcd\n",
        "\n",
        "class TTLinear(nn.Module):\n",
        "    def __init__(self, in_features=None, out_features=None, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.lfeat = len(in_features)\n",
        "        if self.lfeat==1: lst = in_features + out_features\n",
        "        elif self.lfeat>=2: lst = [i*j for i, j in zip(in_features, out_features)]\n",
        "        last = len(lst)\n",
        "        var = last/rank**(1/(2*(std**.5)*last))\n",
        "        c=1/last\n",
        "        self.params = nn.ParameterList([nn.Parameter(torch.randn(lst[0], rank).clamp(-c,c)*var),\n",
        "            *[nn.Parameter(torch.randn(rank, ij, rank).clamp(-c,c)*var) for ij in lst[1:-1]],\n",
        "            nn.Parameter(torch.randn(rank, lst[-1]).clamp(-c,c)*var)])\n",
        "        self.einsum_str = make_einsum(last)\n",
        "        self.shape = [p for ij in zip(in_features, out_features) for p in ij]\n",
        "        self.permute = list(range(0, 2*self.lfeat - 1, 2)) + list(range(1, 2*self.lfeat, 2))\n",
        "    def weight(self): return torch.einsum(self.einsum_str, *self.params).reshape(self.shape).permute(self.permute).flatten(0,self.lfeat-1).flatten(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight = self.weight()\n",
        "        return x.to(weight.dtype) @ weight\n",
        "\n",
        "def one_hot(x, in_dim):\n",
        "    return torch.zeros((*x.shape,in_dim), dtype=torch.int8, device=x.device).scatter_(-1, x.unsqueeze(-1).to(int), 1)\n",
        "\n",
        "def one_hot(x, in_dim):\n",
        "    b,t = x.shape\n",
        "    o = torch.zeros((b,t,in_dim), dtype=bool, device=x.device)\n",
        "    o[torch.arange(b).unsqueeze(-1),torch.arange(t).unsqueeze(0),x] = True\n",
        "    return o\n",
        "\n",
        "import math\n",
        "class TTEmbedding(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, rank=256, std=1):\n",
        "        super().__init__()\n",
        "        self.ttlin = TTLinear(in_dim, d_model, rank, std) # https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "        self.weight = self.ttlin.weight\n",
        "        self.num_classes = math.prod(in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return self.ttlin(F.one_hot(x, self.num_classes))\n",
        "        return self.ttlin(one_hot(x, self.num_classes))\n",
        "# self.out = lambda x: x @ self.tok_emb.weight().T  # weight tying\n",
        "\n",
        "# # in_features=(3,4,5,6); out_features=(2,3,4,5)\n",
        "# in_features=[120]; out_features=[300]\n",
        "# in_features=[12]; out_features=[30]\n",
        "# rank=16\n",
        "# # std=.5\n",
        "# lin = TTLinear(in_features, out_features, rank, std).to(device)\n",
        "# # x = torch.rand(4,math.prod((3,4,5,6)))\n",
        "# x = torch.rand(4,7,math.prod(in_features), device=device)\n",
        "# print(lin.params[0].device)\n",
        "# out = lin(x)\n",
        "# print(out.shape)\n",
        "# print(lin.ttlin.params[0].device)\n",
        "\n",
        "# emb = TTEmbedding(in_features, out_features, rank).to(device)\n",
        "# x = torch.randint(0, math.prod(in_features), (2, 5), device=device)\n",
        "# out = emb(x)\n",
        "# print(out.shape)\n",
        "# print(out)\n",
        "\n",
        "# o=lin.weight\n",
        "# print(o.mean().item(), o.std().item(), o.min().item(), o.max().item())\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.rcParams[\"figure.figsize\"] = (4,4)\n",
        "# # plt.hist(o.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# # plt.hist(o[:100,:100].flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# x = torch.randn(100,100)*std\n",
        "# # plt.hist(x.flatten().tolist(), bins=20, alpha=.5, label='context mask')\n",
        "# plt.hist([o[:100,:100].flatten().tolist(), x.flatten().tolist()], bins=20, alpha=.5, label='context mask')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UCgV-efIfzne"
      },
      "outputs": [],
      "source": [
        "# @title mask translate\n",
        "PAD_IDX=0\n",
        "def make_src_mask(src):\n",
        "    # return (src != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device) # [batch_size, 1, 1, src_len]?\n",
        "    return (src != PAD_IDX)[:,None,None,:].to(device) # [batch_size, 1, 1, src_len]?\n",
        "\n",
        "# attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "def make_trg_mask(trg):\n",
        "    # trg_pad_mask = (trg != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device)\n",
        "    trg_pad_mask = (trg != PAD_IDX)[:,None,None,:].to(device) # [batch, 1, 1, trg_len]\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
        "    # print('make_trg_mask', trg_pad_mask.shape, trg_sub_mask.shape) # [64, 1, 1, 10], [10, 10]\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask # [batch, 1, trg_len, trg_len]?\n",
        "    return trg_mask\n",
        "\n",
        "def translate(model, src_sentence):\n",
        "    model.eval()\n",
        "    src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "    num_tokens = src.shape[1]\n",
        "    trg_indexes = [BOS_IDX]\n",
        "    max_len = src.shape[1]+5\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output = model(src, trg_tensor, src_mask, trg_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == EOS_IDX: break\n",
        "    trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "    return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "def translate_fast(model, src_sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "        num_tokens = src.shape[1]\n",
        "        trg_indexes = [BOS_IDX]\n",
        "        max_len = src.shape[1]+5\n",
        "        src_mask = make_src_mask(src)\n",
        "        output = model.encode(src, src_mask)\n",
        "        for i in range(max_len):\n",
        "            trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "            trg_mask = make_trg_mask(trg_tensor)\n",
        "            output = model.decode(src, trg_tensor, src_mask, trg_mask)\n",
        "            pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "            trg_indexes.append(pred_token)\n",
        "            if pred_token == EOS_IDX: break\n",
        "        trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "        return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "# UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "src, trg = torch.randint(0, 100, (64, 10)), torch.randint(0, 100, (64, 10))\n",
        "sm = make_src_mask(src)\n",
        "tm = make_trg_mask(trg)\n",
        "# print(sm.shape, tm.shape) # [64, 1, 1, 10], [64, 1, 10, 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N9R4hVUCxGiu"
      },
      "outputs": [],
      "source": [
        "# @title translate train test\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "        trg_input = trg[:,:-1]\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "        print('train', src.shape, trg.shape, src_mask.shape, trg_mask.shape)\n",
        "        output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(list(dataloader))\n",
        "\n",
        "def test(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "            trg_input = trg[:,:-1]\n",
        "            src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "            output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(list(dataloader))\n",
        "\n",
        "# @title run\n",
        "import time\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) # lr=0.0001\n",
        "\n",
        "# for epoch in range(20):\n",
        "for epoch in range(1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = test(model, val_loader, loss_fn)\n",
        "    end_time = time.time()\n",
        "    print((f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    # print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people standing in front of an igloo .\n",
        "    # @title inference\n",
        "    print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people stand in front of an igloo .\n",
        "    print(translate(model, \"Ein Koch in weißer Uniform bereitet Essen in einer Restaurantküche zu .\")) # A chef in a white uniform prepares food in a restaurant kitchen .\n",
        "    print(translate(model, \"Zwei junge Mädchen spielen Fußball auf einem Feld. .\")) # Two young girls play soccer on a field. .\n",
        "    print(translate(model, \"Eine Frau mit Hut und Sonnenbrille steht am Strand .\")) # A woman wearing a hat and sunglasses stands on the beach .\n",
        "    print(translate(model, \"Zwei Freunde lachen und genießen ein Eis auf einer wunderschönen Wiese .\")) # Two friends laugh and enjoy ice cream on a beautiful meadow .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-bHqmV4ReUv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Epoch: 1, Train loss: 5.402, Val loss: 4.186, Epoch time = 41.608s\n",
        "A group of people are are are are are are in a .\n",
        "Epoch: 2, Train loss: 3.898, Val loss: 3.545, Epoch time = 41.068s\n",
        "A group of people are standing in a crowd of people .\n",
        "Epoch: 3, Train loss: 3.353, Val loss: 3.125, Epoch time = 41.566s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 4, Train loss: 2.944, Val loss: 2.830, Epoch time = 40.756s\n",
        "A group of people standing in front of a building .\n",
        "Epoch: 5, Train loss: 2.630, Val loss: 2.596, Epoch time = 41.468s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 6, Train loss: 2.375, Val loss: 2.429, Epoch time = 41.023s\n",
        "A group of people standing in front of a house .\n",
        "Epoch: 7, Train loss: 2.166, Val loss: 2.307, Epoch time = 41.604s\n",
        "A group of people stand in front of a house .\n",
        "Epoch: 8, Train loss: 1.984, Val loss: 2.210, Epoch time = 40.876s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 9, Train loss: 1.834, Val loss: 2.131, Epoch time = 41.496s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 10, Train loss: 1.698, Val loss: 2.079, Epoch time = 41.052s\n",
        "A group of people are standing in front of an empty house .\n",
        "Epoch: 11, Train loss: 1.576, Val loss: 2.038, Epoch time = 41.570s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 12, Train loss: 1.475, Val loss: 2.033, Epoch time = 41.067s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 13, Train loss: 1.381, Val loss: 2.017, Epoch time = 41.576s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 14, Train loss: 1.292, Val loss: 1.977, Epoch time = 40.779s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 15, Train loss: 1.213, Val loss: 1.948, Epoch time = 41.461s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 16, Train loss: 1.139, Val loss: 1.940, Epoch time = 40.800s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 17, Train loss: 1.073, Val loss: 1.940, Epoch time = 41.533s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 18, Train loss: 1.006, Val loss: 1.939, Epoch time = 40.856s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 19, Train loss: 0.944, Val loss: 1.947, Epoch time = 41.345s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 20, Train loss: 0.893, Val loss: 1.956, Epoch time = 40.935s\n",
        "A group of people stand in front of an igloo\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3vBEfUjzuobW"
      },
      "outputs": [],
      "source": [
        "# @title bleu\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    for datum in data:\n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "    return bleu_score(pred_trgs, trgs)\n",
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')\n",
        "# 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model.\n",
        "\n",
        "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "\n",
        "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    translations_done = [0] * len(src_tensor)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_tokens = output.argmax(2)[:,-1]\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\n",
        "            trg_indexes[i].append(pred_token_i)\n",
        "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                translations_done[i] = 1\n",
        "        if all(translations_done):\n",
        "            break\n",
        "\n",
        "    # Iterate through each predicted example one by one;\n",
        "    # Cut-off the portion including the after the <eos> token\n",
        "    pred_sentences = []\n",
        "    for trg_sentence in trg_indexes:\n",
        "        pred_sentence = []\n",
        "        for i in range(1, len(trg_sentence)):\n",
        "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                break\n",
        "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
        "        pred_sentences.append(pred_sentence)\n",
        "    return pred_sentences, attention\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            _trgs = []\n",
        "            for sentence in trg:\n",
        "                tmp = []\n",
        "                # Start from the first token which skips the <start> token\n",
        "                for i in sentence[1:]:\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
        "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
        "                        break\n",
        "                    tmp.append(trg_field.vocab.itos[i])\n",
        "                _trgs.append([tmp])\n",
        "            trgs += _trgs\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
        "            pred_trgs += pred_trg\n",
        "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF9atY_nDOoV"
      },
      "source": [
        "## old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAyqkMjq6T2C"
      },
      "outputs": [],
      "source": [
        "# Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://www.mihaileric.com/posts/transformers-attention-in-disguise/\n",
        "# https://jalammar.github.io/illustrated-transformer/\n",
        "# http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "# position embedding <-> \"vocabulary\" size 100 <-> model can accept sentences up to 100 tokens long\n",
        "# learned positional encoding, warm-up and cool-down steps, label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V1teyZuwff9_"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64c91cf87c13c0e83586b8e66e4d74e/translation_transformer.ipynb\n",
        "\n",
        "# https://github.com/pytorch/data\n",
        "# %pip install portalocker\n",
        "# %pip install torchdata\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -qU torchdata torchtext\n",
        "!pip install -qU spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "!git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset\n",
        "\n",
        "# !pip list | grep torch\n",
        "!pip list | grep python\n",
        "\n",
        "import torchtext.datasets as datasets\n",
        "\n",
        "# Load the Multi30k dataset\n",
        "train_iter, valid_iter, test_iter = datasets.Multi30k(split=('train', 'valid', 'test'))\n",
        "\n",
        "# Iterate through the dataset\n",
        "for src, tgt in train_iter:\n",
        "    print(f\"Source: {src}\")\n",
        "    print(f\"Target: {tgt}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5BFat7RgKSwR"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "# modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "de_vocab.set_default_index(UNK_IDX)\n",
        "en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# vocab_transform = {SRC_LANGUAGE:de_vocab, TRG_LANGUAGE:en_vocab}\n",
        "# text_transform = {SRC_LANGUAGE:de_transform, TRG_LANGUAGE:en_transform}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gURN1FYyM-A2"
      },
      "outputs": [],
      "source": [
        "# !pip install -q datasets\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "!python -m spacy download de_core_news_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J-zgIl9FMgPV"
      },
      "outputs": [],
      "source": [
        "# @title hf data\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a translation dataset (e.g., WMT English-German)\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "\n",
        "# Access train and validation splits\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "\n",
        "# # Example of accessing data\n",
        "# # for example in train_data:\n",
        "# #     print(example['translation'])\n",
        "# batch_size = 128 # 128\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "# from torchtext.datasets import multi30k, Multi30k\n",
        "# # modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "# de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "# en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# # Create a blank Tokenizer with just the English vocab\n",
        "# tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# # Construction 2\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# tokenizer = nlp.tokenizer\n",
        "\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "import spacy\n",
        "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "de_tokenizer = spacy.load(\"de_core_news_sm\") # https://spacy.io/models/de\n",
        "\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# from torchtext.vocab import build_vocab_from_iterator\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "# de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# de_vocab.set_default_index(UNK_IDX)\n",
        "# en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "\n",
        "print(\"en_tokens\", en_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "# val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KeuV4EZyqXj5"
      },
      "outputs": [],
      "source": [
        "# @title model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_seq_length=512):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "class LearntPosEnc(nn.Module): # learnt positional embeddings\n",
        "    def __init__(self, d_model, dropout=0.1, max_length=512):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, src_len = x.shape\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # [batch size, src len]\n",
        "        return self.drop(x + self.pos_embedding(pos))\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        # theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "        theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
        "        # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = pos * theta # [seq_len, 1] * [dim // 2] = [seq_len, dim // 2]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: print(\"rot_emb.shape[0] < seq_len\")\n",
        "        rot_emb = self.rot_emb[:seq_len, :].unsqueeze(0).expand(batch, -1, -1)\n",
        "        x = x.view(batch, seq_len, dim // 2, 2)\n",
        "        rot_emb = rot_emb.view(batch, seq_len, dim // 2, 2)\n",
        "        # rot_x = torch.einsum('...ij,...ij->...ij', x, rot_emb)\n",
        "        rot_x = x * rot_emb\n",
        "        # return rot_x.view(*rot_x.shape[:-2], dim)\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        x = self.drop(attention) @ V # x = torch.matmul(self.drop(attention), V)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.norm1(src + self.drop(self.self_attn(src, src, src, src_mask)[0]))\n",
        "        src = self.norm2(src + self.drop(self.ff(src)))\n",
        "        return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.norm3 = nn.RMSNorm(d_model)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.enc_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.norm1(trg + self.drop(self.self_attn(trg, trg, trg, trg_mask)[0]))\n",
        "        trg = self.norm2(trg + self.drop(self.enc_attn(trg, enc_src, enc_src, src_mask)[0]))\n",
        "        trg = self.norm3(trg + self.drop(self.ff(trg)))\n",
        "        return trg\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        return trg\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, enc_layers, nhead, ff_dim, dropout)\n",
        "        self.decoder = Decoder(d_model, dec_layers, nhead, ff_dim, dropout)\n",
        "        # self.pos_enc = PositionalEncoder(d_model, dropout=dropout)\n",
        "        # self.pos_enc = LearntPosEnc(d_model, dropout=dropout)\n",
        "        self.pos_enc = RoPE(d_model)\n",
        "        self.src_tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.trg_tok_emb = nn.Embedding(out_dim, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src = self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model))\n",
        "        trg = self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model))\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.lin(output)\n",
        "        return output\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        return self.encoder(self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model)), src_mask)\n",
        "\n",
        "    def decode(self, trg, memory, trg_mask=None, src_mask=None):\n",
        "        trg = self.decoder(self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model)), memory, trg_mask, src_mask)\n",
        "        return self.lin(trg)\n",
        "\n",
        "\n",
        "in_dim = 50#\n",
        "out_dim = 50\n",
        "model = Seq2Seq(in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zqXMOvhvV1CQ"
      },
      "outputs": [],
      "source": [
        "# @title Attention with kvcache window\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "# mask = torch.rand(b,t,t)<.2\n",
        "def to_midx(mask): # [b,t,t+p] bool mask\n",
        "    b,t = mask.shape[:2]\n",
        "    idx = mask.nonzero(as_tuple=True)\n",
        "    tk = torch.stack(idx[:-1], dim=-1).to(device)\n",
        "    tk = torch.cat([torch.tensor([True], device=device), (tk[:-1]!=tk[1:]).any(-1)], dim=0)\n",
        "    cc = len(idx[0])\n",
        "    positions = torch.arange(cc, device=device)\n",
        "    last_reset_pos = torch.zeros(cc, dtype=int, device=device)\n",
        "    last_reset_pos[tk] = positions[tk]\n",
        "    out = positions - last_reset_pos.cummax(0).values\n",
        "    c=out.max()+1\n",
        "    y = torch.full((b,t,c), -1, device=device)\n",
        "    y[*idx[:-1],out] = idx[-1]\n",
        "    return y # [b,t,c]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # def __init__(self, d_model, cond_dim=None, n_heads=None, d_head=8, drop=0.): # .1\n",
        "    def __init__(self, query_dim, cond_dim=None, n_heads=8, d_head=8, drop=0, w=64):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model = d_head * n_heads\n",
        "        self.d_head, self.n_heads = d_head, n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.pos_enc = RoPE(d_model, base=100) # 10000\n",
        "        self.q = nn.Linear(query_dim, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or query_dim, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(drop) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "        # torch.nn.init.normal_(self.qkv.weight, std=.02)\n",
        "        # torch.nn.init.normal_(self.q.weight, std=1/(math.sqrt(query_dim)+math.sqrt(d_model)))\n",
        "        # torch.nn.init.normal_(self.kv.weight, std=1/(math.sqrt(cond_dim or query_dim)+math.sqrt(d_model)))\n",
        "        t=128\n",
        "        self.midx = (torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)).to(device) # [t,w,1]\n",
        "        self.w = w\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,t,d], [batch, num_tok, cond_dim], [b,t,t(+p)]\n",
        "        b,t = x.shape[:2]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        # q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # kv = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2)#.chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        q = self.pos_enc(self.q(x)).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        k, v = self.kv(cond).chunk(2, dim=-1)\n",
        "        kv = torch.cat([self.pos_enc(k),v], dim=-1).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        import time\n",
        "        start = time.time()\n",
        "\n",
        "        # if mask != None:\n",
        "            # midx = to_midx(mask) # [b,t,w]\n",
        "            # # print('mask', q.shape, mask.shape, midx.shape)\n",
        "            # kv = kv[torch.arange(b)[:,None,None,None], torch.arange(self.n_heads)[None,:,None,None], midx.unsqueeze(1)] # [b,h,t,w,d]\n",
        "\n",
        "            # b,t,d = x.shape\n",
        "            # if t>len(self.midx): self.midx = (torch.arange(w, dtype=torch.int32).repeat(t,1) + torch.arange(1-w, t-w+1, dtype=torch.int32).unsqueeze(-1)).to(device) # [t,w,1]\n",
        "            # midx = self.midx[:t,-min(self.w,t):] # [t,w,1]\n",
        "            # kv = kv[torch.arange(b)[:,None,None,None], torch.arange(self.n_heads)[None,:,None,None], midx[None,None,...]]\n",
        "\n",
        "\n",
        "        kv = F.pad(kv, (0,0,self.w-1,0)) # [b, h, t+w-1, d]\n",
        "        # kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "        kv = kv.as_strided((b,self.n_heads,t,self.w,2*self.d_head), kv.stride()[:-1] + kv.stride()[-2:]) # [b,h,t,w,d] # repeat stride at w's dim\n",
        "\n",
        "\n",
        "            # mmask = midx>=0 # F->mask # [t,c]\n",
        "            # mmask = (midx>=0).unsqueeze(0) # F->mask # [1,t,c]\n",
        "        # else: kv = kv.unsqueeze(3).repeat(1,1,1,t,1) # [b,h,t,t(w),d]\n",
        "        print('midx', time.time()-start)\n",
        "\n",
        "# [batch, n_heads, T, d_head]\n",
        "# [b,h,t,d] @ [b,h,d,t] = [b,h,t,t]\n",
        "\n",
        "# [batch, n_heads, T, d_head]\n",
        "# [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "\n",
        "        mk, mv = kv.chunk(2, dim=-1) # [b,h,t,w,d]\n",
        "        # del kv\n",
        "        # attn fwd q mk attn torch.Size([64, 8, 127, 8]) torch.Size([64, 8, 100, 64, 8])\n",
        "        # print('attn fwd q mk attn', q.shape, mk.shape, kv.shape)\n",
        "        # attn = q.unsqueeze(3) @ mk.transpose(-2,-1) * self.scale # [b,h,t,1,d] @ [b,h,t,d,w] = [b,h,t,1,w]\n",
        "        # print('attn fwd q mk attn', q.shape, mk.shape, attn.shape, mmask.shape)\n",
        "        # q mk attn [64, 8, 100, 8], [64, 8, 100, 3, 8], [64, 8, 100, 1, 3], [64, 100, 3])\n",
        "        # q mk attn [64, 8, 100, 8], [64, 8, 100, 64, 8], [64, 8, 100, 1, 64], [100, 64])\n",
        "# attn fwd q mk attn torch.Size([64, 8, 100, 8]) torch.Size([64, 8, 100, 3, 8]) torch.Size([64, 8, 100, 1, 3]) torch.Size([100, 3])\n",
        "\n",
        "        attn = torch.einsum(\"bhtd,bhtwd->bhtw\", q, mk) * self.scale\n",
        "        # print('attn fwd q mk attn', q.dtype, mk.dtype, attn.dtype)\n",
        "\n",
        "        # if mask != None:\n",
        "        #     # attn = attn.masked_fill(~mmask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        #     attn = attn.masked_fill(~mmask[:,None,:,None], -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,1,t]\n",
        "        attention = torch.softmax(attn, dim=-1) # [b,h,t,1,w]\n",
        "        # out = (self.drop(attention) @ mv).squeeze(-2) # [b,h,t,1,w]@[b,h,t,w,d]=[b,h,t,1,d]\n",
        "        out = torch.einsum(\"bhtw,bhtwd->bhtd\", self.drop(attention), mv)\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "# td*dt->tt t(2t+d)\n",
        "# td*twd->tw t(~wd-(w+2)d)\n",
        "# iff 2t>wd\n",
        "# gen: xqkv, patch: ~xkv ~pq, gen: xqkv pkv\n",
        "# calc xqkv, calc ~xkv ~pq, get xqkv calc pkv\n",
        "\n",
        "# typically: gen td*td=tt\n",
        "# gen t1d*twd=tw, patch pd*tpd=p\n",
        "\n",
        "\n",
        "b,t,d = 64,100,512\n",
        "# mask = torch.rand(b,t,t, device=device)<.2\n",
        "\n",
        "\n",
        "# causal_mask = torch.tril(torch.ones((b,t,x.shape[1]), dtype=bool, device=device)) # [1,t,n_cond+n_patch] # got cond, all can attend to cond\n",
        "# # mask = causal_mask * mask.unsqueeze(1) if mask!=None else causal_mask # *[b,1,t] # mask left side, tril is lower left\n",
        "# mask = causal_mask * ~torch.tril(torch.ones_like(causal_mask, dtype=bool, device=device), diagonal=-64) # sliding window mask\n",
        "\n",
        "\n",
        "msk = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(msk, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(ind.shape, ind)\n",
        "mask = torch.zeros_like(msk, dtype=bool)\n",
        "mask[torch.arange(b)[:,None,None], torch.arange(t)[None,:,None], ind] = True\n",
        "\n",
        "\n",
        "# model = Attention(d).to(device) # 257 ms\n",
        "model = Attention(d, w=3).to(device) # 257 ms\n",
        "x = torch.rand(b,t,d, device=device)\n",
        "\n",
        "import time\n",
        "start = time.time()\n",
        "# out = model(x)\n",
        "out = model(x, mask=mask)\n",
        "print(out.shape)\n",
        "print(time.time()-start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gLnVZ-hYf97r"
      },
      "outputs": [],
      "source": [
        "# @title torch.profiler  as_strided unfold\n",
        "\n",
        "import torch\n",
        "b,t = 2,8\n",
        "mask = torch.rand(b,t,t)<.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "    with record_function(\"model_inference\"):\n",
        "        # to_midx(mask)\n",
        "        midx = midx[:t,-min(w,t):] # [t,w,1]\n",
        "        h=4\n",
        "        d=5\n",
        "        kv = torch.rand(b,h,t,d)\n",
        "        kv = kv[torch.arange(b)[:,None,None,None], torch.arange(h)[None,:,None,None], midx[None,None,...]]\n",
        "\n",
        "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
        "print(prof.key_averages().table())\n",
        "\n",
        "# import time\n",
        "# start = time.time()\n",
        "\n",
        "# print(time.time()-start)\n",
        "\n",
        "x = torch.rand(b,t,t)\n",
        "w = 3\n",
        "_, ind = torch.topk(x, w, dim=-1, sorted=False)\n",
        "\n",
        "# # print(c)\n",
        "b,h,t,d = 2,4,8,5\n",
        "k = torch.rand(b,h,t,d)\n",
        "k = torch.rand(b,h,t,d)\n",
        "q = torch.rand(b,h,t,d)\n",
        "\n",
        "k = F.pad(k, (0, 0, w - 1, 0))  # (B, H, T + W - 1, D)\n",
        "v = F.pad(v, (0, 0, w - 1, 0))\n",
        "k = F.pad(x, (0,0,w-1,0)) # [b, h, t+w-1, d]\n",
        "\n",
        "b,h,l,d = k.shape\n",
        "k_strided = k.as_strided(size=(b,h,t,w,d), stride=(k.stride(0), k.stride(1), k.stride(2), k.stride(2), k.stride(3)))\n",
        "v_strided = v.as_strided(size=(b,h,t,w,d), stride=(v.stride(0), v.stride(1), v.stride(2), v.stride(2), v.stride(3)))\n",
        "\n",
        "o = k.as_strided((b,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "\n",
        "attn_scores = torch.einsum(\"bhtd,bhtwd->bhtw\", q, k_strided) / math.sqrt(d)\n",
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "out = torch.einsum(\"bhtw,bhtwd->bhtd\", attn_weights, v_strided)\n",
        "\n",
        "# # b,h,t,d = 2,4,8,5\n",
        "# k = torch.rand(b,h,t,d)\n",
        "# # # print(k.stride(0), k.stride(1), k.stride(2), k.stride(2), k.stride(3))\n",
        "# print(k.stride())\n",
        "# # print(torch.rand(2,3,5,7).stride())\n",
        "# # 3*5*7,5*7,7,1\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.rand(2,3,5)\n",
        "print(x)\n",
        "# # o = torch.as_strided(x, (2, 2), (1, 2))\n",
        "# o = torch.as_strided(x, (2, 5), (1, 3))\n",
        "# print(o)\n",
        "# o = torch.as_strided(x, (2, 5), (2, 3))\n",
        "b,t,d = x.shape\n",
        "w=5\n",
        "k = F.pad(x, (0,0,w-1,0)) # [b, t, t+w-1, d]\n",
        "print(k.shape)\n",
        "# o = k.as_strided(size=(b,t,w,d), stride=(k.stride(0), k.stride(1), k.stride(1), k.stride(2)))\n",
        "# o = k.as_strided((b,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "# o = k.as_strided((b,h,t,w,d), k.stride()[:-1] + k.stride()[-2:]) # repeat stride at w's dim\n",
        "\n",
        "# o = k.view(b*h,t+w-1,d).unfold(dimension=1, size=w, step=1).view(b,h,t,w,d)\n",
        "# o = k.view(b,t+w-1,d).unfold(dimension=1, size=w, step=1).view(b,t,w,d)\n",
        "# o = k.unfold(dimension=1, size=w, step=1)#.view(b,t,w,d)\n",
        "o = k.unfold(dimension=-2, size=w, step=1).transpose(-2,-1)\n",
        "\n",
        "print(o.shape, o)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NF9atY_nDOoV"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}