{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/transformer/blob/main/gpt_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ugFOONfWAmx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import requests\n",
        "# url=\"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\" # https://www.tensorflow.org/text/tutorials/text_generation\n",
        "url=\"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt\" # train test valid # https://pytorch.org/text/stable/datasets.html#penntreebank\n",
        "out=requests.get(url)\n",
        "with open(\"data.txt\", \"wb\") as f:\n",
        "    f.write(out.content)\n",
        "text = open(\"data.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt\" # train test valid # https://pytorch.org/text/stable/datasets.html#penntreebank\n",
        "out=requests.get(url)\n",
        "with open(\"data_.txt\", \"wb\") as f: f.write(out.content)\n",
        "test_text = open(\"data_.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# print(len(text))\n",
        "# print(text[000:1000])\n",
        "# data = ''.join(text)\n",
        "# chars = sorted(list(set(data)))\n",
        "# print(chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets # restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdvtl69KzIm8",
        "outputId": "0d31fd7c-9677-44d1-9aa3-b21729c67e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hf dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "# https://huggingface.co/datasets/Salesforce/wikitext\n",
        "dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\") # wikitext-103-raw-v1\n",
        "\n",
        "text = dataset[\"train\"][\"text\"]\n",
        "test_text = dataset[\"test\"][\"text\"]\n"
      ],
      "metadata": {
        "id": "TBiusu6rOPJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef989556-9f8c-47de-ffc1-aff2fb5b6f3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title tiktoken dataloader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import tiktoken # https://github.com/openai/tiktoken/tree/main\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        data = ''.join(raw_data)\n",
        "        self.enc = tiktoken.get_encoding(\"gpt2\") # https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "        self.vocab_size = self.enc.n_vocab # gpt2:50257\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor(self.enc.encode(data))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=2, drop_last=True) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle=True, pin_memory=True, batch_size=batch_size, num_workers=0, drop_last=True)\n",
        "\n",
        "# https://github.com/openai/tiktoken/blob/main/tiktoken/core.py\n",
        "def encode(context):\n",
        "    if type(context) == str: return torch.tensor([train_loader.dataset.enc.encode(context)], device=device)\n",
        "    elif type(context) == list: return train_loader.dataset.enc.encode_batch(context)\n",
        "    else: raise Exception\n",
        "def decode(x): return train_loader.dataset.enc.decode(list(x))\n",
        "# for x,y in train_loader:\n",
        "#     break\n",
        "\n"
      ],
      "metadata": {
        "id": "i1iatz1SSK3s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV5HmCFv_ITo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title char dataloader\n",
        "# https://github.com/Sam-Armstrong/tinyGPT/blob/main/Training.py\n",
        "# https://colab.research.google.com/github/karpathy/minGPT/blob/master/play_char.ipynb\n",
        "# https://github.com/karpathy/nanoGPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CharDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, raw_data, seq_len):\n",
        "        data = ''.join(raw_data)\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.vocab_size = len(chars) # 283\n",
        "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "        self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "        self.data = self.data_process(data) # list of int\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def data_process(self, data): # str 10780437\n",
        "        return torch.tensor([self.stoi.get(c) for c in data]) # list of int 4570571 # stoi.get(c,UNK_IDX)\n",
        "\n",
        "    def __len__(self):\n",
        "        # return len(self.data) - self.seq_len\n",
        "        return len(self.data)//(self.seq_len+1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # dix = self.data[idx:idx + self.seq_len + 1]\n",
        "        dix = self.data[idx*(self.seq_len+1) : (idx+1)*(self.seq_len+1)]\n",
        "        x, y = dix[:-1], dix[1:]\n",
        "        return x, y\n",
        "\n",
        "\n",
        "seq_len = 100 # 128\n",
        "train_data = CharDataset(text, seq_len) # one line of poem is roughly 50 characters\n",
        "test_data = CharDataset(test_text, seq_len) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 64 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2) # num_workers = 4\n",
        "test_loader = DataLoader(test_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "\n",
        "def encode(context): return torch.tensor([train_data.stoi.get(c) for c in context], device=device).unsqueeze(0)\n",
        "def decode(x): return ''.join([train_data.itos[int(i)] for i in x])\n",
        "# for x,y in train_loader:\n",
        "#     break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RoPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = (pos * theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).to(device) # [seq_len, dim // 2, 2]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: self.__init__(dim, seq_len)\n",
        "        rot_emb = self.rot_emb[:seq_len].unsqueeze(0).expand(batch, -1, -1, -1) # [batch, seq_len, dim//2, 2]\n",
        "        x = x.reshape(batch, seq_len, dim // 2, 2)\n",
        "        rot_x = x * rot_emb\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "dim=16\n",
        "seq_len=512\n",
        "rope = RoPE(dim, seq_len, base=10000)\n",
        "x = torch.rand(4,64,dim, device=device)\n",
        "out = rope(x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "id": "aZ8q6DxC3P9B",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38ac903-0844-43a1-ad2d-435c56aaf1f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 64, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n9a9OwgKjTUP",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title snake\n",
        "# https://github.com/Aria-K-Alethia/BigCodec/blob/main/vq/activations.py\n",
        "# https://github.com/zhenye234/X-Codec-2.0/blob/main/vq/activations.py#L62\n",
        "# Implementation adapted from https://github.com/EdwardDixon/snake under the MIT license.\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# class Snake(nn.Module):\n",
        "#     def __init__(self, in_features, alpha=1.0, alpha_logscale=False):\n",
        "#         super().__init__()\n",
        "#         # self.in_features = in_features\n",
        "#         self.alpha_logscale = alpha_logscale\n",
        "#         if self.alpha_logscale: # log scale alphas initialized to zeros\n",
        "#             self.alpha = nn.Parameter(torch.zeros(in_features) * alpha)\n",
        "#         else: # linear scale alphas initialized to ones\n",
        "#             self.alpha = nn.Parameter(torch.ones(in_features) * alpha)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         alpha = self.alpha.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "#         if self.alpha_logscale:\n",
        "#             alpha = torch.exp(alpha)\n",
        "#         x = x + (1.0 / (alpha + 1e-9)) * torch.pow(torch.sin(x * alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "#         return x\n",
        "\n",
        "\n",
        "class Snake(nn.Module):\n",
        "    def __init__(self, dim, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        # self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp() # alpha_logscale=True\n",
        "        self.alpha = nn.Parameter(torch.ones(1,1,dim)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + (1.0 / (self.alpha + 1e-9)) * torch.pow(torch.sin(x * self.alpha), 2) # Snake ∶= x + 1/a * sin^2(ax)\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def snake(x, alpha): # [b,c,t], [1,c,1]\n",
        "    return x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2) # [b,c,t]\n",
        "\n",
        "class Snake1d(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,1,dim)).exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,1,dim)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return snake(x, self.alpha)\n",
        "\n",
        "\n",
        "\n",
        "class SnakeBeta(nn.Module):\n",
        "    def __init__(self, dim, alpha=1.0, alpha_logscale=False):\n",
        "        super().__init__()\n",
        "        self.alpha_logscale = alpha_logscale\n",
        "        self.alpha = nn.Parameter(torch.ones(1,1,dim) * (0. if self.alpha_logscale else alpha))\n",
        "        self.beta = nn.Parameter(torch.ones(1,1,dim) * (0. if self.alpha_logscale else alpha))\n",
        "\n",
        "    def forward(self, x): # [b,c,t]\n",
        "        alpha = self.alpha#.unsqueeze(0).unsqueeze(-1) # line up with x to [B, C, T]\n",
        "        beta = self.beta#.unsqueeze(0).unsqueeze(-1)\n",
        "        if self.alpha_logscale:\n",
        "            alpha = torch.exp(alpha)\n",
        "            beta = torch.exp(beta)\n",
        "        x = x + (1. / (beta + 1e-9)) * pow(torch.sin(x * alpha), 2) # SnakeBeta ∶= x + 1/b *sin^2(ax)\n",
        "        return x # [b,c,t]\n",
        "\n",
        "b,c,t = 5,16,7\n",
        "# a1 = Snake(c)\n",
        "a1 = Snake(c, alpha_logscale=True) # 70.4 µs 69.9\n",
        "# a1 = Snake1d(c) # 47.8 µs 48.3\n",
        "# a1 = SnakeBeta(256)\n",
        "x = torch.randn(b,c,t)\n",
        "# x = a1(x)\n",
        "# print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ntuxncLD7PaQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Swwish\n",
        "import torch.nn as nn\n",
        "\n",
        "# import math\n",
        "# s = (2/math.pi)**.5\n",
        "# a1=(2*s)**(1/2)\n",
        "# a2=(4*s)**(1/4)\n",
        "# a3=(8*s)**(1/6)\n",
        "# 1/2(1+x-cos(x))\n",
        "# 1/2(1+x-cos(a*x))\n",
        "\n",
        "@torch.jit.script\n",
        "def swwish(x): return .5 * (1 + x - x.cos())\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def learntswwish(x, alpha): # [b,c,t], [1,c,1]\n",
        "    return .5 * (1 + x - torch.cos(alpha * x)) # [b,c,t]\n",
        "    # return .5 * (1/alpha + x - torch.cos(alpha * x)/alpha) # [b,c,t]\n",
        "\n",
        "class LearntSwwish(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.zeros(1,dim,1)).exp()\n",
        "        self.alpha = nn.Parameter(torch.ones(1,dim,1)*1.) # 1.\n",
        "\n",
        "    def forward(self, x):\n",
        "        return learntswwish(x, self.alpha)\n",
        "\n",
        "\n",
        "class Swwish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return .5 * (1 + x - x.cos())\n",
        "        return .5 * (1 + x - 1.25*x.cos())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title enc dec trans new\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0.): # .1\n",
        "        super().__init__()\n",
        "        self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.scale = self.d_head**-.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        k, v = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(q @ k.T) @ v\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask.unsqueeze(1) if mask != None else None, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # out = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=0) # mask: [batch,len_q, len_v]\n",
        "        # attn = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # if mask != None: attn = attn.masked_fill(mask.unsqueeze(1), -torch.finfo(attn.dtype).max) # [b,t,t]->[b,1,t,t]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ v # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1,2).flatten(2)\n",
        "        return self.drop(self.lin(out)) # [batch, T, d_model]\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module): # https://arxiv.org/pdf/2002.05202\n",
        "    def __init__(self, d_model, ff_dim): # d_model * 3*ff_dim params\n",
        "        super().__init__()\n",
        "        self.lin0 = nn.Linear(d_model, 2*ff_dim, bias=False)\n",
        "        self.lin1 = zero_module(nn.Linear(ff_dim, d_model, bias=False))\n",
        "\n",
        "    def forward(self, x): # [b,t,d]\n",
        "        x0, x1 = self.lin0(x).chunk(2, dim=-1)\n",
        "        return self.lin1(x0*F.silu(x1))\n",
        "\n",
        "# 2048*2\n",
        "# 2048*7\n",
        "# ff: d_model*2 *ff_dim params\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.norm = nn.RMSNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        ff_dim = ff_dim or d_model*4\n",
        "        # act = nn.ReLU() # ReLU SiLU GELU\n",
        "        # act = Snake(ff_dim, alpha_logscale=True) # 70.4 µs 69.9\n",
        "        # act = Snake1d(ff_dim) # 47.8 µs 48.3\n",
        "        # act = SnakeBeta(ff_dim)\n",
        "        # act = Swwish()\n",
        "        # self.ff = nn.Sequential(\n",
        "        #     nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), act,\n",
        "        #     nn.RMSNorm(ff_dim), nn.Dropout(dropout), zero_module(nn.Linear(ff_dim, d_model))\n",
        "        # )\n",
        "        self.ff = SwiGLU(d_model, ff_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.drop(self.attn(self.norm(x), mask=mask))\n",
        "        x = x + self.drop(self.ff(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# b,t,d = 2,5,16\n",
        "# x = torch.rand(b,t,d)\n",
        "# mask = torch.rand(b,t,t)>0\n",
        "# model = EncoderLayer(d_model=d, n_heads=4, ff_dim=16)\n",
        "# model = nn.Sequential(*[EncoderLayer(d_model=d, n_heads=4, ff_dim=16) for _ in range(2)])\n",
        "# out =  model(x, mask)\n",
        "# # out =  model(x)\n",
        "# print(out.shape)\n"
      ],
      "metadata": {
        "id": "Ygkv7B71JHP1",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GPT\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'mask' in layer._fwdparams: args.append(mask)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, in_dim, d_model=64, out_dim=None, n_heads=8, n_layers=1, ff_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # self.pos_enc = PositionalEncoder(d_model, dropout=dropout)\n",
        "        # self.pos_enc = LearntPosEnc(d_model, dropout=dropout)\n",
        "        self.pos_enc = RoPE(d_model, base=10000)\n",
        "        self.tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.encoder = Seq(*[EncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "        self.out = nn.Linear(d_model, out_dim)\n",
        "\n",
        "    def forward(self, x, mask=None): # [b,t], [b,t,t]\n",
        "        x = self.pos_enc(self.tok_emb(x))\n",
        "        # print('gpt fwd', x.shape)\n",
        "        x = self.encoder(x, mask)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "input_size = num_classes = 50257 #train_data.vocab_size#50 # gpt2:50257\n",
        "# print(train_data.vocab_size)\n",
        "# model = GPT(input_size, d_model=64, out_dim=num_classes, n_layers=1).to(device)\n",
        "model = GPT(input_size, d_model=512, out_dim=num_classes, n_layers=6).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "print(sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "# optim = torch.optim.AdamW(model.parameters(), 3e-4)\n",
        "\n",
        "# batch 64, seqlen 100\n",
        "# dim 64, nlyrs 1: 5.0gb\n",
        "# 512, 1, 6.3, 95sec\n",
        "# dim 512, 6 lyrs, 8.7 gb, 125 sec, 59393105 params\n",
        "# dim 768, 12 lyrs, 12.4 gb, 257 sec, 110318161 params\n",
        "\n",
        "# dim 64 ff 256, nlyrs 1, 5.0gb, ptb/wiki2 21/43 sec, 6548817 params\n",
        "# dim 512 ff *4, nlyrs 6, 6.3gb, ptb/wiki2 21/43 sec, 6548817 params\n",
        "\n",
        "# dim 512 ff *4, nlyrs 6, 9.2?gb, wiki2 348 sec, 76685393 params\n",
        "\n",
        "x = torch.randint(0, input_size, (2, 5), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "# print(out)\n"
      ],
      "metadata": {
        "id": "4J73LuO9XUcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccddc558-824c-4526-a9a4-0c74c7d45467"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76685393\n",
            "25171968\n",
            "torch.Size([2, 5, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title perplexity\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# https://www.comet.com/site/blog/perplexity-for-llm-evaluation/\n",
        "\n",
        "def Perplexity(logits, target): # [b,t,vocab_size], [b,t]\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    nll = -log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1) # [b,t]\n",
        "    perplexity = nll.mean().exp()\n",
        "    return perplexity\n",
        "\n",
        "# logits = torch.randn(2, 4, 10)\n",
        "# target = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "\n",
        "# perplexity = Perplexity(logits, target)\n",
        "# # perplexity = Perplexity(logits[:,:-1], y[:,1:])\n",
        "# print(f'Perplexity: {perplexity}')\n"
      ],
      "metadata": {
        "id": "Ac5qCuO4imHk",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCN055Zr7Dq4",
        "outputId": "5e6bba56-17a5-4f8b-be45-ce76df9dc52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "strain 10.91341781616211\n",
            "strain 7.016169548034668\n",
            "strain 6.5468645095825195\n",
            "strain 6.219416618347168\n",
            "strain 6.200089931488037\n",
            "strain 5.935671806335449\n",
            "strain 5.72556209564209\n",
            "strain 5.687353610992432\n",
            "strain 5.615914344787598\n",
            "strain 5.5544328689575195\n",
            "test 6.043789863586426 ppty 7036.03369140625\n",
            "Test Loss: 5.976807771727096\n",
            "this is what it as part of weapons into 68 . The mass and episodes to gather by part of whom he studied in tribute structures that the assassination bear that as Islamic competitiveness . Mynmann , who subsidiary of audio is towards , ignores , were described as he completed angel when a staco , a lack of the violate but asked Squad\n",
            "0 time: 350.43189120292664 350.4318919181824\n",
            "strain 5.009577751159668\n",
            "strain 4.8494768142700195\n",
            "strain 5.0731425285339355\n",
            "strain 4.983185768127441\n",
            "strain 4.875759124755859\n",
            "strain 5.0334062576293945\n",
            "strain 4.875674247741699\n",
            "strain 4.817008018493652\n",
            "strain 4.867505073547363\n",
            "strain 5.014249801635742\n",
            "test 5.752124786376953 ppty 8991.6494140625\n",
            "Test Loss: 5.886448106100393\n",
            "this is what or the need for engaging except by theic graph , or spin cricket , Coldrumic graph , usually a foul with insight orbit in FI ≤ in setting and a finite of the upperparts . It is believed but thus to hand with a canvas edges as a camera . If a newer vertex of Bass spaces . We tend\n",
            "1 time: 347.67666506767273 349.0547535419464\n",
            "strain 4.135540008544922\n",
            "strain 4.148737907409668\n",
            "strain 4.334808826446533\n",
            "strain 4.252145767211914\n",
            "strain 4.243311882019043\n",
            "strain 4.400999546051025\n",
            "strain 4.456773281097412\n",
            "strain 4.477306365966797\n",
            "strain 4.498301982879639\n",
            "strain 4.43996524810791\n",
            "test 6.019599437713623 ppty 10959.248046875\n",
            "Test Loss: 5.956672590832378\n",
            "this is what may remaining in animal horn from pornability each other and English reactions as definitive animals . One , actively transported into these hairs and cross @-@ top @-@ way to have are usually on their post @-@ thickness @-@ one seconds as fade to a low , polyure energy are underparts . This\n",
            "2 time: 348.20471143722534 348.77180592219037\n",
            "strain 3.4916727542877197\n",
            "strain 3.586245536804199\n",
            "strain 3.7131175994873047\n",
            "strain 3.891062021255493\n",
            "strain 3.948087692260742\n",
            "strain 3.916754722595215\n",
            "strain 3.9871819019317627\n",
            "strain 4.109561443328857\n",
            "strain 4.029985427856445\n",
            "strain 4.055621147155762\n",
            "test 6.077413558959961 ppty 16195.544921875\n",
            "Test Loss: 6.114378263784009\n",
            "this is what 20eine Pooa ( 1615 ) and means of firstiley . ) . There is merely known for example , which is still at sound quality of Ettyards , in the Countha Aran , like a moment at communities . \n",
            " Several more de lakhuri ) , and is now the K\n",
            "3 time: 347.8749837875366 348.54801309108734\n",
            "strain 3.111905097961426\n",
            "strain 3.105210065841675\n",
            "strain 3.277383327484131\n",
            "strain 3.2313168048858643\n",
            "strain 3.435654401779175\n",
            "strain 3.435821771621704\n",
            "strain 3.581084966659546\n",
            "strain 3.6593024730682373\n",
            "strain 3.591351270675659\n",
            "strain 3.6730194091796875\n",
            "test 6.306903839111328 ppty 22282.046875\n",
            "Test Loss: 6.300528116004411\n",
            "this is what the shandava firm tied with along with Typical it emergesyin . Once again in the light , it is , and the tiger down and are being called Ska of which is made without being viewed as an resembling an sailing down up to his wife , and the Younger , the 1 kilometre , for the new\n",
            "4 time: 348.07960653305054 348.45468468666076\n",
            "strain 2.5351734161376953\n",
            "strain 2.6638448238372803\n",
            "strain 2.864673137664795\n",
            "strain 2.951021432876587\n",
            "strain 3.081557512283325\n",
            "strain 3.0728402137756348\n",
            "strain 3.132157802581787\n",
            "strain 3.2051281929016113\n",
            "strain 3.2149481773376465\n",
            "strain 3.3325490951538086\n",
            "test 6.448784351348877 ppty 32240.220703125\n",
            "Test Loss: 6.517760343329851\n",
            "this is what Loom was stated and say across manual about that that a cases in Mandarin . \n",
            " Based upon what Maeda was represented a pirate against his parents \" Bass , Togeded , now friends , and a with three hundred servants that \" there intestines portray impatient — has said ! \" X developing mushroom @-@\n",
            "5 time: 347.98443627357483 348.3764237165451\n",
            "strain 2.297191858291626\n",
            "strain 2.3335514068603516\n",
            "strain 2.311148166656494\n",
            "strain 2.495535373687744\n",
            "strain 2.7237560749053955\n",
            "strain 2.700533151626587\n",
            "strain 2.705507516860962\n"
          ]
        }
      ],
      "source": [
        "# @title train test generate\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler()\n",
        "\n",
        "def strain(model, dataloader, optimizer, scheduler=None): # train function with automatic mixed precision\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            causal_mask = torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1) # for F.scaled_dot_product_attention\n",
        "            # causal_mask = ~torch.ones(x.size(1), x.size(1), dtype=bool, device=device).tril(diagonal=0).repeat(x.shape[0],1,1)\n",
        "            logits = model(x, mask=causal_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            # logits, _ = model(x) # rnn\n",
        "            loss = F.cross_entropy(logits.flatten(0,1), y.flatten()) # [b*t,d], [b*t]\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optim)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # if scheduler is not None: scheduler.step()\n",
        "        if i % (len(dataloader)//10) == 0: print(\"strain\",loss.item())\n",
        "        total_loss += loss.item()\n",
        "        # try: wandb.log({\"train loss\": loss.item()/len(y)})\n",
        "        try: wandb.log({\"train loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            # logits, _ = model(x) # rnn\n",
        "            perplexity = Perplexity(logits[:,:-1], y[:,1:])\n",
        "        loss = F.cross_entropy(logits.flatten(0,-2), y.flatten()) # [batch*seq_len, vocab_size], [batch*seq_len]\n",
        "        total_loss+=loss.item()\n",
        "        # if i % 100 == 0: print(\"test\",loss.item())\n",
        "        if i % 100 == 0: print(\"test\",loss.item(),\"ppty\",perplexity.item())\n",
        "        # try: wandb.log({\"test loss\": loss.item()/len(y)})\n",
        "        try: wandb.log({\"test loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def generate(model, context, max_steps=64, temperature=1):\n",
        "    x = encode(context)#.to(device)\n",
        "    model.eval()\n",
        "    for n in range(max_steps):\n",
        "        with torch.no_grad():\n",
        "            output = model(x)\n",
        "            # output, hidden = model(x, hidden) # rnn\n",
        "        # print('generate', output.shape, hidden.shape)\n",
        "        # hidden = hidden[:, -1, :].unsqueeze(1) # RNN/GRU\n",
        "        output = output[:, -1] # get logit for last character\n",
        "        output = output/temperature\n",
        "        output = F.softmax(output, dim=-1) # vocab_size to char\n",
        "        ix = torch.multinomial(output, num_samples=1) # rand sample by output distribution\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    completion = decode(x.squeeze(0))\n",
        "    return completion\n",
        "\n",
        "import time\n",
        "start = begin = time.time()\n",
        "# for i in range(1):\n",
        "for i in range(30):\n",
        "    train_loss = strain(model, train_loader, optim, scheduler=None)\n",
        "    test_loss = test(model, test_loader)\n",
        "    print('Test Loss:', test_loss)\n",
        "    print(generate(model, \"this is what\"))\n",
        "    print(i, 'time:',time.time() - start, (time.time()-begin)/(i+1))\n",
        "    start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "2Nd-sGe6Ku4S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "2cb60251-611d-47f4-c0e9-fd999e442cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test loss</td><td>▆▅▆▇▃▆▆▆▇▇▆▆█▆▆█▆▇▆▄▂▃▂▂▃▂▄▂▃▂▂▄▁▁▂▂▁▂▂▃</td></tr><tr><td>train loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test loss</td><td>6.05217</td></tr><tr><td>train loss</td><td>1.5716104253819703e+24</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">silvery-mountain-6</strong> at: <a href='https://wandb.ai/bobdole/gpt/runs/lty5wafo' target=\"_blank\">https://wandb.ai/bobdole/gpt/runs/lty5wafo</a><br> View project at: <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">https://wandb.ai/bobdole/gpt</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250521_020213-lty5wafo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250521_022424-ktdyre6c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/gpt/runs/ktdyre6c' target=\"_blank\">amber-jazz-7</a></strong> to <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/gpt' target=\"_blank\">https://wandb.ai/bobdole/gpt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/gpt/runs/ktdyre6c' target=\"_blank\">https://wandb.ai/bobdole/gpt/runs/ktdyre6c</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"gpt\", config={\"model\": \"res18\",}) #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RNN pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_dim, d_model, out_dim=None, num_layers=1):\n",
        "        super().__init__()\n",
        "        if out_dim is None: out_dim = in_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.emb = nn.Embedding(in_dim, d_model)\n",
        "        # self.rnn = nn.RNN(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.rnn = nn.GRU(d_model, d_model, num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(d_model, d_model, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(d_model, out_dim)\n",
        "\n",
        "        # for p in self.parameters():\n",
        "        #     if p.dim() > 1:\n",
        "        #         nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x, hc=None): # lstm [batch_size, seq, in_dim]\n",
        "        x = self.emb(x)\n",
        "        if hc is None:\n",
        "            h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "            c0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        else: h0,c0 = hc\n",
        "        # print(x.shape, h0.shape,c0.shape)\n",
        "        out, (h,c) = self.lstm(x, (h0,c0)) # [batch, seq_len, d_model], ([num_layers, batch, d_model] )\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, (h, c)\n",
        "\n",
        "    def forward(self, x, h=None): # rnn/gru\n",
        "        x = self.emb(x)# * self.d_model**.5\n",
        "        if h is None: h0 = torch.zeros((self.num_layers, x.size(0), self.d_model), device=device)\n",
        "        else: h0 = h\n",
        "        # print(x.shape, h0.shape)\n",
        "        out, h = self.rnn(x, h0)\n",
        "        # out = out[:, -1, :] # out: (n, 128)\n",
        "        out = self.fc(out) # out: (n, 10)\n",
        "        return out, h\n",
        "\n",
        "hidden_size = 64 #128\n",
        "num_layers = 1#2\n",
        "input_size = num_classes = 50#train_data.vocab_size#65\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_classes, num_layers).to(device)\n",
        "# print(model)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), 1e-3)\n",
        "\n",
        "# 128,2\n",
        "# Test Loss: 6.360389362062727\n",
        "# this is what ween new york is well it a sign more directly into simply shares\n",
        "# 0 time: 5.910429954528809 5.910431623458862\n",
        "# 64,2\n",
        "# Test Loss: 6.167358561924526\n",
        "# this is what bull morp on its aftant opereals of a b. the hamally plans beces\n",
        "# 0 time: 5.655158996582031 5.655160903930664\n",
        "# 64,1\n",
        "# Test Loss: 5.823708357129778\n",
        "# this is what achan agrive\n",
        "#  the guinesst on of promjects cl funds that jound\n",
        "# 0 time: 4.788918495178223\n",
        "\n",
        "# Test Loss: 8.247572830745153\n",
        "# this is what that has months with unfinsings lide to N by well\n",
        "#  next offited\n",
        "# 29 time: 3.902247905731201 4.377542002995809\n",
        "\n",
        "b,t=2,5\n",
        "\n",
        "x = torch.randint(0,input_size, (b,t), device=device)\n",
        "h = torch.rand(num_layers, b, hidden_size, device=device)\n",
        "out, h = model(x, h)\n",
        "print(out.shape, h.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5CjHMaQE8MM",
        "outputId": "9527a47d-2c98-4256-fd59-93276456a1e3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31410\n",
            "torch.Size([2, 5, 50]) torch.Size([1, 2, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCgV-efIfzne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ac88dc-a723-4a75-dc92-da877e3eca3d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 1, 10]) torch.Size([64, 1, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# @title mask translate\n",
        "PAD_IDX=0\n",
        "def make_src_mask(src):\n",
        "    # return (src != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device) # [batch_size, 1, 1, src_len]?\n",
        "    return (src != PAD_IDX)[:,None,None,:].to(device) # [batch_size, 1, 1, src_len]?\n",
        "\n",
        "# attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "def make_trg_mask(trg):\n",
        "    # trg_pad_mask = (trg != PAD_IDX).unsqueeze(1).unsqueeze(2).to(device)\n",
        "    trg_pad_mask = (trg != PAD_IDX)[:,None,None,:].to(device) # [batch, 1, 1, trg_len]\n",
        "    trg_len = trg.shape[1]\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
        "    # print('make_trg_mask', trg_pad_mask.shape, trg_sub_mask.shape) # [64, 1, 1, 10], [10, 10]\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask # [batch, 1, trg_len, trg_len]?\n",
        "    return trg_mask\n",
        "\n",
        "def translate(model, src_sentence):\n",
        "    model.eval()\n",
        "    src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "    num_tokens = src.shape[1]\n",
        "    trg_indexes = [BOS_IDX]\n",
        "    max_len = src.shape[1]+5\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output = model(src, trg_tensor, src_mask, trg_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == EOS_IDX: break\n",
        "    trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "    return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "def translate_fast(model, src_sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = de_transform(src_sentence).view(1,-1).to(device)\n",
        "        num_tokens = src.shape[1]\n",
        "        trg_indexes = [BOS_IDX]\n",
        "        max_len = src.shape[1]+5\n",
        "        src_mask = make_src_mask(src)\n",
        "        output = model.encode(src, src_mask)\n",
        "        for i in range(max_len):\n",
        "            trg_tensor = torch.tensor(trg_indexes, dtype=torch.long, device=device).unsqueeze(0)\n",
        "            trg_mask = make_trg_mask(trg_tensor)\n",
        "            output = model.decode(src, trg_tensor, src_mask, trg_mask)\n",
        "            pred_token = output.argmax(2)[:,-1].item() # batch_first=F -> ?\n",
        "            trg_indexes.append(pred_token)\n",
        "            if pred_token == EOS_IDX: break\n",
        "        trg_tokens = torch.tensor(trg_indexes[1:-1]).flatten()\n",
        "        return \" \".join(en_vocab.lookup_tokens(list(trg_tokens.cpu().numpy())))\n",
        "\n",
        "# UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "# print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "src, trg = torch.randint(0, 100, (64, 10)), torch.randint(0, 100, (64, 10))\n",
        "sm = make_src_mask(src)\n",
        "tm = make_trg_mask(trg)\n",
        "# print(sm.shape, tm.shape) # [64, 1, 1, 10], [64, 1, 10, 10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5717784-aa63-44e9-ac7d-c967bb03b9b6",
        "id": "N9R4hVUCxGiu",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.402, Val loss: 4.186, Epoch time = 41.608s\n",
            "A group of people are are are are are are in a .\n",
            "Epoch: 2, Train loss: 3.898, Val loss: 3.545, Epoch time = 41.068s\n",
            "A group of people are standing in a crowd of people .\n",
            "Epoch: 3, Train loss: 3.353, Val loss: 3.125, Epoch time = 41.566s\n",
            "A group of people standing in front of a crowd .\n",
            "Epoch: 4, Train loss: 2.944, Val loss: 2.830, Epoch time = 40.756s\n",
            "A group of people standing in front of a building .\n",
            "Epoch: 5, Train loss: 2.630, Val loss: 2.596, Epoch time = 41.468s\n",
            "A group of people standing in front of a crowd .\n",
            "Epoch: 6, Train loss: 2.375, Val loss: 2.429, Epoch time = 41.023s\n",
            "A group of people standing in front of a house .\n",
            "Epoch: 7, Train loss: 2.166, Val loss: 2.307, Epoch time = 41.604s\n",
            "A group of people stand in front of a house .\n",
            "Epoch: 8, Train loss: 1.984, Val loss: 2.210, Epoch time = 40.876s\n",
            "A group of people stand in front of an audience .\n",
            "Epoch: 9, Train loss: 1.834, Val loss: 2.131, Epoch time = 41.496s\n",
            "A group of people are standing in front of an audience .\n",
            "Epoch: 10, Train loss: 1.698, Val loss: 2.079, Epoch time = 41.052s\n",
            "A group of people are standing in front of an empty house .\n",
            "Epoch: 11, Train loss: 1.576, Val loss: 2.038, Epoch time = 41.570s\n",
            "A group of people are standing in front of an audience .\n",
            "Epoch: 12, Train loss: 1.475, Val loss: 2.033, Epoch time = 41.067s\n",
            "A group of people stand in front of an audience .\n",
            "Epoch: 13, Train loss: 1.381, Val loss: 2.017, Epoch time = 41.576s\n",
            "A group of people stand in front of an operation .\n",
            "Epoch: 14, Train loss: 1.292, Val loss: 1.977, Epoch time = 40.779s\n",
            "A group of people stand in front of an operation .\n",
            "Epoch: 15, Train loss: 1.213, Val loss: 1.948, Epoch time = 41.461s\n",
            "A group of people stand in front of an operation .\n",
            "Epoch: 16, Train loss: 1.139, Val loss: 1.940, Epoch time = 40.800s\n",
            "A group of people stand in front of an igloo\n",
            "Epoch: 17, Train loss: 1.073, Val loss: 1.940, Epoch time = 41.533s\n",
            "A group of people stand in front of an igloo\n",
            "Epoch: 18, Train loss: 1.006, Val loss: 1.939, Epoch time = 40.856s\n",
            "A group of people stand in front of an igloo\n",
            "Epoch: 19, Train loss: 0.944, Val loss: 1.947, Epoch time = 41.345s\n",
            "A group of people stand in front of an igloo\n",
            "Epoch: 20, Train loss: 0.893, Val loss: 1.956, Epoch time = 40.935s\n",
            "A group of people stand in front of an igloo\n"
          ]
        }
      ],
      "source": [
        "# @title translate train test\n",
        "\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, trg in dataloader:\n",
        "        src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "        trg_input = trg[:,:-1]\n",
        "        src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "        print('train', src.shape, trg.shape, src_mask.shape, trg_mask.shape)\n",
        "        output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(list(dataloader))\n",
        "\n",
        "def test(model, dataloader, loss_fn):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in dataloader:\n",
        "            src, trg = src.to(device), trg.to(device) #trg = [batch size, trg len]\n",
        "            trg_input = trg[:,:-1]\n",
        "            src_mask, trg_mask = make_src_mask(src), make_trg_mask(trg_input)\n",
        "            output = model(src, trg_input, src_mask, trg_mask) #output = [batch size, trg len - 1, output dim]\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), trg[:,1:].reshape(-1))\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(list(dataloader))\n",
        "\n",
        "# @title run\n",
        "import time\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) # lr=0.0001\n",
        "\n",
        "# for epoch in range(20):\n",
        "for epoch in range(1):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    val_loss = test(model, val_loader, loss_fn)\n",
        "    end_time = time.time()\n",
        "    print((f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    # print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people standing in front of an igloo .\n",
        "    # @title inference\n",
        "    print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people stand in front of an igloo .\n",
        "    print(translate(model, \"Ein Koch in weißer Uniform bereitet Essen in einer Restaurantküche zu .\")) # A chef in a white uniform prepares food in a restaurant kitchen .\n",
        "    print(translate(model, \"Zwei junge Mädchen spielen Fußball auf einem Feld. .\")) # Two young girls play soccer on a field. .\n",
        "    print(translate(model, \"Eine Frau mit Hut und Sonnenbrille steht am Strand .\")) # A woman wearing a hat and sunglasses stands on the beach .\n",
        "    print(translate(model, \"Zwei Freunde lachen und genießen ein Eis auf einer wunderschönen Wiese .\")) # Two friends laugh and enjoy ice cream on a beautiful meadow .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Epoch: 1, Train loss: 5.402, Val loss: 4.186, Epoch time = 41.608s\n",
        "A group of people are are are are are are in a .\n",
        "Epoch: 2, Train loss: 3.898, Val loss: 3.545, Epoch time = 41.068s\n",
        "A group of people are standing in a crowd of people .\n",
        "Epoch: 3, Train loss: 3.353, Val loss: 3.125, Epoch time = 41.566s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 4, Train loss: 2.944, Val loss: 2.830, Epoch time = 40.756s\n",
        "A group of people standing in front of a building .\n",
        "Epoch: 5, Train loss: 2.630, Val loss: 2.596, Epoch time = 41.468s\n",
        "A group of people standing in front of a crowd .\n",
        "Epoch: 6, Train loss: 2.375, Val loss: 2.429, Epoch time = 41.023s\n",
        "A group of people standing in front of a house .\n",
        "Epoch: 7, Train loss: 2.166, Val loss: 2.307, Epoch time = 41.604s\n",
        "A group of people stand in front of a house .\n",
        "Epoch: 8, Train loss: 1.984, Val loss: 2.210, Epoch time = 40.876s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 9, Train loss: 1.834, Val loss: 2.131, Epoch time = 41.496s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 10, Train loss: 1.698, Val loss: 2.079, Epoch time = 41.052s\n",
        "A group of people are standing in front of an empty house .\n",
        "Epoch: 11, Train loss: 1.576, Val loss: 2.038, Epoch time = 41.570s\n",
        "A group of people are standing in front of an audience .\n",
        "Epoch: 12, Train loss: 1.475, Val loss: 2.033, Epoch time = 41.067s\n",
        "A group of people stand in front of an audience .\n",
        "Epoch: 13, Train loss: 1.381, Val loss: 2.017, Epoch time = 41.576s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 14, Train loss: 1.292, Val loss: 1.977, Epoch time = 40.779s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 15, Train loss: 1.213, Val loss: 1.948, Epoch time = 41.461s\n",
        "A group of people stand in front of an operation .\n",
        "Epoch: 16, Train loss: 1.139, Val loss: 1.940, Epoch time = 40.800s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 17, Train loss: 1.073, Val loss: 1.940, Epoch time = 41.533s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 18, Train loss: 1.006, Val loss: 1.939, Epoch time = 40.856s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 19, Train loss: 0.944, Val loss: 1.947, Epoch time = 41.345s\n",
        "A group of people stand in front of an igloo\n",
        "Epoch: 20, Train loss: 0.893, Val loss: 1.956, Epoch time = 40.935s\n",
        "A group of people stand in front of an igloo\n",
        "'''"
      ],
      "metadata": {
        "id": "x-bHqmV4ReUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title bleu\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    for datum in data:\n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "    return bleu_score(pred_trgs, trgs)\n",
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')\n",
        "# 36.52, which beats the ~34 of the convolutional sequence-to-sequence model and ~28 of the attention based RNN model.\n",
        "\n",
        "def translate_sentence_vectorized(src_tensor, src_field, trg_field, model, device, max_len=50):\n",
        "    assert isinstance(src_tensor, torch.Tensor)\n",
        "\n",
        "    model.eval()\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    # enc_src = [batch_sz, src_len, hid_dim]\n",
        "\n",
        "    trg_indexes = [[trg_field.vocab.stoi[trg_field.init_token]] for _ in range(len(src_tensor))]\n",
        "    # Even though some examples might have been completed by producing a <eos> token\n",
        "    # we still need to feed them through the model because other are not yet finished\n",
        "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
        "    # <eos> token, then we can stop predicting.\n",
        "    translations_done = [0] * len(src_tensor)\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_tokens = output.argmax(2)[:,-1]\n",
        "        for i, pred_token_i in enumerate(pred_tokens):\n",
        "            trg_indexes[i].append(pred_token_i)\n",
        "            if pred_token_i == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                translations_done[i] = 1\n",
        "        if all(translations_done):\n",
        "            break\n",
        "\n",
        "    # Iterate through each predicted example one by one;\n",
        "    # Cut-off the portion including the after the <eos> token\n",
        "    pred_sentences = []\n",
        "    for trg_sentence in trg_indexes:\n",
        "        pred_sentence = []\n",
        "        for i in range(1, len(trg_sentence)):\n",
        "            if trg_sentence[i] == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "                break\n",
        "            pred_sentence.append(trg_field.vocab.itos[trg_sentence[i]])\n",
        "        pred_sentences.append(pred_sentence)\n",
        "    return pred_sentences, attention\n",
        "\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu_alt(iterator, src_field, trg_field, model, device, max_len = 50):\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            _trgs = []\n",
        "            for sentence in trg:\n",
        "                tmp = []\n",
        "                # Start from the first token which skips the <start> token\n",
        "                for i in sentence[1:]:\n",
        "                    # Targets are padded. So stop appending as soon as a padding or eos token is encountered\n",
        "                    if i == trg_field.vocab.stoi[trg_field.eos_token] or i == trg_field.vocab.stoi[trg_field.pad_token]:\n",
        "                        break\n",
        "                    tmp.append(trg_field.vocab.itos[i])\n",
        "                _trgs.append([tmp])\n",
        "            trgs += _trgs\n",
        "            pred_trg, _ = translate_sentence_vectorized(src, src_field, trg_field, model, device)\n",
        "            pred_trgs += pred_trg\n",
        "    return pred_trgs, trgs, bleu_score(pred_trgs, trgs)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3vBEfUjzuobW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## old"
      ],
      "metadata": {
        "id": "NF9atY_nDOoV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAyqkMjq6T2C"
      },
      "outputs": [],
      "source": [
        "# Attention Is All You Need https://arxiv.org/pdf/1706.03762.pdf\n",
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
        "# https://www.mihaileric.com/posts/transformers-attention-in-disguise/\n",
        "# https://jalammar.github.io/illustrated-transformer/\n",
        "# http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
        "\n",
        "# position embedding <-> \"vocabulary\" size 100 <-> model can accept sentences up to 100 tokens long\n",
        "# learned positional encoding, warm-up and cool-down steps, label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V1teyZuwff9_"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "\n",
        "# https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
        "# https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c64c91cf87c13c0e83586b8e66e4d74e/translation_transformer.ipynb\n",
        "\n",
        "# https://github.com/pytorch/data\n",
        "# %pip install portalocker\n",
        "# %pip install torchdata\n",
        "\n",
        "# Create source and target language tokenizer. Make sure to install the dependencies.\n",
        "!pip install -qU torchdata torchtext\n",
        "!pip install -qU spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "!git clone --recursive https://github.com/multi30k/dataset.git multi30k-dataset\n",
        "\n",
        "# !pip list | grep torch\n",
        "!pip list | grep python\n",
        "\n",
        "import torchtext.datasets as datasets\n",
        "\n",
        "# Load the Multi30k dataset\n",
        "train_iter, valid_iter, test_iter = datasets.Multi30k(split=('train', 'valid', 'test'))\n",
        "\n",
        "# Iterate through the dataset\n",
        "for src, tgt in train_iter:\n",
        "    print(f\"Source: {src}\")\n",
        "    print(f\"Target: {tgt}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BFat7RgKSwR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "# modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "de_vocab.set_default_index(UNK_IDX)\n",
        "en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "# vocab_transform = {SRC_LANGUAGE:de_vocab, TRG_LANGUAGE:en_vocab}\n",
        "# text_transform = {SRC_LANGUAGE:de_transform, TRG_LANGUAGE:en_transform}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q datasets\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "!python -m spacy download de_core_news_sm\n"
      ],
      "metadata": {
        "id": "gURN1FYyM-A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title hf data\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a translation dataset (e.g., WMT English-German)\n",
        "dataset = load_dataset(\"wmt14\", \"de-en\")\n",
        "\n",
        "# Access train and validation splits\n",
        "train_data = dataset['train']\n",
        "val_data = dataset['validation']\n",
        "\n",
        "# # Example of accessing data\n",
        "# # for example in train_data:\n",
        "# #     print(example['translation'])\n",
        "# batch_size = 128 # 128\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "# val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "# from torchtext.datasets import multi30k, Multi30k\n",
        "# # modify the URLs for the dataset since the links to the original dataset are broken https://github.com/pytorch/text/issues/1756#issuecomment-1163664163\n",
        "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TRG_LANGUAGE = 'en'\n",
        "\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "# de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "# en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# from spacy.tokenizer import Tokenizer\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# # Create a blank Tokenizer with just the English vocab\n",
        "# tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# # Construction 2\n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# tokenizer = nlp.tokenizer\n",
        "\n",
        "import numpy as np\n",
        "np.float_ = np.float64\n",
        "np.complex_ = np.complex128\n",
        "import spacy\n",
        "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "de_tokenizer = spacy.load(\"de_core_news_sm\") # https://spacy.io/models/de\n",
        "\n",
        "\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 # unknown, pad, bigining, end of sentence\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# from torchtext.vocab import build_vocab_from_iterator\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "\n",
        "de_tokens = [de_tokenizer(data_sample[0]) for data_sample in train_iter]\n",
        "en_tokens = [en_tokenizer(data_sample[1]) for data_sample in train_iter]\n",
        "\n",
        "# de_vocab = build_vocab_from_iterator(de_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# en_vocab = build_vocab_from_iterator(en_tokens, min_freq=1, specials=special_symbols, special_first=True)\n",
        "# de_vocab.set_default_index(UNK_IDX)\n",
        "# en_vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "\n",
        "print(\"en_tokens\", en_tokens)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def de_transform(o):\n",
        "    o=de_tokenizer(o)\n",
        "    o=de_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "def en_transform(o):\n",
        "    o=en_tokenizer(o)\n",
        "    o=en_vocab(o)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(o), torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch): # convert a batch of raw strings into batch tensors\n",
        "    src_batch, trg_batch = [], []\n",
        "    for src_sample, trg_sample in batch:\n",
        "        src_batch.append(de_transform(src_sample.rstrip(\"\\n\")))\n",
        "        trg_batch.append(en_transform(trg_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "# val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
        "batch_size = 128 # 128\n",
        "train_loader = torch.utils.data.DataLoader(train_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_iter, batch_size=batch_size, collate_fn=collate_fn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "J-zgIl9FMgPV",
        "outputId": "fe72f8de-a2c7-4329-bf60-0ba7db0a3fd2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed70ac70a94f>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0men_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mde_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"de_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# https://spacy.io/models/de\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_seq_length=512):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        pos = torch.arange(0, max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop(x + self.pe[:, : x.size(1)])\n",
        "\n",
        "class LearntPosEnc(nn.Module): # learnt positional embeddings\n",
        "    def __init__(self, d_model, dropout=0.1, max_length=512):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, src_len = x.shape\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device) # [batch size, src len]\n",
        "        return self.drop(x + self.pos_embedding(pos))\n",
        "\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512):\n",
        "        super().__init__()\n",
        "        # theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
        "        theta = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n",
        "        # pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        pos = torch.arange(seq_len).unsqueeze(1)\n",
        "        angles = pos * theta # [seq_len, 1] * [dim // 2] = [seq_len, dim // 2]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim]\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, seq_len, dim = x.shape\n",
        "        # if rot_emb.shape[0] < seq_len: print(\"rot_emb.shape[0] < seq_len\")\n",
        "        rot_emb = self.rot_emb[:seq_len, :].unsqueeze(0).expand(batch, -1, -1)\n",
        "        x = x.view(batch, seq_len, dim // 2, 2)\n",
        "        rot_emb = rot_emb.view(batch, seq_len, dim // 2, 2)\n",
        "        # rot_x = torch.einsum('...ij,...ij->...ij', x, rot_emb)\n",
        "        rot_x = x * rot_emb\n",
        "        # return rot_x.view(*rot_x.shape[:-2], dim)\n",
        "        return rot_x.flatten(-2)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.lin = nn.Linear(d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.tensor((self.head_dim,), dtype=torch.float, device=device))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.q(query).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k(key).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(value).view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        attn = Q @ K.transpose(2, 3) / self.scale # attn = torch.matmul(Q, K.transpose(2, 3)) / self.scale\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e10) # [batch, n_heads, seq_len, seq_len]\n",
        "        attention = torch.softmax(attn, dim=-1)\n",
        "        x = self.drop(attention) @ V # x = torch.matmul(self.drop(attention), V)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, -1, self.d_model)\n",
        "        x = self.lin(x)\n",
        "        return x, attention\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.norm1(src + self.drop(self.self_attn(src, src, src, src_mask)[0]))\n",
        "        src = self.norm2(src + self.drop(self.ff(src)))\n",
        "        return src\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.RMSNorm(d_model)\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.norm3 = nn.RMSNorm(d_model)\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.enc_attn = MultiHeadAttention(d_model, n_heads, dropout=0)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        trg = self.norm1(trg + self.drop(self.self_attn(trg, trg, trg, trg_mask)[0]))\n",
        "        trg = self.norm2(trg + self.drop(self.enc_attn(trg, enc_src, enc_src, src_mask)[0]))\n",
        "        trg = self.norm3(trg + self.drop(self.ff(trg)))\n",
        "        return trg\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, n_layers, n_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        return trg\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, enc_layers, nhead, ff_dim, dropout)\n",
        "        self.decoder = Decoder(d_model, dec_layers, nhead, ff_dim, dropout)\n",
        "        # self.pos_enc = PositionalEncoder(d_model, dropout=dropout)\n",
        "        # self.pos_enc = LearntPosEnc(d_model, dropout=dropout)\n",
        "        self.pos_enc = RoPE(d_model)\n",
        "        self.src_tok_emb = nn.Embedding(in_dim, d_model)\n",
        "        self.trg_tok_emb = nn.Embedding(out_dim, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.lin = nn.Linear(d_model, out_dim)\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
        "        src = self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model))\n",
        "        trg = self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model))\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.lin(output)\n",
        "        return output\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        return self.encoder(self.pos_enc(self.src_tok_emb(src) * math.sqrt(self.d_model)), src_mask)\n",
        "\n",
        "    def decode(self, trg, memory, trg_mask=None, src_mask=None):\n",
        "        trg = self.decoder(self.pos_enc(self.trg_tok_emb(trg) * math.sqrt(self.d_model)), memory, trg_mask, src_mask)\n",
        "        return self.lin(trg)\n",
        "\n",
        "\n",
        "in_dim = 50#\n",
        "out_dim = 50\n",
        "model = Seq2Seq(in_dim, out_dim, d_model=512, nhead=8, enc_layers=3, dec_layers=3, ff_dim=512, dropout=0.1).to(device)\n"
      ],
      "metadata": {
        "id": "KeuV4EZyqXj5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "063e54f3-358b-449f-93a3-ff39e41d19c5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'de_vocab' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-23d76cdf20ff>\u001b[0m in \u001b[0;36m<cell line: 174>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0min_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0mout_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'de_vocab' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}